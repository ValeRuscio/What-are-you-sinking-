{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ccf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy.linalg\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from google.colab import drive\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fbd312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "class OptimizedAttentionLaplacianAnalysis:\n",
    "    \"\"\"\n",
    "    Optimized version of the attention Laplacian analysis that:\n",
    "    1. Eliminates individual sample storage (avoids circular references)\n",
    "    2. Directly aggregates metrics during analysis\n",
    "    3. Reduces I/O operations\n",
    "    4. Optimizes Laplacian computations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, output_dir=\"./attention_laplacian_analysis\"):\n",
    "        \"\"\"Initialize the analysis with model name and output directory.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = output_dir\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.aggregated_results = {}\n",
    "\n",
    "        self.sample_summary = []\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        self.vis_dir = os.path.join(output_dir, \"visualizations\")\n",
    "        os.makedirs(self.vis_dir, exist_ok=True)\n",
    "\n",
    "        self.peak_memory = 0\n",
    "        self.track_memory_usage = True\n",
    "\n",
    "    def _track_memory(self, message=\"\"):\n",
    "        \"\"\"Track GPU memory usage.\"\"\"\n",
    "        if not self.track_memory_usage or not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        current = torch.cuda.memory_allocated() / (1024 ** 3)  # GB\n",
    "        peak = torch.cuda.max_memory_allocated() / (1024 ** 3)  # GB\n",
    "\n",
    "        if peak > self.peak_memory:\n",
    "            self.peak_memory = peak\n",
    "            if message:\n",
    "                print(f\"Memory usage ({message}): Current {current:.2f} GB, Peak {peak:.2f} GB\")\n",
    "\n",
    "    def load_model(self, use_4bit=True):\n",
    "        \"\"\"Load the LLaMA model and tokenizer.\"\"\"\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        if use_4bit:\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "\n",
    "                quantization_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_compute_dtype=torch.float16\n",
    "                )\n",
    "\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    quantization_config=quantization_config,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error with 4-bit quantization: {e}\")\n",
    "                print(\"Falling back to standard loading...\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        self._track_memory(\"After model loading\")\n",
    "\n",
    "        print(f\"Model loaded: {self.model_name}\")\n",
    "        return self.model, self.tokenizer\n",
    "\n",
    "    def compute_laplacian_eigenvalues(self, attention_matrix, threshold=0.01, verbose=False):\n",
    "        \"\"\"\n",
    "        Compute Laplacian matrix eigenvalues from an attention matrix.\n",
    "\n",
    "        Args:\n",
    "            attention_matrix: Attention weight matrix (n x n)\n",
    "            threshold: Threshold to binarize the attention weights\n",
    "            verbose: Whether to print detailed diagnostics (default: False)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with eigenvalues and graph metrics\n",
    "        \"\"\"\n",
    "        if attention_matrix is None or attention_matrix.size == 0:\n",
    "            if verbose:\n",
    "                print(\"Warning: Empty attention matrix\")\n",
    "            return {\n",
    "                \"eigenvalues\": [],\n",
    "                \"first_eigenvalue\": 0.0,\n",
    "                \"fiedler_value\": 0.0,\n",
    "                \"spectral_gap\": 0.0,\n",
    "                \"max_in_degree\": 0,\n",
    "                \"star_likeness\": 0.0,\n",
    "                \"degree_centralization\": 0.0,  \n",
    "                \"degree_variance\": 0.0,        \n",
    "                \"avg_shortest_path\": None,\n",
    "                \"clustering_coefficient\": None,\n",
    "                \"error\": \"Empty attention matrix\"\n",
    "            }\n",
    "\n",
    "        # Ensure attention matrix has at least 2x2 dimensions for meaningful analysis\n",
    "        if min(attention_matrix.shape) < 2:\n",
    "            if verbose:\n",
    "                print(f\"Warning: Attention matrix too small: {attention_matrix.shape}\")\n",
    "            return {\n",
    "                \"eigenvalues\": [0.0] * min(attention_matrix.shape),\n",
    "                \"first_eigenvalue\": 0.0,\n",
    "                \"fiedler_value\": 0.0,\n",
    "                \"spectral_gap\": 0.0,\n",
    "                \"max_in_degree\": 0,\n",
    "                \"star_likeness\": 0.0,\n",
    "                \"degree_centralization\": 0.0,\n",
    "                \"degree_variance\": 0.0,\n",
    "                \"avg_shortest_path\": None,\n",
    "                \"clustering_coefficient\": None,\n",
    "                \"error\": \"Matrix too small for analysis\"\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Binarize the attention matrix based on threshold to create adjacency matrix\n",
    "            adjacency_matrix = (attention_matrix > threshold).astype(float)\n",
    "\n",
    "            # Check if we have enough connections after thresholding\n",
    "            connection_count = np.sum(adjacency_matrix)\n",
    "\n",
    "            if connection_count < 2:\n",
    "                if verbose:\n",
    "                    print(\"Warning: Too few connections after thresholding\")\n",
    "                return {\n",
    "                    \"eigenvalues\": [0.0] * min(attention_matrix.shape),\n",
    "                    \"first_eigenvalue\": 0.0,\n",
    "                    \"fiedler_value\": 0.0,\n",
    "                    \"spectral_gap\": 0.0,\n",
    "                    \"max_in_degree\": 0,\n",
    "                    \"star_likeness\": 0.0,\n",
    "                    \"degree_centralization\": 0.0,\n",
    "                    \"degree_variance\": 0.0,\n",
    "                    \"avg_shortest_path\": None,\n",
    "                    \"clustering_coefficient\": None,\n",
    "                    \"error\": \"Too few connections after thresholding\"\n",
    "                }\n",
    "\n",
    "            # OPTIMIZATION: Use sparse graph representation for large matrices\n",
    "            use_sparse = attention_matrix.shape[0] > 50\n",
    "\n",
    "            # Create a graph to analyze the structure - use sparse for large matrices\n",
    "            if use_sparse:\n",
    "                G = nx.DiGraph()\n",
    "                rows, cols = np.where(adjacency_matrix > 0)\n",
    "                edges = list(zip(rows.tolist(), cols.tolist()))\n",
    "                G.add_edges_from(edges)\n",
    "            else:\n",
    "                G = nx.from_numpy_array(adjacency_matrix, create_using=nx.DiGraph)\n",
    "\n",
    "            # Compute degree metrics\n",
    "            degrees = np.array([d for _, d in G.degree()])\n",
    "            in_degrees = np.array([d for _, d in G.in_degree()])\n",
    "            out_degrees = np.array([d for _, d in G.out_degree()])\n",
    "\n",
    "            # Compute star-likeness\n",
    "            total_nodes = len(in_degrees)\n",
    "            max_in_degree = np.max(in_degrees) if len(in_degrees) > 0 else 0\n",
    "\n",
    "            basic_star_likeness = max_in_degree / max(1, total_nodes - 1)\n",
    "\n",
    "            # 2. Gini coefficient of degree distribution \n",
    "            sorted_in_degrees = np.sort(in_degrees)\n",
    "            if len(sorted_in_degrees) > 0 and np.sum(sorted_in_degrees) > 0:\n",
    "                n = len(sorted_in_degrees)\n",
    "                indices = np.arange(1, n+1)\n",
    "                # Higher gini = more unequal distribution = more star-like\n",
    "                gini = 2 * np.sum(indices * sorted_in_degrees) / (n * np.sum(sorted_in_degrees)) - (n + 1) / n\n",
    "            else:\n",
    "                gini = 0.0\n",
    "\n",
    "            # 3. Degree centralization \n",
    "            max_possible_diff = (total_nodes - 1) * (total_nodes - 1)\n",
    "            if max_possible_diff > 0 and max_in_degree > 0:\n",
    "                sum_diff = sum(max_in_degree - d for d in in_degrees)\n",
    "                degree_centralization = sum_diff / max_possible_diff\n",
    "            else:\n",
    "                degree_centralization = 0.0\n",
    "\n",
    "            # 4. Degree variance \n",
    "            degree_variance = np.var(in_degrees) if len(in_degrees) > 1 else 0.0\n",
    "\n",
    "            # Weight the gini coefficient more heavily as it's less affected by network size\n",
    "            star_likeness = 0.3 * basic_star_likeness + 0.7 * gini\n",
    "\n",
    "            # Symmetrized adjacency for a sensible Laplacian\n",
    "            adj_sym = (adjacency_matrix + adjacency_matrix.T) / 2.0\n",
    "\n",
    "            # OPTIMIZATION: Compute Laplacian eigenvalues more efficiently\n",
    "            n = adjacency_matrix.shape[0]\n",
    "\n",
    "            if n <= 50: \n",
    "                degree_matrix = np.diag(np.sum(adj_sym, axis=1))\n",
    "\n",
    "                laplacian_matrix = degree_matrix - adj_sym\n",
    "\n",
    "                eigenvalues = np.linalg.eigvalsh(laplacian_matrix)\n",
    "                eigenvalues = np.sort(eigenvalues)  # Sort in ascending order\n",
    "            else:  # Large matrix\n",
    "                # Use sparse approach\n",
    "                try:\n",
    "                    adj_sparse = sp.csr_matrix(adj_sym)\n",
    "\n",
    "                    # Compute degrees and create sparse degree matrix\n",
    "                    degrees = np.array(adj_sparse.sum(axis=1)).flatten()\n",
    "                    D = sp.diags(degrees)\n",
    "\n",
    "                    # Laplacian matrix\n",
    "                    L = D - adj_sparse\n",
    "\n",
    "                    eigenvalues = spla.eigsh(L, k=min(3, n-1), which='SM', return_eigenvectors=False)\n",
    "                    eigenvalues = np.sort(eigenvalues)\n",
    "                except Exception:\n",
    "                    # Fallback to dense method if sparse fails\n",
    "                    degree_matrix = np.diag(np.sum(adj_sym, axis=1))\n",
    "                    laplacian_matrix = degree_matrix - adj_sym\n",
    "                    eigenvalues = np.linalg.eigvalsh(laplacian_matrix)\n",
    "                    eigenvalues = np.sort(eigenvalues)[:3]  # Take only the smallest 3\n",
    "\n",
    "            # Extract important eigenvalues \n",
    "            if len(eigenvalues) > 0:\n",
    "                first_eigenvalue = float(eigenvalues[0])\n",
    "            else:\n",
    "                first_eigenvalue = 0.0\n",
    "\n",
    "            # Second eigenvalue is the Fiedler value (algebraic connectivity)\n",
    "            if len(eigenvalues) > 1:\n",
    "                fiedler_value = float(eigenvalues[1])\n",
    "            else:\n",
    "                fiedler_value = 0.0\n",
    "\n",
    "            # Spectral gap \n",
    "            if len(eigenvalues) > 2:\n",
    "                spectral_gap = float(eigenvalues[2] - eigenvalues[1])\n",
    "            else:\n",
    "                spectral_gap = 0.0\n",
    "\n",
    "            avg_shortest_path = None\n",
    "            clustering_coef = None\n",
    "\n",
    "            if n <= 30:  # Only for small graphs\n",
    "                try:\n",
    "                    # Use undirected graph for these metrics\n",
    "                    if use_sparse:\n",
    "                        # Create symmetric sparse adjacency\n",
    "                        G_sym = nx.Graph()\n",
    "                        rows, cols = np.where(adj_sym > 0)\n",
    "                        edges = list(zip(rows.tolist(), cols.tolist()))\n",
    "                        G_sym.add_edges_from(edges)\n",
    "                    else:\n",
    "                        G_sym = nx.from_numpy_array(adj_sym)\n",
    "\n",
    "                    # Check if graph is connected before computing path lengths\n",
    "                    if nx.is_connected(G_sym):\n",
    "                        avg_shortest_path = nx.average_shortest_path_length(G_sym)\n",
    "                    else:\n",
    "                        # Fall back to largest connected component\n",
    "                        components = list(nx.connected_components(G_sym))\n",
    "                        if components:\n",
    "                            largest_cc = max(components, key=len)\n",
    "                            if len(largest_cc) > 1:  # Need at least 2 nodes for path length\n",
    "                                largest_cc_graph = G_sym.subgraph(largest_cc)\n",
    "                                avg_shortest_path = nx.average_shortest_path_length(largest_cc_graph)\n",
    "\n",
    "                    # Compute clustering \n",
    "                    clustering_coef = nx.average_clustering(G_sym)\n",
    "\n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"Error computing graph metrics: {str(e)}\")\n",
    "\n",
    "            return {\n",
    "                \"eigenvalues\": eigenvalues.tolist()[:3],  # Only store first 3 eigenvalues\n",
    "                \"first_eigenvalue\": float(first_eigenvalue),\n",
    "                \"fiedler_value\": float(fiedler_value),\n",
    "                \"spectral_gap\": float(spectral_gap),\n",
    "                \"max_in_degree\": int(max_in_degree),\n",
    "                \"star_likeness\": float(star_likeness),\n",
    "                \"basic_star_likeness\": float(basic_star_likeness),  \n",
    "                \"gini_coefficient\": float(gini),                    \n",
    "                \"degree_centralization\": float(degree_centralization),  \n",
    "                \"degree_variance\": float(degree_variance),         \n",
    "                \"avg_shortest_path\": float(avg_shortest_path) if avg_shortest_path is not None else None,\n",
    "                \"clustering_coefficient\": float(clustering_coef) if clustering_coef is not None else None,\n",
    "                \"connections\": int(connection_count),\n",
    "                \"total_possible_connections\": adjacency_matrix.size,\n",
    "                \"connection_density\": float(connection_count/adjacency_matrix.size)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Error in compute_laplacian_eigenvalues: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "            return {\n",
    "                \"eigenvalues\": [],\n",
    "                \"first_eigenvalue\": 0.0,\n",
    "                \"fiedler_value\": 0.0,\n",
    "                \"spectral_gap\": 0.0,\n",
    "                \"max_in_degree\": 0,\n",
    "                \"star_likeness\": 0.0,\n",
    "                \"degree_centralization\": 0.0,\n",
    "                \"degree_variance\": 0.0,\n",
    "                \"avg_shortest_path\": None,\n",
    "                \"clustering_coefficient\": None,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    def _init_aggregated_data(self, layer_indices, thresholds):\n",
    "        \"\"\"Initialize aggregated data structures.\"\"\"\n",
    "        # Create data structure for aggregation\n",
    "        aggregated = {\n",
    "            \"layer_stats\": {layer_idx: {} for layer_idx in layer_indices},\n",
    "            \"fiedler_value_trends\": {threshold: [] for threshold in thresholds},\n",
    "            \"spectral_gap_trends\": {threshold: [] for threshold in thresholds},\n",
    "            \"star_likeness_trends\": {threshold: [] for threshold in thresholds},\n",
    "            \"gini_coefficient_trends\": {threshold: [] for threshold in thresholds},\n",
    "            \"degree_centralization_trends\": {threshold: [] for threshold in thresholds},\n",
    "            \"degree_variance_trends\": {threshold: [] for threshold in thresholds},\n",
    "            \"threshold_effectiveness\": {threshold: {\"effective_layers\": []} for threshold in thresholds},\n",
    "            \"model\": self.model_name,\n",
    "            \"thresholds\": thresholds,\n",
    "            \"samples_analyzed\": 0\n",
    "        }\n",
    "\n",
    "        metrics_collectors = {}\n",
    "        for layer_idx in layer_indices:\n",
    "            metrics_collectors[layer_idx] = {}\n",
    "            for threshold in thresholds:\n",
    "                metrics_collectors[layer_idx][threshold] = {\n",
    "                    \"fiedler_values\": [],\n",
    "                    \"spectral_gaps\": [],\n",
    "                    \"star_likeness\": [],\n",
    "                    \"gini_coefficients\": [],\n",
    "                    \"degree_centralizations\": [],\n",
    "                    \"degree_variances\": [],\n",
    "                    \"avg_shortest_paths\": [],\n",
    "                    \"clustering_coefficients\": [],\n",
    "                    \"connection_densities\": []\n",
    "                }\n",
    "\n",
    "        return aggregated, metrics_collectors\n",
    "\n",
    "    def _update_aggregated_metrics(self, metrics_collectors, layer_idx, threshold, results):\n",
    "        \"\"\"Update the metrics collectors with results from a single analysis.\"\"\"\n",
    "        collectors = metrics_collectors[layer_idx][threshold]\n",
    "\n",
    "        # Update collectors if the metric exists and is valid\n",
    "        for metric_name, collector_name in [\n",
    "            (\"fiedler_value\", \"fiedler_values\"),\n",
    "            (\"spectral_gap\", \"spectral_gaps\"),\n",
    "            (\"star_likeness\", \"star_likeness\"),\n",
    "            (\"gini_coefficient\", \"gini_coefficients\"),\n",
    "            (\"degree_centralization\", \"degree_centralizations\"),\n",
    "            (\"degree_variance\", \"degree_variances\"),\n",
    "            (\"avg_shortest_path\", \"avg_shortest_paths\"),\n",
    "            (\"clustering_coefficient\", \"clustering_coefficients\"),\n",
    "            (\"connection_density\", \"connection_densities\")\n",
    "        ]:\n",
    "            if metric_name in results and results[metric_name] is not None:\n",
    "                collectors[collector_name].append(results[metric_name])\n",
    "\n",
    "    def _compute_aggregated_stats(self, metrics_collectors, aggregated):\n",
    "        \"\"\"Compute final statistics from the collected metrics.\"\"\"\n",
    "        # For each layer and threshold, compute statistics\n",
    "        for layer_idx, layer_data in metrics_collectors.items():\n",
    "            layer_stats = {}\n",
    "\n",
    "            for threshold, threshold_metrics in layer_data.items():\n",
    "                threshold_stats = {}\n",
    "\n",
    "                # For each metric, compute statistics\n",
    "                for metric_name, metric_values in threshold_metrics.items():\n",
    "                    if metric_values:\n",
    "                        threshold_stats[metric_name] = {\n",
    "                            \"mean\": float(np.mean(metric_values)),\n",
    "                            \"std\": float(np.std(metric_values)) if len(metric_values) > 1 else 0.0,\n",
    "                            \"min\": float(np.min(metric_values)) if metric_values else None,\n",
    "                            \"max\": float(np.max(metric_values)) if metric_values else None,\n",
    "                            \"count\": len(metric_values)\n",
    "                        }\n",
    "\n",
    "                layer_stats[str(threshold)] = threshold_stats\n",
    "\n",
    "                for metric_name, trend_key in [\n",
    "                    (\"fiedler_values\", \"fiedler_value_trends\"),\n",
    "                    (\"spectral_gaps\", \"spectral_gap_trends\"),\n",
    "                    (\"star_likeness\", \"star_likeness_trends\"),\n",
    "                    (\"gini_coefficients\", \"gini_coefficient_trends\"),\n",
    "                    (\"degree_centralizations\", \"degree_centralization_trends\"),\n",
    "                    (\"degree_variances\", \"degree_variance_trends\")\n",
    "                ]:\n",
    "                    values = threshold_metrics[metric_name]\n",
    "                    if values:\n",
    "                        aggregated[trend_key][threshold].append({\n",
    "                            \"layer\": layer_idx,\n",
    "                            \"value\": float(np.mean(values)),\n",
    "                            \"std\": float(np.std(values)) if len(values) > 1 else 0.0\n",
    "                        })\n",
    "\n",
    "                # Check threshold effectiveness\n",
    "                connection_densities = threshold_metrics[\"connection_densities\"]\n",
    "                if connection_densities:\n",
    "                    avg_density = np.mean(connection_densities)\n",
    "                    if 0.05 <= avg_density <= 0.95:\n",
    "                        if layer_idx not in aggregated[\"threshold_effectiveness\"][threshold][\"effective_layers\"]:\n",
    "                            aggregated[\"threshold_effectiveness\"][threshold][\"effective_layers\"].append(layer_idx)\n",
    "\n",
    "            # Add layer stats to aggregated data\n",
    "            aggregated[\"layer_stats\"][layer_idx] = layer_stats\n",
    "\n",
    "        # Calculate threshold effectiveness scores\n",
    "        total_layers = len(aggregated[\"layer_stats\"])\n",
    "        for threshold in aggregated[\"threshold_effectiveness\"]:\n",
    "            effective_layers = aggregated[\"threshold_effectiveness\"][threshold][\"effective_layers\"]\n",
    "            effectiveness = len(effective_layers) / total_layers if total_layers > 0 else 0\n",
    "            aggregated[\"threshold_effectiveness\"][threshold][\"effectiveness_score\"] = effectiveness\n",
    "\n",
    "        return aggregated\n",
    "\n",
    "    def analyze_samples_aggregate(self, texts, thresholds=[0.01, 0.05, 0.1, 0.2],\n",
    "                                 batch_size=10, sample_limit=None):\n",
    "        \"\"\"\n",
    "        Analyze multiple text samples directly into aggregated results without\n",
    "        storing each individual sample separately.\n",
    "\n",
    "        Args:\n",
    "            texts: List of text samples to analyze\n",
    "            thresholds: List of thresholds for attention binarization\n",
    "            batch_size: Number of samples to process in each batch\n",
    "            sample_limit: Maximum number of samples to process (None=all)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with aggregated results\n",
    "        \"\"\"\n",
    "        print(f\"Analyzing up to {len(texts)} text samples (limit: {sample_limit or 'None'})...\")\n",
    "\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "\n",
    "        if sample_limit is not None:\n",
    "            texts = texts[:sample_limit]\n",
    "\n",
    "        print(\"Processing first sample to initialize aggregation...\")\n",
    "        first_result = self._analyze_single_sample(texts[0], thresholds, get_layer_info=True)\n",
    "        layer_indices = first_result[\"layer_indices\"]\n",
    "        print(f\"Model has {len(layer_indices)} layers\")\n",
    "\n",
    "        aggregated, metrics_collectors = self._init_aggregated_data(layer_indices, thresholds)\n",
    "\n",
    "        # Add the first sample's data to the metrics collectors\n",
    "        for layer_idx in layer_indices:\n",
    "            for threshold in thresholds:\n",
    "                layer_results = first_result[\"layer_results\"].get(layer_idx, {})\n",
    "                threshold_results = layer_results.get(threshold, {})\n",
    "                if threshold_results:  # Skip if no results for this threshold\n",
    "                    self._update_aggregated_metrics(\n",
    "                        metrics_collectors, layer_idx, threshold, threshold_results\n",
    "                    )\n",
    "\n",
    "        # Process the remaining samples in batches\n",
    "        remaining_texts = texts[1:]\n",
    "        total_samples = 1  # Already processed 1\n",
    "\n",
    "        # Process in batches\n",
    "        for batch_start in range(0, len(remaining_texts), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(remaining_texts))\n",
    "            batch = remaining_texts[batch_start:batch_end]\n",
    "\n",
    "            print(f\"\\nProcessing batch {batch_start//batch_size + 1}: samples {batch_start+1+1} to {batch_end+1}\")\n",
    "\n",
    "            # Process each text in the batch\n",
    "            batch_results = []\n",
    "            for i, text in enumerate(tqdm(batch, desc=\"Processing batch\")):\n",
    "                sample_idx = batch_start + i + 1 + 1  \n",
    "                try:\n",
    "                    result = self._analyze_single_sample(text, thresholds, get_layer_info=False)\n",
    "                    batch_results.append((sample_idx, result))\n",
    "\n",
    "                    # Add to summary\n",
    "                    self.sample_summary.append({\n",
    "                        \"sample_id\": sample_idx,\n",
    "                        \"text_preview\": text[:50] + (\"...\" if len(text) > 50 else \"\"),\n",
    "                        \"token_count\": result.get(\"token_count\", 0),\n",
    "                        \"status\": \"success\"\n",
    "                    })\n",
    "\n",
    "                    total_samples += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error analyzing text {sample_idx}: {str(e)}\")\n",
    "                    traceback.print_exc()\n",
    "\n",
    "                    # Add to summary\n",
    "                    self.sample_summary.append({\n",
    "                        \"sample_id\": sample_idx,\n",
    "                        \"text_preview\": text[:50] + (\"...\" if len(text) > 50 else \"\"),\n",
    "                        \"status\": \"error\",\n",
    "                        \"error\": str(e)\n",
    "                    })\n",
    "\n",
    "            # Update metrics collectors with batch results\n",
    "            for sample_idx, result in batch_results:\n",
    "                for layer_idx in layer_indices:\n",
    "                    for threshold in thresholds:\n",
    "                        layer_results = result[\"layer_results\"].get(layer_idx, {})\n",
    "                        threshold_results = layer_results.get(threshold, {})\n",
    "                        if threshold_results:  # Skip if no results for this threshold\n",
    "                            self._update_aggregated_metrics(\n",
    "                                metrics_collectors, layer_idx, threshold, threshold_results\n",
    "                            )\n",
    "\n",
    "            # Force cleanup between batches\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            self._track_memory(f\"After batch {batch_start//batch_size + 1}\")\n",
    "\n",
    "            # Save intermediate aggregated results periodically\n",
    "            if (batch_start + batch_size) % (batch_size * 5) == 0 or batch_end == len(remaining_texts):\n",
    "                print(\"Generating intermediate aggregated results...\")\n",
    "                interim_aggregated = self._compute_aggregated_stats(metrics_collectors,\n",
    "                                                                    aggregated.copy())\n",
    "                interim_aggregated[\"samples_analyzed\"] = total_samples\n",
    "\n",
    "                # Save interim results\n",
    "                interim_file = os.path.join(self.output_dir, f\"interim_results_{total_samples}_samples.json\")\n",
    "                with open(interim_file, 'w') as f:\n",
    "                    json.dump(interim_aggregated, f, indent=2)\n",
    "                print(f\"Saved interim results after {total_samples} samples.\")\n",
    "\n",
    "        # Compute final aggregated statistics\n",
    "        print(\"\\nComputing final aggregated statistics...\")\n",
    "        aggregated = self._compute_aggregated_stats(metrics_collectors, aggregated)\n",
    "        aggregated[\"samples_analyzed\"] = total_samples\n",
    "\n",
    "        # Save sample summary\n",
    "        summary_file = os.path.join(self.output_dir, \"sample_summary.json\")\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(self.sample_summary, f, indent=2)\n",
    "\n",
    "        self.aggregated_results = aggregated\n",
    "\n",
    "        # Save full aggregated results\n",
    "        results_file = os.path.join(self.output_dir, \"aggregated_results.json\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(aggregated, f, indent=2)\n",
    "        print(f\"Saved aggregated results from {total_samples} samples to {results_file}\")\n",
    "\n",
    "        return aggregated\n",
    "\n",
    "    def _analyze_single_sample(self, text, thresholds, get_layer_info=False):\n",
    "        \"\"\"\n",
    "        Analyze a single text sample and extract metrics without storing full results.\n",
    "\n",
    "        Args:\n",
    "            text: Text to analyze\n",
    "            thresholds: List of thresholds to use\n",
    "            get_layer_info: Whether to extract and return layer indices (for first sample)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with minimal results and metrics\n",
    "        \"\"\"\n",
    "        text_preview = text[:30] + (\"...\" if len(text) > 30 else \"\")\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "        token_count = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                **inputs,\n",
    "                output_attentions=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "        # Extract attention patterns\n",
    "        attentions = outputs.attentions  # tuple of (layer, batch, head, seq_len, seq_len)\n",
    "\n",
    "        # Get layer indices if needed\n",
    "        layer_indices = list(range(len(attentions))) if get_layer_info else None\n",
    "\n",
    "        layer_results = {}\n",
    "        threshold_effects = {}\n",
    "\n",
    "        # Process each layer\n",
    "        for layer_idx, layer_attention in enumerate(attentions):\n",
    "            # Each layer has shape (batch, head, seq_len, seq_len)\n",
    "            layer_attention = layer_attention[0]  # shape: (head, seq_len, seq_len)\n",
    "\n",
    "            # Compute average attention pattern across all heads\n",
    "            avg_attention = layer_attention.mean(dim=0).cpu().numpy()\n",
    "\n",
    "            # Store layer results\n",
    "            layer_results[layer_idx] = {}\n",
    "\n",
    "            # Analyze for each threshold\n",
    "            for threshold in thresholds:\n",
    "                # Check threshold effects\n",
    "                adj_matrix = (avg_attention > threshold).astype(float)\n",
    "                connection_density = np.sum(adj_matrix) / adj_matrix.size\n",
    "                is_effective = 0.05 <= connection_density <= 0.95\n",
    "\n",
    "                if threshold not in threshold_effects:\n",
    "                    threshold_effects[threshold] = []\n",
    "\n",
    "                threshold_effects[threshold].append({\n",
    "                    \"layer\": layer_idx,\n",
    "                    \"connection_density\": float(connection_density),\n",
    "                    \"effective\": is_effective\n",
    "                })\n",
    "\n",
    "                # Compute eigenvalues and metrics - only if this is an effective threshold\n",
    "                if is_effective or layer_idx % 4 == 0:  \n",
    "                    layer_results[layer_idx][threshold] = self.compute_laplacian_eigenvalues(\n",
    "                        avg_attention, threshold=threshold, verbose=False\n",
    "                    )\n",
    "                else:\n",
    "                    # Just store the connection density for non-effective thresholds\n",
    "                    layer_results[layer_idx][threshold] = {\n",
    "                        \"connection_density\": float(connection_density)\n",
    "                    }\n",
    "\n",
    "        # Free memory\n",
    "        del outputs, attentions, layer_attention, avg_attention\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return {\n",
    "            \"token_count\": token_count,\n",
    "            \"layer_indices\": layer_indices,\n",
    "            \"layer_results\": layer_results,\n",
    "            \"threshold_effects\": threshold_effects\n",
    "        }\n",
    "\n",
    "    def visualize_results(self):\n",
    "        \"\"\"Create visualizations of the aggregated results.\"\"\"\n",
    "        if not self.aggregated_results:\n",
    "            print(\"No aggregated results to visualize.\")\n",
    "            return\n",
    "\n",
    "        print(\"Generating visualizations...\")\n",
    "\n",
    "        thresholds = self.aggregated_results[\"thresholds\"]\n",
    "\n",
    "        self._visualize_threshold_effectiveness()\n",
    "        self._visualize_fiedler_value_trends()\n",
    "        self._visualize_star_likeness_metrics()\n",
    "        self._visualize_correlation_plots()\n",
    "        self._visualize_fiedler_heatmap()\n",
    "        self._visualize_layer_adjacency_comparison()\n",
    "\n",
    "        print(f\"Visualizations saved to {self.vis_dir}\")\n",
    "\n",
    "    def _visualize_threshold_effectiveness(self):\n",
    "        \"\"\"Visualize threshold effectiveness.\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        effectiveness_scores = []\n",
    "        threshold_labels = []\n",
    "\n",
    "        for threshold in sorted(self.aggregated_results[\"thresholds\"]):\n",
    "            if threshold in self.aggregated_results[\"threshold_effectiveness\"]:\n",
    "                score = self.aggregated_results[\"threshold_effectiveness\"][threshold].get(\"effectiveness_score\", 0)\n",
    "                effectiveness_scores.append(score)\n",
    "                threshold_labels.append(str(threshold))\n",
    "\n",
    "        plt.bar(threshold_labels, effectiveness_scores, color='skyblue')\n",
    "        plt.title('Threshold Effectiveness (% of Effective Layers)')\n",
    "        plt.xlabel('Threshold Value')\n",
    "        plt.ylabel('Effectiveness Score')\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        plot_path = os.path.join(self.vis_dir, 'threshold_effectiveness.png')\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "\n",
    "    def _visualize_fiedler_value_trends(self):\n",
    "        \"\"\"Visualize Fiedler value trends across layers.\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        has_data = False\n",
    "        for threshold in self.aggregated_results[\"thresholds\"]:\n",
    "            if threshold not in self.aggregated_results[\"fiedler_value_trends\"]:\n",
    "                continue\n",
    "\n",
    "            trend_data = self.aggregated_results[\"fiedler_value_trends\"][threshold]\n",
    "            if not trend_data:\n",
    "                continue\n",
    "\n",
    "            trend_data = sorted(trend_data, key=lambda x: x[\"layer\"])\n",
    "\n",
    "            layers = [item[\"layer\"] for item in trend_data]\n",
    "            values = [item[\"value\"] for item in trend_data]\n",
    "            stds = [item[\"std\"] for item in trend_data]\n",
    "\n",
    "            plt.errorbar(\n",
    "                layers, values, yerr=stds,\n",
    "                marker='o', linestyle='-',\n",
    "                label=f'Threshold = {threshold}'\n",
    "            )\n",
    "            has_data = True\n",
    "\n",
    "        if has_data:\n",
    "            plt.title('Algebraic Connectivity (Fiedler Value) Across Layers')\n",
    "            plt.xlabel('Layer')\n",
    "            plt.ylabel('Fiedler Value')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plot_path = os.path.join(self.vis_dir, 'fiedler_value_trends.png')\n",
    "            plt.savefig(plot_path)\n",
    "        else:\n",
    "            print(\"No data available for Fiedler value trends plot\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def _visualize_star_likeness_metrics(self):\n",
    "        \"\"\"Visualize star-likeness metrics across layers.\"\"\"\n",
    "        # First try the gini coefficient as it's more stable\n",
    "        if \"gini_coefficient_trends\" in self.aggregated_results:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "\n",
    "            has_data = False\n",
    "            for threshold in self.aggregated_results[\"thresholds\"]:\n",
    "                if threshold not in self.aggregated_results[\"gini_coefficient_trends\"]:\n",
    "                    continue\n",
    "\n",
    "                trend_data = self.aggregated_results[\"gini_coefficient_trends\"][threshold]\n",
    "                if not trend_data:\n",
    "                    continue\n",
    "\n",
    "                trend_data = sorted(trend_data, key=lambda x: x[\"layer\"])\n",
    "\n",
    "                layers = [item[\"layer\"] for item in trend_data]\n",
    "                values = [item[\"value\"] for item in trend_data]\n",
    "                stds = [item[\"std\"] for item in trend_data]\n",
    "\n",
    "                plt.errorbar(\n",
    "                    layers, values, yerr=stds,\n",
    "                    marker='o', linestyle='-',\n",
    "                    label=f'Threshold = {threshold}'\n",
    "                )\n",
    "                has_data = True\n",
    "\n",
    "            if has_data:\n",
    "                plt.title('Gini Coefficient of Degree Distribution Across Layers')\n",
    "                plt.xlabel('Layer')\n",
    "                plt.ylabel('Gini Coefficient (higher = more star-like)')\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "\n",
    "                plot_path = os.path.join(self.vis_dir, 'gini_coefficient_trends.png')\n",
    "                plt.savefig(plot_path)\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        has_data = False\n",
    "        for threshold in self.aggregated_results[\"thresholds\"]:\n",
    "            if threshold not in self.aggregated_results[\"star_likeness_trends\"]:\n",
    "                continue\n",
    "\n",
    "            trend_data = self.aggregated_results[\"star_likeness_trends\"][threshold]\n",
    "            if not trend_data:\n",
    "                continue\n",
    "\n",
    "            trend_data = sorted(trend_data, key=lambda x: x[\"layer\"])\n",
    "\n",
    "            layers = [item[\"layer\"] for item in trend_data]\n",
    "            values = [item[\"value\"] for item in trend_data]\n",
    "            stds = [item[\"std\"] for item in trend_data]\n",
    "\n",
    "            plt.errorbar(\n",
    "                layers, values, yerr=stds,\n",
    "                marker='o', linestyle='-',\n",
    "                label=f'Threshold = {threshold}'\n",
    "            )\n",
    "            has_data = True\n",
    "\n",
    "        if has_data:\n",
    "            plt.title('Improved Star-likeness Metric Across Layers')\n",
    "            plt.xlabel('Layer')\n",
    "            plt.ylabel('Star-likeness (higher = more star-like)')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save the plot\n",
    "            plot_path = os.path.join(self.vis_dir, 'star_likeness_trends.png')\n",
    "            plt.savefig(plot_path)\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def _visualize_correlation_plots(self):\n",
    "        \"\"\"Visualize correlations between metrics.\"\"\"\n",
    "        # Get the most effective threshold\n",
    "        best_threshold = None\n",
    "        max_effectiveness = 0\n",
    "        for threshold in self.aggregated_results[\"thresholds\"]:\n",
    "            if threshold in self.aggregated_results[\"threshold_effectiveness\"]:\n",
    "                effectiveness = self.aggregated_results[\"threshold_effectiveness\"][threshold].get(\"effectiveness_score\", 0)\n",
    "                if effectiveness > max_effectiveness:\n",
    "                    max_effectiveness = effectiveness\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        if best_threshold is None and self.aggregated_results[\"thresholds\"]:\n",
    "            best_threshold = self.aggregated_results[\"thresholds\"][0]  # Fallback\n",
    "\n",
    "        # List of metrics to try correlating with Fiedler value\n",
    "        correlation_metrics = [\n",
    "            (\"gini_coefficient_trends\", \"Gini Coefficient\"),\n",
    "            (\"star_likeness_trends\", \"Improved Star-likeness\"),\n",
    "            (\"degree_centralization_trends\", \"Degree Centralization\"),\n",
    "            (\"degree_variance_trends\", \"Degree Variance\")\n",
    "        ]\n",
    "\n",
    "        all_correlations = []\n",
    "\n",
    "        for trend_key, metric_name in correlation_metrics:\n",
    "            if trend_key not in self.aggregated_results:\n",
    "                continue\n",
    "\n",
    "            plt.figure(figsize=(12, 8))\n",
    "\n",
    "            has_data = False\n",
    "\n",
    "            for threshold in self.aggregated_results[\"thresholds\"]:\n",
    "                if threshold not in self.aggregated_results[\"fiedler_value_trends\"] or \\\n",
    "                   threshold not in self.aggregated_results[trend_key]:\n",
    "                    continue\n",
    "\n",
    "                fiedler_trend = self.aggregated_results[\"fiedler_value_trends\"][threshold]\n",
    "                star_trend = self.aggregated_results[trend_key][threshold]\n",
    "\n",
    "                if not fiedler_trend or not star_trend:\n",
    "                    continue\n",
    "\n",
    "                # Match up layers\n",
    "                fiedler_by_layer = {item[\"layer\"]: item[\"value\"] for item in fiedler_trend}\n",
    "                star_by_layer = {item[\"layer\"]: item[\"value\"] for item in star_trend}\n",
    "\n",
    "                # Get layers present in both\n",
    "                common_layers = sorted(set(fiedler_by_layer.keys()) & set(star_by_layer.keys()))\n",
    "\n",
    "                if not common_layers:\n",
    "                    continue\n",
    "\n",
    "                # Extract paired values\n",
    "                fiedler_values = [fiedler_by_layer[layer] for layer in common_layers]\n",
    "                star_values = [star_by_layer[layer] for layer in common_layers]\n",
    "\n",
    "                # Check if there's actually variation in the values\n",
    "                if np.std(star_values) < 1e-10 or np.std(fiedler_values) < 1e-10:\n",
    "                    continue\n",
    "\n",
    "                plt.scatter(\n",
    "                    star_values, fiedler_values,\n",
    "                    label=f'Threshold = {threshold}',\n",
    "                    alpha=0.7, s=80\n",
    "                )\n",
    "\n",
    "                # Highlight best threshold with different marker\n",
    "                if threshold == best_threshold:\n",
    "                    # Add a linear regression line for the best threshold\n",
    "                    z = np.polyfit(star_values, fiedler_values, 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    x_range = np.linspace(min(star_values), max(star_values), 100)\n",
    "                    plt.plot(x_range, p(x_range), '--', color='red', alpha=0.7)\n",
    "\n",
    "                # Add text labels for layers\n",
    "                for i, layer in enumerate(common_layers):\n",
    "                    plt.annotate(\n",
    "                        f\"L{layer}\",\n",
    "                        (star_values[i], fiedler_values[i]),\n",
    "                        xytext=(5, 5),\n",
    "                        textcoords='offset points'\n",
    "                    )\n",
    "\n",
    "                # Calculate correlation\n",
    "                if len(fiedler_values) > 1:\n",
    "                    corr = np.corrcoef(star_values, fiedler_values)[0, 1]\n",
    "                    all_correlations.append((threshold, metric_name, corr))\n",
    "\n",
    "                has_data = True\n",
    "\n",
    "            if has_data:\n",
    "                plt.title(f'Correlation: {metric_name} vs. Algebraic Connectivity')\n",
    "                plt.xlabel(f'{metric_name} (higher = more star-like)')\n",
    "                plt.ylabel('Fiedler Value (Algebraic Connectivity)')\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "\n",
    "                metric_key = trend_key.replace(\"_trends\", \"\")\n",
    "                plot_path = os.path.join(self.vis_dir, f'{metric_key}_fiedler_correlation.png')\n",
    "                plt.savefig(plot_path)\n",
    "            else:\n",
    "                print(f\"No data available for {metric_name} correlation plot\")\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "        # Save correlations to file\n",
    "        corr_file = os.path.join(self.vis_dir, 'correlations.txt')\n",
    "        with open(corr_file, 'w') as f:\n",
    "            f.write(\"CORRELATIONS WITH FIEDLER VALUE\\n\")\n",
    "            f.write(\"====================================================\\n\\n\")\n",
    "            f.write(\"{:<10} {:<25} {:<10}\\n\".format(\"Threshold\", \"Metric\", \"Correlation\"))\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            for threshold, metric, corr in sorted(all_correlations, key=lambda x: (x[0], x[1])):\n",
    "                f.write(\"{:<10} {:<25} {:<10.4f}\\n\".format(threshold, metric, corr))\n",
    "\n",
    "    def _visualize_fiedler_heatmap(self):\n",
    "        \"\"\"Create a heatmap of Fiedler values.\"\"\"\n",
    "        # Get the most effective threshold\n",
    "        best_threshold = None\n",
    "        max_effectiveness = 0\n",
    "        for threshold in self.aggregated_results[\"thresholds\"]:\n",
    "            if threshold in self.aggregated_results[\"threshold_effectiveness\"]:\n",
    "                effectiveness = self.aggregated_results[\"threshold_effectiveness\"][threshold].get(\"effectiveness_score\", 0)\n",
    "                if effectiveness > max_effectiveness:\n",
    "                    max_effectiveness = effectiveness\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        if best_threshold is None and self.aggregated_results[\"thresholds\"]:\n",
    "            best_threshold = self.aggregated_results[\"thresholds\"][0]  # Fallback\n",
    "\n",
    "        # Create heatmap for the best threshold\n",
    "        if best_threshold is not None and best_threshold in self.aggregated_results[\"fiedler_value_trends\"]:\n",
    "            best_trend_data = self.aggregated_results[\"fiedler_value_trends\"][best_threshold]\n",
    "            if best_trend_data:\n",
    "                best_trend_data = sorted(best_trend_data, key=lambda x: x[\"layer\"])\n",
    "\n",
    "                layers = np.array([item[\"layer\"] for item in best_trend_data])\n",
    "                values = np.array([item[\"value\"] for item in best_trend_data])\n",
    "\n",
    "                # Reshape for heatmap\n",
    "                heatmap_data = values.reshape(1, -1)\n",
    "\n",
    "                plt.figure(figsize=(14, 4))\n",
    "                sns.heatmap(\n",
    "                    heatmap_data,\n",
    "                    cmap='viridis',\n",
    "                    annot=True,\n",
    "                    fmt=\".3f\",\n",
    "                    xticklabels=layers,\n",
    "                    yticklabels=[\"Fiedler Value\"]\n",
    "                )\n",
    "\n",
    "                plt.title(f'Algebraic Connectivity Across Layers (Threshold = {best_threshold})')\n",
    "                plt.xlabel('Layer')\n",
    "                plt.tight_layout()\n",
    "\n",
    "                plot_path = os.path.join(self.vis_dir, 'fiedler_value_heatmap.png')\n",
    "                plt.savefig(plot_path)\n",
    "            else:\n",
    "                print(\"No data available for Fiedler value heatmap\")\n",
    "        else:\n",
    "            print(\"Could not determine best threshold for Fiedler value heatmap\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def _visualize_layer_adjacency_comparison(self):\n",
    "        \"\"\"Create a comparison of adjacency matrices from early, middle, and late layers.\"\"\"\n",
    "        if not self.sample_summary:\n",
    "            print(\"No sample summary available for adjacency comparison\")\n",
    "            return\n",
    "\n",
    "        if self.model is None:\n",
    "            print(\"Model not loaded, skipping layer adjacency comparison\")\n",
    "            return\n",
    "\n",
    "        success_samples = [s for s in self.sample_summary if s[\"status\"] == \"success\"]\n",
    "        if not success_samples:\n",
    "            print(\"No successful samples found for adjacency comparison\")\n",
    "            return\n",
    "\n",
    "        # Use the first successful sample\n",
    "        sample_text = None\n",
    "        for sample in success_samples:\n",
    "            preview = sample[\"text_preview\"]\n",
    "            if preview.endswith(\"...\"):\n",
    "                sample_text = preview[:-3] \n",
    "            else:\n",
    "                sample_text = preview\n",
    "\n",
    "            if len(sample_text) > 10:  # Ensure we have enough text\n",
    "                break\n",
    "\n",
    "        if not sample_text:\n",
    "            print(\"No suitable sample text found for adjacency comparison\")\n",
    "            return\n",
    "\n",
    "        # Get layer indices from aggregated results\n",
    "        layer_indices = sorted([int(idx) for idx in self.aggregated_results[\"layer_stats\"].keys()])\n",
    "\n",
    "        if len(layer_indices) < 3:\n",
    "            print(\"Not enough layers for comparison\")\n",
    "            return\n",
    "\n",
    "        # Select representative layers\n",
    "        early_idx = layer_indices[0]\n",
    "        middle_idx = layer_indices[len(layer_indices) // 2]\n",
    "        late_idx = layer_indices[-1]\n",
    "\n",
    "        selected_layers = [\n",
    "            (\"Early\", early_idx),\n",
    "            (\"Middle\", middle_idx),\n",
    "            (\"Late\", late_idx)\n",
    "        ]\n",
    "\n",
    "        # Find the most effective threshold to use\n",
    "        thresholds = self.aggregated_results[\"thresholds\"]\n",
    "        best_threshold = None\n",
    "        max_effectiveness = 0\n",
    "\n",
    "        if \"threshold_effectiveness\" in self.aggregated_results:\n",
    "            for threshold in thresholds:\n",
    "                if threshold in self.aggregated_results[\"threshold_effectiveness\"]:\n",
    "                    effectiveness = self.aggregated_results[\"threshold_effectiveness\"][threshold].get(\"effectiveness_score\", 0)\n",
    "                    if effectiveness > max_effectiveness:\n",
    "                        max_effectiveness = effectiveness\n",
    "                        best_threshold = threshold\n",
    "\n",
    "        if best_threshold is None and thresholds:\n",
    "            best_threshold = thresholds[0]  # Fallback\n",
    "\n",
    "        print(f\"Using threshold {best_threshold} for layer comparison visualization\")\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        for i, (stage, layer_idx) in enumerate(selected_layers):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "\n",
    "            # Get properties from aggregated results\n",
    "            layer_data = self.aggregated_results[\"layer_stats\"].get(layer_idx, {})\n",
    "            threshold_data = layer_data.get(str(best_threshold), {})\n",
    "\n",
    "            fiedler_data = threshold_data.get(\"fiedler_values\", {})\n",
    "            star_data = None\n",
    "\n",
    "            # Try to get the best star-likeness metric available\n",
    "            for metric_name in [\"gini_coefficients\", \"star_likeness\", \"degree_centralizations\"]:\n",
    "                if metric_name in threshold_data:\n",
    "                    star_data = threshold_data[metric_name]\n",
    "                    star_metric_name = metric_name.replace(\"s\", \"\")  # Remove plural\n",
    "                    break\n",
    "\n",
    "            if star_data is None:\n",
    "                star_metric_name = \"star_likeness\"\n",
    "                star_data = threshold_data.get(\"star_likeness\", {})\n",
    "\n",
    "            fiedler_value = fiedler_data.get(\"mean\", \"N/A\") if isinstance(fiedler_data, dict) else \"N/A\"\n",
    "            star_likeness = star_data.get(\"mean\", \"N/A\") if isinstance(star_data, dict) else \"N/A\"\n",
    "\n",
    "            # Generate a new adjacency matrix for visualization\n",
    "            try:\n",
    "                inputs = self.tokenizer(sample_text, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(\n",
    "                        **inputs,\n",
    "                        output_attentions=True,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "\n",
    "                # Check if we have enough layers\n",
    "                if layer_idx >= len(outputs.attentions):\n",
    "                    plt.text(0.5, 0.5, f\"Layer {layer_idx} exceeds model layers\",\n",
    "                             ha='center', va='center', transform=plt.gca().transAxes)\n",
    "                    continue\n",
    "\n",
    "                layer_attention = outputs.attentions[layer_idx][0]  # (head, seq, seq)\n",
    "                avg_attention = layer_attention.mean(dim=0).cpu().numpy()\n",
    "\n",
    "                # Create binary adjacency for visualization\n",
    "                adjacency_matrix = (avg_attention > best_threshold).astype(float)\n",
    "\n",
    "                plt.imshow(adjacency_matrix, cmap='Blues')\n",
    "                plt.colorbar(label='Connection')\n",
    "\n",
    "                title_parts = [f'{stage} Layer (L{layer_idx})']\n",
    "                if fiedler_value != \"N/A\":\n",
    "                    title_parts.append(f\"Fiedler={fiedler_value:.3f}\")\n",
    "                if star_likeness != \"N/A\":\n",
    "                    title_parts.append(f\"{star_metric_name.capitalize()}={star_likeness:.3f}\")\n",
    "\n",
    "                plt.title('\\n'.join(title_parts))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error visualizing layer {layer_idx}: {e}\")\n",
    "                plt.text(0.5, 0.5, f\"Visualization failed: {str(e)[:30]}...\",\n",
    "                         ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "            plt.axis('on')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plot_path = os.path.join(self.vis_dir, 'layer_adjacency_comparison.png')\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"Saved layer comparison to {plot_path}\")\n",
    "        plt.close()\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a text report summarizing the findings.\"\"\"\n",
    "        if not self.aggregated_results:\n",
    "            print(\"No aggregated results to report.\")\n",
    "            return None\n",
    "\n",
    "        print(\"Generating comprehensive report...\")\n",
    "\n",
    "        report_path = os.path.join(self.output_dir, \"laplacian_eigenvalue_report.txt\")\n",
    "\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"ATTENTION GRAPH LAPLACIAN EIGENVALUE ANALYSIS\\n\")\n",
    "            f.write(\"===========================================\\n\\n\")\n",
    "\n",
    "            f.write(f\"Model: {self.model_name}\\n\")\n",
    "            f.write(f\"Analysis date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "            n_samples = self.aggregated_results.get(\"samples_analyzed\", 0)\n",
    "            f.write(f\"Number of samples analyzed: {n_samples}\\n\\n\")\n",
    "\n",
    "            thresholds = self.aggregated_results[\"thresholds\"]\n",
    "            f.write(\"Thresholds used for attention binarization:\\n\")\n",
    "            for threshold in thresholds:\n",
    "                f.write(f\"  - {threshold}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"THRESHOLD EFFECTIVENESS ANALYSIS\\n\")\n",
    "            f.write(\"-------------------------------\\n\\n\")\n",
    "\n",
    "            best_threshold = None\n",
    "            max_effectiveness = 0\n",
    "\n",
    "            if \"threshold_effectiveness\" in self.aggregated_results:\n",
    "                for threshold in thresholds:\n",
    "                    if threshold in self.aggregated_results[\"threshold_effectiveness\"]:\n",
    "                        effectiveness = self.aggregated_results[\"threshold_effectiveness\"][threshold].get(\"effectiveness_score\", 0)\n",
    "                        effective_layers = len(self.aggregated_results[\"threshold_effectiveness\"][threshold].get(\"effective_layers\", []))\n",
    "                        total_layers = len(self.aggregated_results[\"layer_stats\"])\n",
    "\n",
    "                        f.write(f\"Threshold {threshold}:\\n\")\n",
    "                        f.write(f\"  Effectiveness score: {effectiveness:.2f}\\n\")\n",
    "                        f.write(f\"  Effective on {effective_layers}/{total_layers} layers\\n\")\n",
    "\n",
    "                        if effectiveness > max_effectiveness:\n",
    "                            max_effectiveness = effectiveness\n",
    "                            best_threshold = threshold\n",
    "\n",
    "                if best_threshold is not None:\n",
    "                    f.write(f\"\\nMost effective threshold: {best_threshold} (score: {max_effectiveness:.2f})\\n\")\n",
    "                    f.write(f\"This threshold creates meaningful graph structures across the highest\\n\")\n",
    "                    f.write(f\"percentage of layers, with connection densities between 5% and 95%.\\n\")\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"ALGEBRAIC CONNECTIVITY ANALYSIS\\n\")\n",
    "            f.write(\"------------------------------\\n\\n\")\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                if threshold not in self.aggregated_results[\"fiedler_value_trends\"]:\n",
    "                    continue\n",
    "\n",
    "                fiedler_trend = self.aggregated_results[\"fiedler_value_trends\"][threshold]\n",
    "\n",
    "                if not fiedler_trend:\n",
    "                    continue\n",
    "\n",
    "                f.write(f\"Threshold {threshold}:\\n\")\n",
    "\n",
    "                fiedler_trend = sorted(fiedler_trend, key=lambda x: x[\"layer\"])\n",
    "\n",
    "                max_fiedler_idx = np.argmax([item[\"value\"] for item in fiedler_trend])\n",
    "                max_fiedler_layer = fiedler_trend[max_fiedler_idx][\"layer\"]\n",
    "                max_fiedler_value = fiedler_trend[max_fiedler_idx][\"value\"]\n",
    "\n",
    "                f.write(f\"  Maximum algebraic connectivity at layer {max_fiedler_layer}: {max_fiedler_value:.4f}\\n\")\n",
    "\n",
    "                # Check if there's a peak in the middle\n",
    "                n_layers = len(fiedler_trend)\n",
    "                if n_layers >= 5:  # Need enough layers to meaningfully define \"middle\"\n",
    "                    early_layers = fiedler_trend[:n_layers//3]\n",
    "                    middle_layers = fiedler_trend[n_layers//3:2*n_layers//3]\n",
    "                    late_layers = fiedler_trend[2*n_layers//3:]\n",
    "\n",
    "                    early_avg = np.mean([item[\"value\"] for item in early_layers])\n",
    "                    middle_avg = np.mean([item[\"value\"] for item in middle_layers])\n",
    "                    late_avg = np.mean([item[\"value\"] for item in late_layers])\n",
    "\n",
    "                    f.write(f\"  Average Fiedler values:\\n\")\n",
    "                    f.write(f\"    Early layers: {early_avg:.4f}\\n\")\n",
    "                    f.write(f\"    Middle layers: {middle_avg:.4f}\\n\")\n",
    "                    f.write(f\"    Late layers: {late_avg:.4f}\\n\")\n",
    "\n",
    "                    if middle_avg > early_avg and middle_avg > late_avg:\n",
    "                        peak_factor = min(middle_avg/early_avg, middle_avg/late_avg)\n",
    "                        f.write(f\"  ✓ CONFIRMED: Middle layers show peak algebraic connectivity ({peak_factor:.2f}x higher)\\n\")\n",
    "                    elif middle_avg > early_avg:\n",
    "                        f.write(f\"  ± PARTIAL: Middle layers have higher connectivity than early layers, but not late layers\\n\")\n",
    "                    elif middle_avg > late_avg:\n",
    "                        f.write(f\"  ± PARTIAL: Middle layers have higher connectivity than late layers, but not early layers\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"  ✗ NOT CONFIRMED: Middle layers do not show peak algebraic connectivity\\n\")\n",
    "\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"GRAPH STRUCTURE ANALYSIS\\n\")\n",
    "            f.write(\"---------------------------\\n\\n\")\n",
    "\n",
    "            star_metrics = [\n",
    "                (\"star_likeness_trends\", \"Star-likeness\"),\n",
    "                (\"gini_coefficient_trends\", \"Gini coefficient\"),\n",
    "                (\"degree_centralization_trends\", \"Degree centralization\"),\n",
    "                (\"degree_variance_trends\", \"Degree variance\")\n",
    "            ]\n",
    "\n",
    "            for trend_key, metric_name in star_metrics:\n",
    "                if trend_key not in self.aggregated_results:\n",
    "                    continue\n",
    "\n",
    "                f.write(f\"{metric_name.upper()} ANALYSIS\\n\")\n",
    "                f.write(\"-\" * len(f\"{metric_name.upper()} ANALYSIS\") + \"\\n\\n\")\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    if threshold not in self.aggregated_results[trend_key]:\n",
    "                        continue\n",
    "\n",
    "                    trend_data = self.aggregated_results[trend_key][threshold]\n",
    "\n",
    "                    if not trend_data:\n",
    "                        continue\n",
    "\n",
    "                    f.write(f\"Threshold {threshold}:\\n\")\n",
    "\n",
    "                    trend_data = sorted(trend_data, key=lambda x: x[\"layer\"])\n",
    "\n",
    "                    # Find the layer with maximum value\n",
    "                    max_idx = np.argmax([item[\"value\"] for item in trend_data])\n",
    "                    max_layer = trend_data[max_idx][\"layer\"]\n",
    "                    max_value = trend_data[max_idx][\"value\"]\n",
    "\n",
    "                    f.write(f\"  Maximum {metric_name} at layer {max_layer}: {max_value:.4f}\\n\")\n",
    "\n",
    "                    # Check for variation across layers\n",
    "                    values = [item[\"value\"] for item in trend_data]\n",
    "                    if np.std(values) < 1e-10:\n",
    "                        f.write(f\"  ⚠ WARNING: {metric_name} values show almost no variation across layers.\\n\")\n",
    "                        f.write(f\"    This may indicate an issue with the analysis or the threshold value.\\n\")\n",
    "\n",
    "                    # Check if there's a peak in the middle\n",
    "                    n_layers = len(trend_data)\n",
    "                    if n_layers >= 5:  # Need enough layers\n",
    "                        early_layers = trend_data[:n_layers//3]\n",
    "                        middle_layers = trend_data[n_layers//3:2*n_layers//3]\n",
    "                        late_layers = trend_data[2*n_layers//3:]\n",
    "\n",
    "                        early_avg = np.mean([item[\"value\"] for item in early_layers])\n",
    "                        middle_avg = np.mean([item[\"value\"] for item in middle_layers])\n",
    "                        late_avg = np.mean([item[\"value\"] for item in late_layers])\n",
    "\n",
    "                        f.write(f\"  Average {metric_name}:\\n\")\n",
    "                        f.write(f\"    Early layers: {early_avg:.4f}\\n\")\n",
    "                        f.write(f\"    Middle layers: {middle_avg:.4f}\\n\")\n",
    "                        f.write(f\"    Late layers: {late_avg:.4f}\\n\")\n",
    "\n",
    "                        # Check if middle layers have higher values\n",
    "                        if middle_avg > early_avg and middle_avg > late_avg:\n",
    "                            peak_factor = min(middle_avg/early_avg, middle_avg/late_avg)\n",
    "                            f.write(f\"  ✓ CONFIRMED: Middle layers show peak {metric_name} ({peak_factor:.2f}x higher)\\n\")\n",
    "                        elif middle_avg > early_avg:\n",
    "                            f.write(f\"  ± PARTIAL: Middle layers have higher {metric_name} than early layers, but not late layers\\n\")\n",
    "                        elif middle_avg > late_avg:\n",
    "                            f.write(f\"  ± PARTIAL: Middle layers have higher {metric_name} than late layers, but not early layers\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"  ✗ NOT CONFIRMED: Middle layers do not show peak {metric_name}\\n\")\n",
    "\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"CORRELATION ANALYSIS\\n\")\n",
    "            f.write(\"-------------------\\n\\n\")\n",
    "\n",
    "            corr_file = os.path.join(self.vis_dir, 'correlations.txt')\n",
    "            if os.path.exists(corr_file):\n",
    "                with open(corr_file, 'r') as corr_f:\n",
    "                    correlations = corr_f.read()\n",
    "                f.write(correlations)\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(\"No correlation data available.\\n\\n\")\n",
    "\n",
    "            f.write(\"SUMMARY OF FINDINGS\\n\")\n",
    "            f.write(\"-----------------\\n\\n\")\n",
    "\n",
    "            f.write(\"\\nADDITIONAL NOTES\\n\")\n",
    "            f.write(\"---------------\\n\")\n",
    "            f.write(\"1. Threshold selection is critical for this analysis. The results suggest that\\n\")\n",
    "            f.write(\"   values around 0.01-0.05 provide the most meaningful graph structures for\\n\")\n",
    "            f.write(\"   the LLaMA model analyzed.\\n\\n\")\n",
    "\n",
    "            f.write(\"2. The improved star-likeness metrics (particularly the Gini coefficient of\\n\")\n",
    "            f.write(\"   degree distribution) provide more stable measurements of attention graph\\n\")\n",
    "            f.write(\"   structure than the basic star-likeness ratio used in earlier analyses.\\n\\n\")\n",
    "\n",
    "            f.write(\"\\nPERFORMANCE METRICS\\n\")\n",
    "            f.write(\"------------------\\n\")\n",
    "            f.write(f\"Samples processed: {n_samples}\\n\")\n",
    "            f.write(f\"Peak memory usage: {self.peak_memory:.2f} GB\\n\")\n",
    "\n",
    "            f.write(\"\\nVISUALIZATIONS\\n\")\n",
    "            f.write(\"---------------\\n\")\n",
    "            f.write(\"The following visualizations are available in the 'visualizations' directory:\\n\")\n",
    "            f.write(\"1. threshold_effectiveness.png - Comparison of threshold effectiveness\\n\")\n",
    "            f.write(\"2. fiedler_value_trends.png - Algebraic connectivity across layers\\n\")\n",
    "            f.write(\"3. gini_coefficient_trends.png - Gini coefficient of degree distribution\\n\")\n",
    "            f.write(\"4. star_likeness_trends.png - Improved star-likeness metric\\n\")\n",
    "            f.write(\"5. gini_coefficient_fiedler_correlation.png - Correlation: Gini vs. Fiedler\\n\")\n",
    "            f.write(\"6. star_likeness_fiedler_correlation.png - Correlation: Star-likeness vs. Fiedler\\n\")\n",
    "            f.write(\"7. fiedler_value_heatmap.png - Heatmap of Fiedler values\\n\")\n",
    "            f.write(\"8. layer_adjacency_comparison.png - Attention adjacency matrices comparison\\n\")\n",
    "\n",
    "        print(f\"Report generated at: {report_path}\")\n",
    "        return report_path\n",
    "\n",
    "    def run_analysis(self, texts=None, thresholds=[0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2],\n",
    "                    batch_size=10, sample_limit=None):\n",
    "        \"\"\"\n",
    "        Run the complete optimized analysis pipeline.\n",
    "\n",
    "        Args:\n",
    "            texts: List of text samples to analyze (if None, example texts are used)\n",
    "            thresholds: List of thresholds for attention binarization\n",
    "            batch_size: Number of samples to process in each batch\n",
    "            sample_limit: Maximum number of samples to process (None=all)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with analysis results\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "\n",
    "        if texts is None or not texts:\n",
    "            texts = [\n",
    "                \"The concept of eigenvalues relates to how transformations affect spaces.\",\n",
    "                \"Algebraic connectivity measures how well-connected a graph is.\",\n",
    "                \"Star graphs have one central node connected to all other nodes.\",\n",
    "                \"In transformers, attention mechanisms create dynamic connections between tokens.\"\n",
    "            ]\n",
    "\n",
    "        self.analyze_samples_aggregate(\n",
    "            texts=texts,\n",
    "            thresholds=thresholds,\n",
    "            batch_size=batch_size,\n",
    "            sample_limit=sample_limit\n",
    "        )\n",
    "\n",
    "        self.visualize_results()\n",
    "\n",
    "        self.generate_report()\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Analysis completed in {total_time/60:.2f} minutes\")\n",
    "\n",
    "        return self.aggregated_results\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.model is not None:\n",
    "            self.model = self.model.to(\"cpu\")\n",
    "            del self.model\n",
    "            self.model = None\n",
    "\n",
    "        if self.tokenizer is not None:\n",
    "            del self.tokenizer\n",
    "            self.tokenizer = None\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"Resources cleaned up\")\n",
    "\n",
    "\n",
    "def get_sample_texts_from_dataset(dataset_path, n_samples=50):\n",
    "    \"\"\"\n",
    "    Extract sample texts from a dataset for analysis.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the dataset CSV\n",
    "        n_samples: Number of samples to extract\n",
    "\n",
    "    Returns:\n",
    "        List of text samples\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load dataset\n",
    "        data = pd.read_csv(dataset_path)\n",
    "        print(f\"Loaded dataset with {len(data)} rows\")\n",
    "\n",
    "        # Check if text column exists\n",
    "        if 'text' not in data.columns:\n",
    "            print(\"Error: No 'text' column found in dataset\")\n",
    "            return []\n",
    "\n",
    "        # Sample texts\n",
    "        if len(data) > n_samples:\n",
    "            samples = data.sample(n_samples)\n",
    "        else:\n",
    "            samples = data\n",
    "\n",
    "        # Extract texts\n",
    "        texts = samples['text'].tolist()\n",
    "        return texts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63492f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the optimized Laplacian eigenvalue analysis pipeline.\"\"\"\n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Set paths\n",
    "    model_name = \"mistralai/Mistral-7B-v0.1\" \n",
    "    dataset_path = \"/content/drive/MyDrive/wiki_dataset.csv\" \n",
    "    output_dir = f\"/content/drive/MyDrive/Sink/laplacian/{model_name.split('/')[-1]}\"\n",
    "\n",
    "    analyzer = OptimizedAttentionLaplacianAnalysis(\n",
    "        model_name=model_name,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    # Get sample texts from dataset\n",
    "    texts = get_sample_texts_from_dataset(dataset_path, n_samples=500)\n",
    "\n",
    "    if not texts:\n",
    "        # Use default examples if no texts available\n",
    "        texts = [\n",
    "            \"The concept of eigenvalues relates to how transformations affect spaces.\",\n",
    "            \"Algebraic connectivity measures how well-connected a graph is.\",\n",
    "            \"Star graphs have one central node connected to all other nodes.\",\n",
    "            \"In transformers, attention mechanisms create dynamic connections between tokens.\"\n",
    "        ]\n",
    "\n",
    "    try:\n",
    "        analyzer.run_analysis(\n",
    "            texts=texts,\n",
    "            thresholds=[0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2],\n",
    "            batch_size=10,  \n",
    "            sample_limit=500  \n",
    "        )\n",
    "        print(\"Analysis completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Analysis error: {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        analyzer.cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
