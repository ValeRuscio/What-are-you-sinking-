{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f13064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedAttentionMatrixRMTAnalysis:\n",
    "    \"\"\"\n",
    "    Optimized analyzer for attention matrices using Random Matrix Theory (RMT).\n",
    "    Streamlined to focus on generating the final report only.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_size=\"pythia-70m\", output_dir=\"./pythia_attention_rmt_analysis\", verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize the analysis with model size and output directory.\n",
    "\n",
    "        Args:\n",
    "            model_size: Size of Pythia model (e.g., \"pythia-70m\", \"pythia-2.8b\")\n",
    "            output_dir: Directory to save results\n",
    "            verbose: Whether to print detailed progress information\n",
    "        \"\"\"\n",
    "        self.model_size = model_size\n",
    "        self.base_model_name = f\"EleutherAI/{model_size}\"\n",
    "        self.output_dir = output_dir\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.results = {}\n",
    "        self.verbose = verbose\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def log(self, message, force=False):\n",
    "        \"\"\"Print messages only when verbose is enabled or forced.\"\"\"\n",
    "        if self.verbose or force:\n",
    "            print(message)\n",
    "\n",
    "    def load_model_checkpoint(self, step=\"step0\"):\n",
    "        \"\"\"\n",
    "        Load a specific checkpoint of the Pythia model.\n",
    "\n",
    "        Args:\n",
    "            step: Checkpoint step (e.g., \"step0\", \"step1000\", \"step143000\")\n",
    "\n",
    "        Returns:\n",
    "            Loaded model and tokenizer\n",
    "        \"\"\"\n",
    "        self.log(f\"Loading {self.base_model_name} at {step}\", force=True)\n",
    "\n",
    "        # Clean up any existing model\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.base_model_name,\n",
    "                revision=step\n",
    "            )\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.base_model_name,\n",
    "                revision=step,\n",
    "                torch_dtype=torch.float32,\n",
    "                device_map=\"cpu\"\n",
    "            )\n",
    "\n",
    "            self.log(f\"Successfully loaded {self.base_model_name} at {step}\", force=True)\n",
    "            return self.model, self.tokenizer\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log(f\"Error loading model at {step}: {e}\", force=True)\n",
    "            if self.verbose:\n",
    "                traceback.print_exc()\n",
    "            raise e\n",
    "\n",
    "    def compute_eigenspectrum(self, attention_matrix):\n",
    "        \"\"\"\n",
    "        Compute eigenvalues and spectral statistics of an attention matrix.\n",
    "        Optimized for speed by computing only essential metrics.\n",
    "\n",
    "        Args:\n",
    "            attention_matrix: Attention weight matrix of shape (seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with eigenspectrum analysis\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(attention_matrix):\n",
    "            attention_matrix = attention_matrix.cpu().numpy()\n",
    "\n",
    "        try:\n",
    "            eigenvalues = np.linalg.eigvals(attention_matrix)\n",
    "\n",
    "            # convert to real numbers if there's a negligible imaginary component\n",
    "            if np.iscomplex(eigenvalues).any():\n",
    "                if np.all(np.abs(eigenvalues.imag) < 1e-10):\n",
    "                    eigenvalues = eigenvalues.real\n",
    "\n",
    "            # Sort eigenvalues by magnitude (descending)\n",
    "            eigenvalues = np.sort(np.abs(eigenvalues))[::-1]\n",
    "\n",
    "            # Calculate spectral gap (ratio between largest and second largest eigenvalue)\n",
    "            spectral_gap = float(eigenvalues[0] / eigenvalues[1]) if len(eigenvalues) > 1 else np.inf\n",
    "\n",
    "            # Calculate effective rank using participation ratio\n",
    "            squared_sum = np.sum(eigenvalues) ** 2\n",
    "            sum_squares = np.sum(eigenvalues ** 2)\n",
    "            participation_ratio = squared_sum / sum_squares if sum_squares > 0 else 0\n",
    "\n",
    "            # Calculate singular values (for actual rank estimation)\n",
    "            singular_values = np.linalg.svd(attention_matrix, compute_uv=False)\n",
    "\n",
    "            # Stable rank calculation\n",
    "            stable_rank = np.sum(singular_values ** 2) / (singular_values[0] ** 2) if singular_values[0] > 0 else 0\n",
    "\n",
    "            # Calculate spectral norm (largest singular value)\n",
    "            spectral_norm = singular_values[0]\n",
    "\n",
    "            # Calculate \"bulk edge\" - the edge of the main distribution of eigenvalues\n",
    "            if len(eigenvalues) > 2:\n",
    "                eigenvalue_gaps = eigenvalues[:-1] - eigenvalues[1:]\n",
    "                largest_gap_idx = np.argmax(eigenvalue_gaps)\n",
    "                bulk_edge = (eigenvalues[largest_gap_idx] + eigenvalues[largest_gap_idx + 1]) / 2\n",
    "            else:\n",
    "                bulk_edge = eigenvalues[-1]\n",
    "\n",
    "            # Calculate bulk statistics (excluding potential outliers)\n",
    "            bulk_eigenvalues = eigenvalues[eigenvalues < bulk_edge]\n",
    "            if len(bulk_eigenvalues) > 1:\n",
    "                bulk_mean = float(np.mean(bulk_eigenvalues))\n",
    "                bulk_std = float(np.std(bulk_eigenvalues))\n",
    "            else:\n",
    "                bulk_mean = float(np.mean(eigenvalues))\n",
    "                bulk_std = float(np.std(eigenvalues))\n",
    "\n",
    "            # Return all metrics as float\n",
    "            return {\n",
    "                \"eigenvalues\": eigenvalues.tolist()[:10],  # Keep only top 10 eigenvalues to save memory\n",
    "                \"largest_eigenvalue\": float(eigenvalues[0]) if len(eigenvalues) > 0 else 0,\n",
    "                \"spectral_gap\": float(spectral_gap),\n",
    "                \"participation_ratio\": float(participation_ratio),\n",
    "                \"stable_rank\": float(stable_rank),\n",
    "                \"spectral_norm\": float(spectral_norm),\n",
    "                \"bulk_edge\": float(bulk_edge),\n",
    "                \"bulk_mean\": bulk_mean,\n",
    "                \"bulk_std\": bulk_std\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log(f\"Error in eigenspectrum computation: {str(e)}\")\n",
    "            if self.verbose:\n",
    "                traceback.print_exc()\n",
    "            return {\n",
    "                \"eigenvalues\": [],\n",
    "                \"largest_eigenvalue\": 0.0,\n",
    "                \"spectral_gap\": 0.0,\n",
    "                \"participation_ratio\": 0.0,\n",
    "                \"stable_rank\": 0.0,\n",
    "                \"spectral_norm\": 0.0,\n",
    "                \"bulk_edge\": 0.0,\n",
    "                \"bulk_mean\": 0.0,\n",
    "                \"bulk_std\": 0.0,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    def compute_marchenko_pastur_distance(self, eigenvalues, aspect_ratio=1.0):\n",
    "        \"\"\"\n",
    "        Compute the distance between the empirical eigenvalue distribution\n",
    "        and the Marchenko-Pastur law.\n",
    "        Simplified and optimized version.\n",
    "\n",
    "        Args:\n",
    "            eigenvalues: List of eigenvalues\n",
    "            aspect_ratio: Ratio of rows to columns in the matrix\n",
    "\n",
    "        Returns:\n",
    "            KL divergence metric between distributions\n",
    "        \"\"\"\n",
    "        if not eigenvalues or len(eigenvalues) < 2:\n",
    "            return {\"kl_divergence\": float('nan')}\n",
    "\n",
    "        try:\n",
    "            # Normalize eigenvalues to have mean 1\n",
    "            eigenvalues = np.array(eigenvalues)\n",
    "            normalized_eigenvalues = eigenvalues / np.mean(eigenvalues)\n",
    "\n",
    "            # Create empirical distribution\n",
    "            hist, bin_edges = np.histogram(normalized_eigenvalues, bins=20, density=True)\n",
    "            bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "            # Marchenko-Pastur density function\n",
    "            def mp_density(x, gamma=aspect_ratio):\n",
    "                if gamma == 1:\n",
    "                    gamma = 0.99  # Avoid singularity at gamma=1\n",
    "\n",
    "                # Parameters for MP distribution\n",
    "                a = (1 - np.sqrt(gamma))**2\n",
    "                b = (1 + np.sqrt(gamma))**2\n",
    "\n",
    "                # MP density\n",
    "                result = np.zeros_like(x, dtype=float)\n",
    "                mask = (x >= a) & (x <= b)\n",
    "                result[mask] = np.sqrt((b - x[mask]) * (x[mask] - a)) / (2 * np.pi * gamma * x[mask])\n",
    "                return result\n",
    "\n",
    "            # Generate MP distribution for the given bins\n",
    "            mp_dist = mp_density(bin_centers)\n",
    "\n",
    "            # Normalize distributions for comparison\n",
    "            hist_normalized = hist / np.sum(hist)\n",
    "            mp_dist_normalized = mp_dist / np.sum(mp_dist)\n",
    "\n",
    "            # Compute KL divergence from MP distribution\n",
    "            kl_div = stats.entropy(hist_normalized, mp_dist_normalized)\n",
    "\n",
    "            return {\"kl_divergence\": float(kl_div)}\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log(f\"Error in Marchenko-Pastur calculation: {str(e)}\")\n",
    "            if self.verbose:\n",
    "                traceback.print_exc()\n",
    "            return {\"kl_divergence\": float('nan')}\n",
    "\n",
    "    def analyze_attention_matrix(self, attention_matrix, layer_idx, head_idx):\n",
    "        \"\"\"\n",
    "        Analyze a single attention matrix.\n",
    "        Optimized version with minimal metric calculation.\n",
    "\n",
    "        Args:\n",
    "            attention_matrix: Attention matrix to analyze\n",
    "            layer_idx: Layer index\n",
    "            head_idx: Head index\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with analysis results\n",
    "        \"\"\"\n",
    "        seq_len = attention_matrix.shape[0]\n",
    "\n",
    "        # Check for attention sinks (tokens receiving high total attention)\n",
    "        col_sums = np.sum(attention_matrix, axis=0)  # Sum over source tokens\n",
    "\n",
    "        # Identify potential sinks \n",
    "        sink_threshold = np.percentile(col_sums, 90)\n",
    "        sink_indices = np.where(col_sums >= sink_threshold)[0]\n",
    "\n",
    "        # Calculate attention concentration\n",
    "        sink_attention = np.sum(col_sums[sink_indices])\n",
    "        total_attention = np.sum(col_sums)\n",
    "        sink_concentration = sink_attention / total_attention if total_attention > 0 else 0\n",
    "\n",
    "        # Calculate entropy of the attention distribution\n",
    "        row_entropies = []\n",
    "        for i in range(seq_len):\n",
    "            row = attention_matrix[i]\n",
    "            # Avoid log(0) by adding a small epsilon\n",
    "            entropy_i = -np.sum(row * np.log2(row + 1e-10))\n",
    "            row_entropies.append(entropy_i)\n",
    "\n",
    "        avg_entropy = float(np.mean(row_entropies))\n",
    "\n",
    "        # Compute eigenspectrum\n",
    "        eigenspectrum = self.compute_eigenspectrum(attention_matrix)\n",
    "\n",
    "        # Compute distance from Marchenko-Pastur distribution\n",
    "        mp_distance = self.compute_marchenko_pastur_distance(\n",
    "            eigenspectrum[\"eigenvalues\"],\n",
    "            aspect_ratio=1.0  # Assuming square attention matrices\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"sink_concentration\": float(sink_concentration),\n",
    "            \"entropy\": avg_entropy,\n",
    "            \"spectral_gap\": eigenspectrum[\"spectral_gap\"],\n",
    "            \"participation_ratio\": eigenspectrum[\"participation_ratio\"],\n",
    "            \"stable_rank\": eigenspectrum[\"stable_rank\"],\n",
    "            \"marchenko_pastur_kl\": mp_distance[\"kl_divergence\"]\n",
    "        }\n",
    "\n",
    "    def analyze_text(self, text, step):\n",
    "        \"\"\"\n",
    "        Analyze a text sample through the model.\n",
    "        Optimized for speed with minimal outputs.\n",
    "\n",
    "        Args:\n",
    "            text: Text to analyze\n",
    "            step: Checkpoint step being analyzed\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with results for each layer and head\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            text_preview = text[:50] + (\"...\" if len(text) > 50 else \"\")\n",
    "            self.log(f\"Analyzing text at step {step}: {text_preview}\")\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                **inputs,\n",
    "                output_attentions=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "        attentions = outputs.attentions  # tuple of (layer, batch, head, seq_len, seq_len)\n",
    "\n",
    "        results = {\n",
    "            \"layers\": {}\n",
    "        }\n",
    "\n",
    "        # Process each layer and head\n",
    "        for layer_idx, layer_attention in enumerate(attentions):\n",
    "            # Each layer has shape (batch, head, seq_len, seq_len)\n",
    "            layer_attention = layer_attention[0].cpu().numpy()  # shape: (head, seq_len, seq_len)\n",
    "\n",
    "            # Get number of heads and sequence length\n",
    "            num_heads, seq_len, _ = layer_attention.shape\n",
    "            if self.verbose:\n",
    "                self.log(f\"Processing layer {layer_idx} with {num_heads} heads, sequence length {seq_len}\")\n",
    "\n",
    "            # Store layer info\n",
    "            results[\"layers\"][str(layer_idx)] = {\n",
    "                \"heads\": {},\n",
    "                \"avg_head\": {}\n",
    "            }\n",
    "\n",
    "            # Process each attention head\n",
    "            for head_idx in range(num_heads):\n",
    "                # Get attention matrix for this head\n",
    "                attention_matrix = layer_attention[head_idx]\n",
    "\n",
    "                # Analyze this attention matrix\n",
    "                head_results = self.analyze_attention_matrix(\n",
    "                    attention_matrix, layer_idx, head_idx\n",
    "                )\n",
    "\n",
    "                results[\"layers\"][str(layer_idx)][\"heads\"][str(head_idx)] = head_results\n",
    "\n",
    "            avg_attention = np.mean(layer_attention, axis=0)\n",
    "\n",
    "            avg_results = self.analyze_attention_matrix(\n",
    "                avg_attention, layer_idx, \"avg\"\n",
    "            )\n",
    "\n",
    "            results[\"layers\"][str(layer_idx)][\"avg_head\"] = avg_results\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze_checkpoint(self, step, texts):\n",
    "        \"\"\"\n",
    "        Analyze a specific checkpoint across multiple text samples.\n",
    "        Optimized to skip intermediate file outputs.\n",
    "\n",
    "        Args:\n",
    "            step: Checkpoint step to analyze (e.g., \"step0\", \"step1000\")\n",
    "            texts: List of text samples to analyze\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with aggregated results\n",
    "        \"\"\"\n",
    "        self.log(f\"Analyzing checkpoint {step} with {len(texts)} text samples...\", force=True)\n",
    "\n",
    "        self.load_model_checkpoint(step=step)\n",
    "\n",
    "        checkpoint_results = []\n",
    "        for i, text in enumerate(tqdm(texts, desc=f\"Processing texts for {step}\", disable=not self.verbose)):\n",
    "            try:\n",
    "                result = self.analyze_text(text, step)\n",
    "                checkpoint_results.append(result)\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log(f\"Error analyzing text {i+1}: {e}\", force=True)\n",
    "                if self.verbose:\n",
    "                    traceback.print_exc()\n",
    "\n",
    "        aggregated = self.aggregate_results(checkpoint_results, step)\n",
    "\n",
    "        return {\n",
    "            \"step\": step,\n",
    "            \"aggregated_results\": aggregated\n",
    "        }\n",
    "\n",
    "    def aggregate_results(self, all_results, step):\n",
    "        \"\"\"\n",
    "        Aggregate results from multiple texts for a checkpoint.\n",
    "        Optimized to focus only on metrics needed for the final report.\n",
    "\n",
    "        Args:\n",
    "            all_results: List of results from analyze_text\n",
    "            step: Checkpoint step\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with aggregated statistics\n",
    "        \"\"\"\n",
    "        self.log(f\"Aggregating results for checkpoint {step}...\")\n",
    "\n",
    "        if not all_results:\n",
    "            self.log(\"No results to aggregate!\", force=True)\n",
    "            return {}\n",
    "\n",
    "        first_result = all_results[0]\n",
    "        layer_indices = sorted([int(idx) for idx in first_result[\"layers\"].keys()])\n",
    "\n",
    "        head_indices = []\n",
    "        if layer_indices:\n",
    "            first_layer = first_result[\"layers\"][str(layer_indices[0])]\n",
    "            head_indices = sorted([int(idx) for idx in first_layer[\"heads\"].keys()])\n",
    "\n",
    "        self.log(f\"Found {len(layer_indices)} layers and {len(head_indices)} heads to aggregate\")\n",
    "\n",
    "        aggregated = {\n",
    "            \"step\": step,\n",
    "            \"layer_stats\": {},\n",
    "            \"rmt_trends\": {\n",
    "                \"spectral_gap\": [],\n",
    "                \"participation_ratio\": [],\n",
    "                \"stable_rank\": [],\n",
    "                \"marchenko_pastur_kl\": [],\n",
    "                \"entropy\": [],\n",
    "                \"sink_concentration\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for layer_idx in layer_indices:\n",
    "            layer_key = str(layer_idx)\n",
    "            layer_stats = {\"avg_head\": {}}\n",
    "\n",
    "            avg_metrics = {\n",
    "                \"spectral_gap\": [],\n",
    "                \"participation_ratio\": [],\n",
    "                \"stable_rank\": [],\n",
    "                \"marchenko_pastur_kl\": [],\n",
    "                \"entropy\": [],\n",
    "                \"sink_concentration\": []\n",
    "            }\n",
    "\n",
    "            for result in all_results:\n",
    "                if layer_key not in result[\"layers\"]:\n",
    "                    continue\n",
    "\n",
    "                layer_data = result[\"layers\"][layer_key]\n",
    "                if \"avg_head\" not in layer_data:\n",
    "                    continue\n",
    "\n",
    "                avg_data = layer_data[\"avg_head\"]\n",
    "\n",
    "                for metric in avg_metrics.keys():\n",
    "                    if metric in avg_data:\n",
    "                        avg_metrics[metric].append(avg_data[metric])\n",
    "\n",
    "            # Compute statistics for each metric\n",
    "            avg_head_stats = {}\n",
    "            for metric_name, values in avg_metrics.items():\n",
    "                if values:\n",
    "                    valid_values = [v for v in values if not (np.isnan(v) or np.isinf(v))]\n",
    "                    if valid_values:\n",
    "                        avg_head_stats[metric_name] = {\n",
    "                            \"mean\": float(np.nanmean(valid_values)),\n",
    "                            \"std\": float(np.nanstd(valid_values)) if len(valid_values) > 1 else 0.0,\n",
    "                            \"min\": float(np.nanmin(valid_values)),\n",
    "                            \"max\": float(np.nanmax(valid_values)),\n",
    "                            \"count\": len(valid_values)\n",
    "                        }\n",
    "\n",
    "            layer_stats[\"avg_head\"] = avg_head_stats\n",
    "\n",
    "            aggregated[\"layer_stats\"][layer_key] = layer_stats\n",
    "\n",
    "            for trend_key in aggregated[\"rmt_trends\"].keys():\n",
    "                if trend_key in avg_head_stats and \"mean\" in avg_head_stats[trend_key]:\n",
    "                    aggregated[\"rmt_trends\"][trend_key].append({\n",
    "                        \"layer\": layer_idx,\n",
    "                        \"value\": avg_head_stats[trend_key][\"mean\"],\n",
    "                        \"std\": avg_head_stats[trend_key][\"std\"]\n",
    "                    })\n",
    "\n",
    "        return aggregated\n",
    "\n",
    "    def compare_checkpoints(self, checkpoint_results):\n",
    "        \"\"\"\n",
    "        Compare results between different checkpoints.\n",
    "        Simplified to focus only on metrics needed for the final report.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_results: Dictionary of results for each checkpoint\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with comparison metrics\n",
    "        \"\"\"\n",
    "        self.log(\"Comparing checkpoints...\")\n",
    "\n",
    "        steps = sorted(checkpoint_results.keys())\n",
    "        if len(steps) < 2:\n",
    "            self.log(\"Need at least two checkpoints to compare!\", force=True)\n",
    "            return {}\n",
    "\n",
    "        comparisons = {\n",
    "            \"steps\": steps,\n",
    "            \"metrics\": {}\n",
    "        }\n",
    "\n",
    "        metrics_to_compare = [\n",
    "            \"spectral_gap\",\n",
    "            \"participation_ratio\",\n",
    "            \"stable_rank\",\n",
    "            \"marchenko_pastur_kl\",\n",
    "            \"entropy\",\n",
    "            \"sink_concentration\"\n",
    "        ]\n",
    "\n",
    "        # For each metric, track changes across checkpoints\n",
    "        for metric in metrics_to_compare:\n",
    "            metric_data = {}\n",
    "\n",
    "            # Get data for all steps\n",
    "            for step in steps:\n",
    "                if step in checkpoint_results and \"aggregated_results\" in checkpoint_results[step]:\n",
    "                    agg_results = checkpoint_results[step][\"aggregated_results\"]\n",
    "                    if \"rmt_trends\" in agg_results and metric in agg_results[\"rmt_trends\"]:\n",
    "                        metric_data[step] = agg_results[\"rmt_trends\"][metric]\n",
    "\n",
    "            # Calculate overall changes\n",
    "            if len(metric_data) >= 2:\n",
    "                first_step = steps[0]\n",
    "                last_step = steps[-1]\n",
    "\n",
    "                if first_step in metric_data and last_step in metric_data:\n",
    "                    # Get layer-wise values for first and last step\n",
    "                    first_values = {item[\"layer\"]: item[\"value\"] for item in metric_data[first_step]}\n",
    "                    last_values = {item[\"layer\"]: item[\"value\"] for item in metric_data[last_step]}\n",
    "\n",
    "                    # Find common layers\n",
    "                    common_layers = sorted(set(first_values.keys()) & set(last_values.keys()))\n",
    "\n",
    "                    if common_layers:\n",
    "                        # Calculate absolute and percentage changes\n",
    "                        abs_changes = [last_values[layer] - first_values[layer] for layer in common_layers]\n",
    "\n",
    "                        # Calculate percentage changes safely\n",
    "                        pct_changes = []\n",
    "                        for layer in common_layers:\n",
    "                            if first_values[layer] != 0:\n",
    "                                pct_change = (last_values[layer] - first_values[layer]) / abs(first_values[layer]) * 100\n",
    "                            else:\n",
    "                                pct_change = float('inf') if last_values[layer] > 0 else float('-inf') if last_values[layer] < 0 else 0\n",
    "                            pct_changes.append(pct_change)\n",
    "\n",
    "                        comparisons[\"metrics\"][metric] = {\n",
    "                            \"common_layers\": common_layers,\n",
    "                            \"first_step_values\": [first_values[layer] for layer in common_layers],\n",
    "                            \"last_step_values\": [last_values[layer] for layer in common_layers],\n",
    "                            \"absolute_changes\": abs_changes,\n",
    "                            \"percentage_changes\": pct_changes,\n",
    "                            \"avg_absolute_change\": float(np.nanmean(abs_changes)),\n",
    "                            \"avg_percentage_change\": float(np.nanmean([p for p in pct_changes if not np.isinf(p)]))\n",
    "                        }\n",
    "\n",
    "        return comparisons\n",
    "\n",
    "    def generate_report(self, checkpoint_results, comparison_results):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive text report summarizing the findings.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_results: Dictionary of results for each checkpoint\n",
    "            comparison_results: Results of comparing checkpoints\n",
    "\n",
    "        Returns:\n",
    "            Path to the saved report\n",
    "        \"\"\"\n",
    "        self.log(\"Generating comprehensive report...\", force=True)\n",
    "\n",
    "        report_path = os.path.join(self.output_dir, \"rmt_analysis_report.txt\")\n",
    "\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"RANDOM MATRIX THEORY ANALYSIS OF ATTENTION MATRICES\\n\")\n",
    "            f.write(\"===============================================\\n\\n\")\n",
    "\n",
    "            f.write(f\"Model: {self.model_size}\\n\")\n",
    "\n",
    "            steps = sorted(checkpoint_results.keys())\n",
    "            f.write(f\"Checkpoints analyzed: {', '.join(steps)}\\n\\n\")\n",
    "\n",
    "            if len(steps) >= 2:\n",
    "                first_step = steps[0]\n",
    "                last_step = steps[-1]\n",
    "\n",
    "                f.write(f\"COMPARISON FROM {first_step} TO {last_step}\\n\")\n",
    "                f.write(\"--------------------------------\\n\\n\")\n",
    "\n",
    "                if \"metrics\" in comparison_results and \"spectral_gap\" in comparison_results[\"metrics\"]:\n",
    "                    spectral_gap_data = comparison_results[\"metrics\"][\"spectral_gap\"]\n",
    "                    avg_sg_change = spectral_gap_data.get(\"avg_absolute_change\", 0)\n",
    "\n",
    "                    f.write(\"SPECTRAL GAP\\n\")\n",
    "                    f.write(f\"Average change: {avg_sg_change:.4f}\\n\")\n",
    "                    if avg_sg_change > 0:\n",
    "                        f.write(\"Spectral gap INCREASED during training, suggesting the emergence of\\n\")\n",
    "                        f.write(\"more dominant eigenvalues and more structured attention patterns.\\n\")\n",
    "                        f.write(\"This indicates attention becoming less random and more organized.\\n\\n\")\n",
    "                    else:\n",
    "                        f.write(\"Spectral gap DECREASED during training, suggesting attention patterns\\n\")\n",
    "                        f.write(\"becoming more diffuse without strong dominant directions.\\n\\n\")\n",
    "\n",
    "                if \"metrics\" in comparison_results and \"participation_ratio\" in comparison_results[\"metrics\"]:\n",
    "                    pr_data = comparison_results[\"metrics\"][\"participation_ratio\"]\n",
    "                    avg_pr_change = pr_data.get(\"avg_absolute_change\", 0)\n",
    "\n",
    "                    f.write(\"PARTICIPATION RATIO\\n\")\n",
    "                    f.write(f\"Average change: {avg_pr_change:.4f}\\n\")\n",
    "                    if avg_pr_change < 0:\n",
    "                        f.write(\"Participation ratio DECREASED during training, indicating attention\\n\")\n",
    "                        f.write(\"becoming more concentrated in fewer dimensions. This is consistent\\n\")\n",
    "                        f.write(\"with the emergence of attention sinks and more structured patterns.\\n\\n\")\n",
    "                    else:\n",
    "                        f.write(\"Participation ratio INCREASED during training, suggesting attention\\n\")\n",
    "                        f.write(\"remains distributed across many dimensions without strong concentration.\\n\\n\")\n",
    "\n",
    "                if \"metrics\" in comparison_results and \"entropy\" in comparison_results[\"metrics\"]:\n",
    "                    entropy_data = comparison_results[\"metrics\"][\"entropy\"]\n",
    "                    avg_entropy_change = entropy_data.get(\"avg_absolute_change\", 0)\n",
    "\n",
    "                    f.write(\"ATTENTION ENTROPY\\n\")\n",
    "                    f.write(f\"Average change: {avg_entropy_change:.4f}\\n\")\n",
    "                    if avg_entropy_change < 0:\n",
    "                        f.write(\"Attention entropy DECREASED during training, indicating attention\\n\")\n",
    "                        f.write(\"becoming more concentrated and less uniform. This suggests the\\n\")\n",
    "                        f.write(\"development of more focused attention patterns.\\n\\n\")\n",
    "                    else:\n",
    "                        f.write(\"Attention entropy INCREASED during training, suggesting attention\\n\")\n",
    "                        f.write(\"becoming more uniformly distributed across tokens.\\n\\n\")\n",
    "\n",
    "                if \"metrics\" in comparison_results and \"sink_concentration\" in comparison_results[\"metrics\"]:\n",
    "                    sink_data = comparison_results[\"metrics\"][\"sink_concentration\"]\n",
    "                    avg_sink_change = sink_data.get(\"avg_absolute_change\", 0)\n",
    "\n",
    "                    f.write(\"SINK CONCENTRATION\\n\")\n",
    "                    f.write(f\"Average change: {avg_sink_change:.4f}\\n\")\n",
    "                    if avg_sink_change > 0:\n",
    "                        f.write(\"Sink concentration INCREASED during training, confirming the emergence\\n\")\n",
    "                        f.write(\"of attention sinks where specific tokens receive disproportionate attention.\\n\")\n",
    "                        f.write(\"This is a key feature of trained transformer models.\\n\\n\")\n",
    "                    else:\n",
    "                        f.write(\"Sink concentration DECREASED or remained stable during training,\\n\")\n",
    "                        f.write(\"suggesting less pronounced attention sinks in this model.\\n\\n\")\n",
    "\n",
    "            f.write(\"\\nLAYER-SPECIFIC ANALYSIS\\n\")\n",
    "            f.write(\"----------------------\\n\\n\")\n",
    "\n",
    "            first_step = steps[0]\n",
    "            last_step = steps[-1]\n",
    "\n",
    "            if first_step in checkpoint_results and \"aggregated_results\" in checkpoint_results[first_step]:\n",
    "                first_agg = checkpoint_results[first_step][\"aggregated_results\"]\n",
    "                last_agg = checkpoint_results[last_step][\"aggregated_results\"] if last_step in checkpoint_results else None\n",
    "\n",
    "                if \"layer_stats\" in first_agg:\n",
    "                    layer_indices = sorted([int(idx) for idx in first_agg[\"layer_stats\"].keys()])\n",
    "\n",
    "                    for layer_idx in layer_indices:\n",
    "                        layer_key = str(layer_idx)\n",
    "                        f.write(f\"\\nLayer {layer_idx}:\\n\")\n",
    "\n",
    "                        if layer_key in first_agg[\"layer_stats\"] and \"avg_head\" in first_agg[\"layer_stats\"][layer_key]:\n",
    "                            first_metrics = first_agg[\"layer_stats\"][layer_key][\"avg_head\"]\n",
    "                            last_metrics = None\n",
    "\n",
    "                            if last_agg and \"layer_stats\" in last_agg and layer_key in last_agg[\"layer_stats\"]:\n",
    "                                if \"avg_head\" in last_agg[\"layer_stats\"][layer_key]:\n",
    "                                    last_metrics = last_agg[\"layer_stats\"][layer_key][\"avg_head\"]\n",
    "\n",
    "                            for metric in [\"spectral_gap\", \"participation_ratio\", \"entropy\", \"sink_concentration\"]:\n",
    "                                if metric in first_metrics and \"mean\" in first_metrics[metric]:\n",
    "                                    first_val = first_metrics[metric][\"mean\"]\n",
    "                                    last_val = \"N/A\"\n",
    "                                    change = \"N/A\"\n",
    "\n",
    "                                    if last_metrics and metric in last_metrics and \"mean\" in last_metrics[metric]:\n",
    "                                        last_val = last_metrics[metric][\"mean\"]\n",
    "                                        change = last_val - first_val\n",
    "\n",
    "                                    f.write(f\"  {metric.replace('_', ' ').title()}: {first_val:.4f} -> {last_val if last_val == 'N/A' else f'{last_val:.4f}'}\")\n",
    "                                    if change != \"N/A\":\n",
    "                                        direction = \"↑\" if change > 0 else \"↓\" if change < 0 else \"=\"\n",
    "                                        f.write(f\" {direction} ({change:.4f})\")\n",
    "                                    f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"\\nPERFORMANCE METRICS\\n\")\n",
    "            f.write(\"------------------\\n\")\n",
    "            f.write(f\"Checkpoints analyzed: {len(steps)}\\n\")\n",
    "\n",
    "            total_samples = 0\n",
    "            for step in steps:\n",
    "                if step in checkpoint_results and \"aggregated_results\" in checkpoint_results[step]:\n",
    "                    if \"layer_stats\" in checkpoint_results[step][\"aggregated_results\"]:\n",
    "                        layer_stats = checkpoint_results[step][\"aggregated_results\"][\"layer_stats\"]\n",
    "                        if layer_stats and len(layer_stats) > 0:\n",
    "                            first_layer = next(iter(layer_stats.values()))\n",
    "                            if \"avg_head\" in first_layer and first_layer[\"avg_head\"]:\n",
    "                                first_metric = next(iter(first_layer[\"avg_head\"].values()))\n",
    "                                if \"count\" in first_metric:\n",
    "                                    total_samples += first_metric[\"count\"]\n",
    "                                    break\n",
    "\n",
    "            f.write(f\"Total samples processed: {total_samples}\\n\")\n",
    "\n",
    "        self.log(f\"Analysis report saved to {report_path}\", force=True)\n",
    "        return report_path\n",
    "\n",
    "    def run_analysis(self, checkpoint_steps=None, texts=None):\n",
    "        \"\"\"\n",
    "        Run the complete analysis pipeline on multiple checkpoints.\n",
    "        Optimized to focus only on generating the final report.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_steps: List of checkpoint steps to analyze (e.g., [\"step0\", \"step143000\"])\n",
    "            texts: List of text samples to analyze (if None, example texts are used)\n",
    "\n",
    "        Returns:\n",
    "            Path to the generated report\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if checkpoint_steps is None:\n",
    "            checkpoint_steps = [\"step0\", \"step17000\", \"step72000\", \"step143000\"]\n",
    "\n",
    "        # Use example texts if none provided\n",
    "        if texts is None or not texts:\n",
    "            texts = [\n",
    "                \"The theory of random matrices has applications in many fields including physics, statistics, and data analysis.\",\n",
    "                \"Eigenvalues of large random matrices tend to follow specific distributions that depend on the matrix ensemble.\",\n",
    "                \"The eigenvectors of random matrices show interesting localization properties as dimensions increase.\",\n",
    "                \"Spectral analysis of matrices can reveal structural properties that are not immediately obvious.\"\n",
    "            ]\n",
    "\n",
    "        # Analyze each checkpoint\n",
    "        checkpoint_results = {}\n",
    "        for step in checkpoint_steps:\n",
    "            try:\n",
    "                self.log(f\"\\n{'='*60}\\nAnalyzing checkpoint: {step}\\n{'='*60}\\n\", force=True)\n",
    "                results = self.analyze_checkpoint(step, texts)\n",
    "                checkpoint_results[step] = results\n",
    "            except Exception as e:\n",
    "                self.log(f\"Error analyzing checkpoint {step}: {e}\", force=True)\n",
    "                if self.verbose:\n",
    "                    traceback.print_exc()\n",
    "                continue  # Continue with the next checkpoint even if this one fails\n",
    "\n",
    "        # Compare checkpoints (only if we have more than one)\n",
    "        if len(checkpoint_results) > 1:\n",
    "            self.log(\"\\nComparing checkpoints...\", force=True)\n",
    "            comparison_results = self.compare_checkpoints(checkpoint_results)\n",
    "\n",
    "            report_path = self.generate_report(checkpoint_results, comparison_results)\n",
    "        else:\n",
    "            self.log(\"Not enough checkpoints to compare. Need at least two.\", force=True)\n",
    "            report_path = None\n",
    "\n",
    "        results_file = os.path.join(self.output_dir, \"aggregated_results.json\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            serializable_results = {}\n",
    "            for step, results in checkpoint_results.items():\n",
    "                if \"aggregated_results\" in results:\n",
    "                    serializable_results[step] = results[\"aggregated_results\"]\n",
    "            json.dump(serializable_results, f)\n",
    "        self.log(f\"Saved aggregated results to {results_file}\", force=True)\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        self.log(f\"Analysis completed in {total_time/60:.2f} minutes\", force=True)\n",
    "\n",
    "        return report_path\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "\n",
    "        if self.tokenizer is not None:\n",
    "            del self.tokenizer\n",
    "            self.tokenizer = None\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.log(\"Resources cleaned up\", force=True)\n",
    "\n",
    "\n",
    "def get_sample_texts_from_dataset(dataset_path, n_samples=50):\n",
    "    \"\"\"\n",
    "    Extract sample texts from a dataset for analysis.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the dataset CSV\n",
    "        n_samples: Number of samples to extract\n",
    "\n",
    "    Returns:\n",
    "        List of text samples\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(dataset_path)\n",
    "        print(f\"Loaded dataset with {len(data)} rows\")\n",
    "\n",
    "        if 'text' not in data.columns:\n",
    "            print(\"Error: No 'text' column found in dataset\")\n",
    "            return []\n",
    "\n",
    "        if len(data) > n_samples:\n",
    "            samples = data.sample(n_samples)\n",
    "        else:\n",
    "            samples = data\n",
    "\n",
    "        texts = samples['text'].tolist()\n",
    "        return texts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83373f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Run the optimized analysis pipeline.\"\"\"\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        # Mount Google Drive if in Colab\n",
    "        drive.mount('/content/drive')\n",
    "        is_colab = True\n",
    "    except ImportError:\n",
    "        is_colab = False\n",
    "        print(\"Not running in Google Colab, skipping drive mount\")\n",
    "\n",
    "\n",
    "    model_size = \"pythia-2.8b\"  # Options: \"pythia-70m\", \"pythia-160m\", \"pythia-410m\", \"pythia-1b\", \"pythia-2.8b\", \"pythia-6.9b\", \"pythia-12b\"\n",
    "\n",
    "\n",
    "    # Full training is 143000 steps\n",
    "    checkpoint_steps = [\"step0\", \"step1\", \"step2\", \"step4\", \"step8\", \"step2000\", \"step143000\"]\n",
    "\n",
    "    # Path settings\n",
    "    if is_colab:\n",
    "        dataset_path = \"/content/drive/MyDrive/wiki_dataset_position.csv\"\n",
    "        output_dir = f\"/content/drive/MyDrive/rmt/{model_name.split('/')[-1]}\"\n",
    "    else:\n",
    "        dataset_path = \"./wiki_dataset_position.csv\"\n",
    "        output_dir = f\"./pythia_attention_rmt_{model_size}_optimized\"\n",
    "\n",
    "    num_samples = 100      \n",
    "    verbose = False       # Set to True for detailed progress logs\n",
    "\n",
    "    print(f\"Starting analysis with model: {model_size}\")\n",
    "    print(f\"Dataset path: {dataset_path}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"Analyzing checkpoints: {', '.join(checkpoint_steps)}\")\n",
    "    print(f\"Using {num_samples} samples\")\n",
    "\n",
    "    analyzer = OptimizedAttentionMatrixRMTAnalysis(\n",
    "        model_size=model_size,\n",
    "        output_dir=output_dir,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    texts = get_sample_texts_from_dataset(dataset_path, n_samples=num_samples)\n",
    "\n",
    "    if not texts:\n",
    "        print(\"No texts loaded from dataset. Using default examples.\")\n",
    "        texts = [\n",
    "            \"The theory of random matrices has applications in many fields including physics, statistics, and data analysis.\",\n",
    "            \"Eigenvalues of large random matrices tend to follow specific distributions that depend on the matrix ensemble.\",\n",
    "            \"The eigenvectors of random matrices show interesting localization properties as dimensions increase.\",\n",
    "            \"Spectral analysis of matrices can reveal structural properties that are not immediately obvious.\"\n",
    "        ]\n",
    "\n",
    "    try:\n",
    "        report_path = analyzer.run_analysis(\n",
    "            checkpoint_steps=checkpoint_steps,\n",
    "            texts=texts\n",
    "        )\n",
    "        print(f\"Analysis completed successfully! Report saved to: {report_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Analysis error: {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        analyzer.cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
