{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ef02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "class FisherInformationAnalysis:\n",
    "    \"\"\"\n",
    "    Simple analyzer for computing Fisher Information Matrix for transformer models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, model_type=\"decoder\", output_dir=\"./fisher_info\"):\n",
    "        \"\"\"\n",
    "        Initialize the Fisher Information analysis.\n",
    "        Args:\n",
    "            model_name: HuggingFace model name or path\n",
    "            model_type: Type of model - \"decoder\" or \"encoder\"\n",
    "            output_dir: Directory to save results\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.output_dir = output_dir\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Output will be saved to {output_dir}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the transformer model and tokenizer.\"\"\"\n",
    "        print(f\"Loading {self.model_name}...\")\n",
    "\n",
    "        # Clean up any existing model\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        try:\n",
    "            from transformers import AutoTokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "            if self.tokenizer.pad_token is None and self.model_type == \"decoder\":\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            if self.model_type == \"decoder\":\n",
    "                from transformers import AutoModelForCausalLM\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "            else:  # encoder models like BERT\n",
    "                from transformers import AutoModelForMaskedLM\n",
    "                self.model = AutoModelForMaskedLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "\n",
    "            print(f\"Successfully loaded {self.model_name}\")\n",
    "            return self.model, self.tokenizer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def compute_fisher_information(self, text_batch):\n",
    "        \"\"\"\n",
    "        Compute the Fisher Information Matrix for text samples.\n",
    "        Args:\n",
    "            text_batch: List of text samples\n",
    "        Returns:\n",
    "            Dictionary with Fisher Information analysis results\n",
    "        \"\"\"\n",
    "        print(f\"Computing Fisher Information for {len(text_batch)} samples...\")\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            text_batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(next(self.model.parameters()).device)\n",
    "\n",
    "        fisher_results = {\n",
    "            'layer_wise_fisher_norm': {},\n",
    "            'param_groups': {\n",
    "                'attention': {'norm': 0},\n",
    "                'mlp': {'norm': 0},\n",
    "                'embedding': {'norm': 0}\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Process one sample at a time\n",
    "        for i in range(len(text_batch)):\n",
    "            # Create mini-batch with single sample\n",
    "            mini_encodings = {k: v[i:i+1] for k, v in encodings.items()}\n",
    "\n",
    "            # Forward pass and loss computation\n",
    "            if self.model_type == \"decoder\":\n",
    "                outputs = self.model(\n",
    "                    input_ids=mini_encodings['input_ids'],\n",
    "                    attention_mask=mini_encodings['attention_mask'],\n",
    "                    labels=mini_encodings['input_ids'],\n",
    "                )\n",
    "            else:\n",
    "                # For encoder models, use masked language modeling\n",
    "                masked_input_ids = mini_encodings['input_ids'].clone()\n",
    "                mask_prob = torch.full(masked_input_ids.shape, 0.15)\n",
    "                masked_indices = torch.bernoulli(mask_prob).bool().to(masked_input_ids.device)\n",
    "                masked_indices = masked_indices & (mini_encodings['attention_mask'] == 1)\n",
    "\n",
    "                original_ids = masked_input_ids.clone()\n",
    "                masked_input_ids[masked_indices] = self.tokenizer.mask_token_id\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=masked_input_ids,\n",
    "                    attention_mask=mini_encodings['attention_mask'],\n",
    "                    labels=original_ids\n",
    "                )\n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            self.model.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Process the gradients to compute Fisher Information\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if torch.all(param.grad == 0):\n",
    "                        continue\n",
    "\n",
    "                    # Compute Fisher norm (squared L2 norm of gradient)\n",
    "                    grad = param.grad.detach()\n",
    "                    fisher_norm = grad.norm().item() ** 2\n",
    "\n",
    "                    # Extract layer name \n",
    "                    layer_name = \"other\"\n",
    "\n",
    "                    # Try to identify layer number\n",
    "                    if 'layers.' in name:\n",
    "                        parts = name.split('layers.')\n",
    "                        if len(parts) > 1:\n",
    "                            # Extract layer number\n",
    "                            layer_parts = parts[1].split('.')\n",
    "                            if layer_parts[0].isdigit():\n",
    "                                layer_name = f\"layer_{layer_parts[0]}\"\n",
    "                    elif 'layer.' in name:\n",
    "                        parts = name.split('layer.')\n",
    "                        if len(parts) > 1:\n",
    "                            layer_parts = parts[1].split('.')\n",
    "                            if layer_parts[0].isdigit():\n",
    "                                layer_name = f\"layer_{layer_parts[0]}\"\n",
    "\n",
    "                    # Create entry for this layer if it doesn't exist\n",
    "                    if layer_name not in fisher_results['layer_wise_fisher_norm']:\n",
    "                        fisher_results['layer_wise_fisher_norm'][layer_name] = 0\n",
    "\n",
    "                    # Update layer's Fisher norm\n",
    "                    fisher_results['layer_wise_fisher_norm'][layer_name] += fisher_norm / len(text_batch)\n",
    "\n",
    "                    # Categorize parameter by group\n",
    "                    if any(key in name.lower() for key in ['attention', 'attn', 'self', 'q_proj', 'k_proj', 'v_proj']):\n",
    "                        group = 'attention'\n",
    "                    elif any(key in name.lower() for key in ['mlp', 'feed_forward', 'ffn', 'dense', 'fc']):\n",
    "                        group = 'mlp'\n",
    "                    elif any(key in name.lower() for key in ['embed', 'token', 'wte', 'wpe']):\n",
    "                        group = 'embedding'\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    # Update parameter group's Fisher norm\n",
    "                    fisher_results['param_groups'][group]['norm'] += fisher_norm / len(text_batch)\n",
    "\n",
    "            self.model.zero_grad()\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            if (i+1) % 5 == 0 or i == len(text_batch)-1:\n",
    "                print(f\"Processed {i+1}/{len(text_batch)} samples\")\n",
    "\n",
    "        return fisher_results\n",
    "\n",
    "    def compute_correlations_and_pvalues(self, fisher_results):\n",
    "        \"\"\"\n",
    "        Compute correlations between different Fisher information metrics with p-values.\n",
    "\n",
    "        Args:\n",
    "            fisher_results: Dictionary with Fisher Information analysis results\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with correlation analyses and their p-values\n",
    "        \"\"\"\n",
    "        print(\"Computing correlations with p-values...\")\n",
    "        correlation_results = {\n",
    "            'layer_position_correlation': {},\n",
    "            'component_correlations': {},\n",
    "            'statistical_significance': {}\n",
    "        }\n",
    "\n",
    "        # 1. Correlation between layer position and Fisher norm\n",
    "        layer_positions = []\n",
    "        layer_norms = []\n",
    "\n",
    "        # Extract numeric layer indices and corresponding norms\n",
    "        for layer_name, norm in fisher_results['layer_wise_fisher_norm'].items():\n",
    "            if layer_name.startswith('layer_'):\n",
    "                try:\n",
    "                    layer_idx = int(layer_name.split('_')[1])\n",
    "                    layer_positions.append(layer_idx)\n",
    "                    layer_norms.append(norm)\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "\n",
    "        if len(layer_positions) >= 2:\n",
    "            # Calculate Pearson correlation coefficient and p-value\n",
    "            correlation, p_value = stats.pearsonr(layer_positions, layer_norms)\n",
    "\n",
    "            correlation_results['layer_position_correlation'] = {\n",
    "                'correlation': float(correlation),\n",
    "                'p_value': float(p_value),\n",
    "                'significant': bool(p_value < 0.05),  # Convert to Python bool\n",
    "                'sample_size': int(len(layer_positions))\n",
    "            }\n",
    "\n",
    "            spearman_corr, spearman_p = stats.spearmanr(layer_positions, layer_norms)\n",
    "            correlation_results['layer_position_correlation']['spearman_correlation'] = float(spearman_corr)\n",
    "            correlation_results['layer_position_correlation']['spearman_p_value'] = float(spearman_p)\n",
    "            correlation_results['layer_position_correlation']['spearman_significant'] = bool(spearman_p < 0.05)\n",
    "\n",
    "        # 2. Correlation between consecutive layers\n",
    "        if len(layer_positions) >= 3:  # Need at least 3 points for meaningful consecutive correlation\n",
    "            consecutive_correlations = []\n",
    "            consecutive_p_values = []\n",
    "\n",
    "            # Sort by layer position\n",
    "            sorted_indices = np.argsort(layer_positions)\n",
    "            sorted_positions = [layer_positions[i] for i in sorted_indices]\n",
    "            sorted_norms = [layer_norms[i] for i in sorted_indices]\n",
    "\n",
    "            # Calculate correlations between each layer and its neighbor\n",
    "            # Using a sliding window of 3 layers to get enough points for correlation\n",
    "            window_size = min(3, len(sorted_norms) - 1)\n",
    "\n",
    "            for i in range(len(sorted_norms) - window_size + 1):\n",
    "                window_positions = list(range(i, i + window_size))\n",
    "                window_norms = sorted_norms[i:i + window_size]\n",
    "\n",
    "                if len(set(window_norms)) > 1:  # Ensure variance in the data\n",
    "                    try:\n",
    "                        corr, p_val = stats.pearsonr(window_positions, window_norms)\n",
    "                        consecutive_correlations.append(corr)\n",
    "                        consecutive_p_values.append(p_val)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            if consecutive_correlations:\n",
    "                correlation_results['consecutive_layer_correlation'] = {\n",
    "                    'mean_correlation': float(np.mean(consecutive_correlations)),\n",
    "                    'mean_p_value': float(np.mean(consecutive_p_values)),\n",
    "                    'significant_ratio': float(sum(p < 0.05 for p in consecutive_p_values) / len(consecutive_p_values))\n",
    "                }\n",
    "\n",
    "        # 3. Correlations between parameter groups\n",
    "        param_groups = fisher_results['param_groups']\n",
    "        if len(param_groups) >= 2:\n",
    "            group_names = []\n",
    "            group_norms = []\n",
    "\n",
    "            for group_name, group_data in param_groups.items():\n",
    "                if 'norm' in group_data:\n",
    "                    group_names.append(group_name)\n",
    "                    group_norms.append(group_data['norm'])\n",
    "\n",
    "            # Compute correlation matrix and p-values between groups\n",
    "            if len(group_names) >= 2:\n",
    "                group_correlations = {}\n",
    "\n",
    "                for i in range(len(group_names)):\n",
    "                    for j in range(i+1, len(group_names)):\n",
    "                        group1 = group_names[i]\n",
    "                        group2 = group_names[j]\n",
    "\n",
    "                        group_correlations[f\"{group1}_vs_{group2}\"] = {\n",
    "                            'correlation': None,\n",
    "                            'p_value': None,\n",
    "                            'note': \"Need layer-wise breakdowns for meaningful correlation\"\n",
    "                        }\n",
    "\n",
    "                correlation_results['component_correlations'] = group_correlations\n",
    "\n",
    "        # 4. Statistical significance summary\n",
    "        significant_count = 0\n",
    "        total_tests = 0\n",
    "\n",
    "        # Count significant correlations in layer position correlation\n",
    "        if 'layer_position_correlation' in correlation_results and 'p_value' in correlation_results['layer_position_correlation']:\n",
    "            total_tests += 1\n",
    "            if correlation_results['layer_position_correlation']['p_value'] < 0.05:\n",
    "                significant_count += 1\n",
    "\n",
    "        # Count significant correlations in consecutive layer correlation\n",
    "        if 'consecutive_layer_correlation' in correlation_results and 'mean_p_value' in correlation_results['consecutive_layer_correlation']:\n",
    "            total_tests += 1\n",
    "            if correlation_results['consecutive_layer_correlation']['mean_p_value'] < 0.05:\n",
    "                significant_count += 1\n",
    "\n",
    "        correlation_results['statistical_significance'] = {\n",
    "            'significant_tests': int(significant_count), \n",
    "            'total_tests': int(total_tests),\n",
    "            'significance_ratio': float(significant_count / total_tests if total_tests > 0 else 0)\n",
    "        }\n",
    "\n",
    "        return correlation_results\n",
    "\n",
    "    def plot_correlation_analysis(self, fisher_results, correlation_results):\n",
    "        \"\"\"\n",
    "        Create plots for correlation analysis.\n",
    "\n",
    "        Args:\n",
    "            fisher_results: Dictionary with Fisher Information results\n",
    "            correlation_results: Dictionary with correlation analyses\n",
    "        \"\"\"\n",
    "        print(\"Creating correlation plots...\")\n",
    "\n",
    "        # 1. Plot layer position vs. Fisher norm\n",
    "        if 'layer_position_correlation' in correlation_results:\n",
    "            try:\n",
    "                layer_positions = []\n",
    "                layer_norms = []\n",
    "\n",
    "                for layer_name, norm in fisher_results['layer_wise_fisher_norm'].items():\n",
    "                    if layer_name.startswith('layer_'):\n",
    "                        try:\n",
    "                            layer_idx = int(layer_name.split('_')[1])\n",
    "                            layer_positions.append(layer_idx)\n",
    "                            layer_norms.append(norm)\n",
    "                        except (ValueError, IndexError):\n",
    "                            continue\n",
    "\n",
    "                if len(layer_positions) >= 2:\n",
    "                    sorted_indices = np.argsort(layer_positions)\n",
    "                    sorted_positions = [layer_positions[i] for i in sorted_indices]\n",
    "                    sorted_norms = [layer_norms[i] for i in sorted_indices]\n",
    "\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.scatter(sorted_positions, sorted_norms, marker='o', alpha=0.7)\n",
    "\n",
    "                    z = np.polyfit(sorted_positions, sorted_norms, 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    plt.plot(sorted_positions, p(sorted_positions), \"r--\", alpha=0.7)\n",
    "\n",
    "                    corr_info = correlation_results['layer_position_correlation']\n",
    "                    plt.title(f\"Layer Position vs. Fisher Norm\\nPearson r={corr_info['correlation']:.3f}, p={corr_info['p_value']:.4f}\")\n",
    "                    plt.xlabel(\"Layer Position\")\n",
    "                    plt.ylabel(\"Fisher Information Norm\")\n",
    "                    plt.grid(alpha=0.3)\n",
    "\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(self.output_dir, \"layer_position_correlation.png\"))\n",
    "                    plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating layer position correlation plot: {e}\")\n",
    "\n",
    "        # 2. Plot Fisher norm by parameter group\n",
    "        try:\n",
    "            param_groups = fisher_results['param_groups']\n",
    "            group_names = []\n",
    "            group_norms = []\n",
    "\n",
    "            for group_name, group_data in param_groups.items():\n",
    "                if 'norm' in group_data:\n",
    "                    group_names.append(group_name)\n",
    "                    group_norms.append(group_data['norm'])\n",
    "\n",
    "            if group_names:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.bar(group_names, group_norms)\n",
    "                plt.title(\"Fisher Information by Parameter Group\")\n",
    "                plt.ylabel(\"Fisher Information Norm\")\n",
    "                plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(self.output_dir, \"parameter_group_fisher.png\"))\n",
    "                plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating parameter group plot: {e}\")\n",
    "\n",
    "    def run_analysis(self, texts):\n",
    "        \"\"\"\n",
    "        Run the Fisher Information analysis pipeline.\n",
    "        Args:\n",
    "            texts: List of text samples to analyze\n",
    "        Returns:\n",
    "            Path to the generated report\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.load_model()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        fisher_results = self.compute_fisher_information(texts)\n",
    "\n",
    "        correlation_results = self.compute_correlations_and_pvalues(fisher_results)\n",
    "\n",
    "        self.plot_correlation_analysis(fisher_results, correlation_results)\n",
    "\n",
    "        report_path = os.path.join(self.output_dir, \"fisher_info_report.json\")\n",
    "\n",
    "        report = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"model_type\": self.model_type,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"parameter_groups\": fisher_results['param_groups'],\n",
    "            \"layer_metrics\": {\n",
    "                \"fisher_norm\": fisher_results['layer_wise_fisher_norm'],\n",
    "            },\n",
    "            \"total_fisher_norm\": sum(fisher_results['layer_wise_fisher_norm'].values()),\n",
    "            \"correlation_analysis\": correlation_results\n",
    "        }\n",
    "\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "        try:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            layers = list(fisher_results['layer_wise_fisher_norm'].keys())\n",
    "            values = list(fisher_results['layer_wise_fisher_norm'].values())\n",
    "\n",
    "            numeric_layers = []\n",
    "            for layer in layers:\n",
    "                if layer.startswith('layer_'):\n",
    "                    try:\n",
    "                        numeric_layers.append(int(layer.split('_')[1]))\n",
    "                    except:\n",
    "                        numeric_layers.append(999)  \n",
    "                else:\n",
    "                    numeric_layers.append(999)\n",
    "\n",
    "            sorted_indices = np.argsort(numeric_layers)\n",
    "            sorted_layers = [layers[i] for i in sorted_indices]\n",
    "            sorted_values = [values[i] for i in sorted_indices]\n",
    "\n",
    "            plt.bar(range(len(sorted_layers)), sorted_values)\n",
    "            plt.xticks(range(len(sorted_layers)), sorted_layers, rotation=90)\n",
    "            plt.title(f\"Layer-wise Fisher Information Norm\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, \"fisher_by_layer.png\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating plot: {e}\")\n",
    "\n",
    "        summary_path = os.path.join(self.output_dir, \"correlation_summary.txt\")\n",
    "        try:\n",
    "            with open(summary_path, 'w') as f:\n",
    "                f.write(f\"FISHER INFORMATION CORRELATION ANALYSIS\\n\")\n",
    "                f.write(f\"=====================================\\n\\n\")\n",
    "                f.write(f\"Model: {self.model_name}\\n\")\n",
    "                f.write(f\"Type: {self.model_type}\\n\")\n",
    "                f.write(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "                f.write(\"LAYER POSITION CORRELATION\\n\")\n",
    "                f.write(\"==========================\\n\")\n",
    "                if 'layer_position_correlation' in correlation_results:\n",
    "                    corr_data = correlation_results['layer_position_correlation']\n",
    "                    f.write(f\"Pearson correlation: {corr_data.get('correlation', 'N/A'):.4f}\\n\")\n",
    "                    f.write(f\"P-value: {corr_data.get('p_value', 'N/A'):.6f}\\n\")\n",
    "                    f.write(f\"Significant (p<0.05): {corr_data.get('significant', 'N/A')}\\n\")\n",
    "                    f.write(f\"Spearman correlation: {corr_data.get('spearman_correlation', 'N/A'):.4f}\\n\")\n",
    "                    f.write(f\"Spearman p-value: {corr_data.get('spearman_p_value', 'N/A'):.6f}\\n\\n\")\n",
    "                else:\n",
    "                    f.write(\"No correlation data available\\n\\n\")\n",
    "\n",
    "                f.write(\"CONSECUTIVE LAYER CORRELATION\\n\")\n",
    "                f.write(\"=============================\\n\")\n",
    "                if 'consecutive_layer_correlation' in correlation_results:\n",
    "                    consec_data = correlation_results['consecutive_layer_correlation']\n",
    "                    f.write(f\"Mean correlation: {consec_data.get('mean_correlation', 'N/A'):.4f}\\n\")\n",
    "                    f.write(f\"Mean p-value: {consec_data.get('mean_p_value', 'N/A'):.6f}\\n\")\n",
    "                    f.write(f\"Significant ratio: {consec_data.get('significant_ratio', 'N/A'):.2f}\\n\\n\")\n",
    "                else:\n",
    "                    f.write(\"No consecutive layer correlation data available\\n\\n\")\n",
    "\n",
    "                f.write(\"STATISTICAL SIGNIFICANCE SUMMARY\\n\")\n",
    "                f.write(\"===============================\\n\")\n",
    "                if 'statistical_significance' in correlation_results:\n",
    "                    sig_data = correlation_results['statistical_significance']\n",
    "                    f.write(f\"Significant tests: {sig_data.get('significant_tests', 0)}/{sig_data.get('total_tests', 0)}\\n\")\n",
    "                    f.write(f\"Significance ratio: {sig_data.get('significance_ratio', 0):.2f}\\n\\n\")\n",
    "                else:\n",
    "                    f.write(\"No statistical significance data available\\n\\n\")\n",
    "\n",
    "                f.write(\"PARAMETER GROUP COMPARISON\\n\")\n",
    "                f.write(\"=========================\\n\")\n",
    "                for group, data in fisher_results['param_groups'].items():\n",
    "                    f.write(f\"{group}: {data.get('norm', 0):.6f}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating summary text file: {e}\")\n",
    "\n",
    "        self.cleanup()\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Analysis completed in {total_time/60:.2f} minutes\")\n",
    "        print(f\"Report saved to {report_path}\")\n",
    "        print(f\"Correlation summary saved to {summary_path}\")\n",
    "\n",
    "        return report_path\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "\n",
    "        if self.tokenizer is not None:\n",
    "            del self.tokenizer\n",
    "            self.tokenizer = None\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Resources cleaned up\")\n",
    "\n",
    "\n",
    "def get_sample_texts(dataset_path, n_samples=10):\n",
    "    \"\"\"Get sample texts from a dataset.\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(dataset_path)\n",
    "        print(f\"Loaded dataset with {len(data)} rows\")\n",
    "\n",
    "        # Find text column\n",
    "        text_col = None\n",
    "        for col in data.columns:\n",
    "            if isinstance(data[col].iloc[0], str) and len(data[col].iloc[0]) > 20:\n",
    "                text_col = col\n",
    "                break\n",
    "\n",
    "        if text_col is None:\n",
    "            # Just use the first column that could be text\n",
    "            for col in data.columns:\n",
    "                if data[col].dtype == 'object':\n",
    "                    text_col = col\n",
    "                    break\n",
    "\n",
    "        if text_col is None:\n",
    "            print(\"No suitable text column found\")\n",
    "            return []\n",
    "\n",
    "        # Sample and get texts\n",
    "        if len(data) > n_samples:\n",
    "            samples = data.sample(n_samples)\n",
    "        else:\n",
    "            samples = data\n",
    "\n",
    "        texts = [str(t)[:500] for t in samples[text_col].tolist() if t is not None]\n",
    "        print(f\"Extracted {len(texts)} text samples\")\n",
    "        return texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        is_colab = True\n",
    "    except ImportError:\n",
    "        is_colab = False\n",
    "        print(\"Not running in Google Colab\")\n",
    "\n",
    "    model_name = \"EleutherAI/pythia-2.8b\" \n",
    "    model_type = \"decoder\"  # \"encoder\" for encoder only model, \"decoder\" for decoder\n",
    "\n",
    "    if is_colab:\n",
    "        dataset_path = \"/content/drive/MyDrive/wiki_dataset_position.csv\"\n",
    "        output_dir = f\"/content/drive/MyDrive/fisher/{model_name.split('/')[-1]}\"\n",
    "    else:\n",
    "        dataset_path = \"dataset.csv\"\n",
    "        output_dir = f\"fisher_info_{model_name.split('/')[-1]}\"\n",
    "\n",
    "    analyzer = FisherInformationAnalysis(\n",
    "        model_name=model_name,\n",
    "        model_type=model_type,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    texts = get_sample_texts(dataset_path, n_samples=500)\n",
    "\n",
    "    if not texts:\n",
    "        texts = [\n",
    "            \"The quick brown fox jumps over the lazy dog.\",\n",
    "            \"Machine learning models process data to make predictions.\",\n",
    "            \"Transformers have revolutionized natural language processing.\"\n",
    "        ]\n",
    "\n",
    "    analyzer.run_analysis(texts=texts)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
