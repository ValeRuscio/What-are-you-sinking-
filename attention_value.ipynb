{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d315a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats\n",
    "from google.colab import drive\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "class AttentionValueCorrelationAnalysis:\n",
    "    \"\"\"\n",
    "    Analyzes correlation patterns between attention distributions and\n",
    "    value transformations in transformer models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, output_dir=\"./attention_value_correlation\"):\n",
    "        \"\"\"Initialize the analysis with model name and output directory.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = output_dir\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.results = {}\n",
    "\n",
    "        self.hidden_states = None\n",
    "        self.attention_matrices = None\n",
    "\n",
    "        self.debug = True\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def load_model(self, use_4bit=True):\n",
    "        \"\"\"Load the model and tokenizer with robust error handling.\"\"\"\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "\n",
    "            # Check for 4-bit quantization\n",
    "            if use_4bit:\n",
    "                try:\n",
    "                    quantization_config = BitsAndBytesConfig(\n",
    "                        load_in_4bit=True,\n",
    "                        bnb_4bit_compute_dtype=torch.float16\n",
    "                    )\n",
    "\n",
    "                    self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                        self.model_name,\n",
    "                        torch_dtype=torch.float16,\n",
    "                        quantization_config=quantization_config,\n",
    "                        device_map=\"auto\",\n",
    "                        trust_remote_code=True,\n",
    "                        output_hidden_states=True\n",
    "                    )\n",
    "                    print(\"Model loaded with 4-bit quantization\")\n",
    "                except (ImportError, ModuleNotFoundError) as e:\n",
    "                    print(f\"BitsAndBytes not available: {e}\")\n",
    "                    print(\"Falling back to standard loading...\")\n",
    "                    self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                        self.model_name,\n",
    "                        torch_dtype=torch.float16,\n",
    "                        device_map=\"auto\",\n",
    "                        trust_remote_code=True,\n",
    "                        output_hidden_states=True\n",
    "                    )\n",
    "            else:\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"This might be due to an unsupported model architecture or outdated transformers library.\")\n",
    "            print(\"Try updating with: pip install --upgrade transformers\")\n",
    "            print(\"Or install from source: pip install git+https://github.com/huggingface/transformers.git\")\n",
    "            print(\"Using a fallback approach...\")\n",
    "            return None, None\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Model loaded successfully\")\n",
    "        return self.model, self.tokenizer\n",
    "\n",
    "    def extract_value_vectors(self, hidden_states, layer_idx):\n",
    "        \"\"\"\n",
    "        Extract value vectors for a specific layer.\n",
    "        This is a modified version that correctly handles the hidden states format\n",
    "        from different model architectures.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: list of tensors, hidden states for each layer\n",
    "            layer_idx: int, index of the layer\n",
    "\n",
    "        Returns:\n",
    "            numpy array of value vectors with shape (seq_len, hidden_dim)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            layer_hidden = hidden_states[layer_idx]\n",
    "\n",
    "            if isinstance(layer_hidden, torch.Tensor):\n",
    "                layer_hidden = layer_hidden.cpu().numpy()\n",
    "\n",
    "            # Ensure the right shape\n",
    "            # Expected shape: (batch_size, seq_len, hidden_dim)\n",
    "            if self.debug:\n",
    "                print(f\"Hidden state shape: {layer_hidden.shape}\")\n",
    "\n",
    "            # Handle different shape formats\n",
    "            if len(layer_hidden.shape) == 3:\n",
    "                if layer_hidden.shape[0] == 1:  # (batch_size, seq_len, hidden_dim)\n",
    "                    return layer_hidden[0]  # Return (seq_len, hidden_dim)\n",
    "                elif layer_hidden.shape[1] == 1:  # (seq_len, batch_size, hidden_dim)\n",
    "                    return layer_hidden[:, 0, :]  # Return (seq_len, hidden_dim)\n",
    "                else:\n",
    "                    # If batch dimension is neither 0 nor 1, use the first batch\n",
    "                    return layer_hidden[0]  # Assume (batch_size, seq_len, hidden_dim)\n",
    "            elif len(layer_hidden.shape) == 2:\n",
    "                # Already in the format (seq_len, hidden_dim)\n",
    "                return layer_hidden\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected hidden state shape: {layer_hidden.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in extract_value_vectors: {e}\")\n",
    "            traceback.print_exc()\n",
    "            # Return a dummy tensor as fallback\n",
    "            return np.zeros((1, 1))\n",
    "\n",
    "    def calculate_attention_entropy(self, attention_matrix):\n",
    "        \"\"\"\n",
    "        Calculate the entropy of attention distributions for each token.\n",
    "        Designed to handle extreme cases with special numerical stability measures.\n",
    "\n",
    "        Args:\n",
    "            attention_matrix: numpy array of shape (seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            numpy array of entropy values for each token\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use machine epsilon for maximum precision\n",
    "            eps = np.finfo(np.float32).eps * 100  \n",
    "            seq_len = attention_matrix.shape[0]\n",
    "            entropy = np.zeros(seq_len)\n",
    "\n",
    "            # Process each row individually \n",
    "            for i in range(seq_len):\n",
    "                attn_dist = attention_matrix[i]\n",
    "\n",
    "                # Replace zeros/negatives with epsilon to avoid log(0)\n",
    "                valid_indices = attn_dist > eps\n",
    "\n",
    "                if not np.any(valid_indices):\n",
    "                    entropy[i] = 0.0\n",
    "                    continue\n",
    "\n",
    "                # Normalize the valid attention weights\n",
    "                valid_attn = attn_dist[valid_indices]\n",
    "                valid_attn_sum = np.sum(valid_attn)\n",
    "\n",
    "                if valid_attn_sum < eps:\n",
    "                    entropy[i] = 0.0\n",
    "                    continue\n",
    "\n",
    "                normalized_attn = valid_attn / valid_attn_sum\n",
    "\n",
    "                # entropy for valid values\n",
    "                # H = -sum(p_i * log(p_i)) - only include terms where p_i > 0\n",
    "                valid_entropy = 0.0\n",
    "                for p in normalized_attn:\n",
    "                    if p > eps:  \n",
    "                        valid_entropy -= p * np.log(p)\n",
    "\n",
    "                entropy[i] = valid_entropy\n",
    "\n",
    "            # Handle any remaining NaN or Inf values\n",
    "            entropy = np.nan_to_num(entropy, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "            return entropy\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating attention entropy: {e}\")\n",
    "            # Return dummy values\n",
    "            return np.ones(attention_matrix.shape[0]) * 0.5\n",
    "\n",
    "\n",
    "    def calculate_semantic_similarity_matrix(self, value_vectors):\n",
    "        \"\"\"\n",
    "        Calculate semantic similarity matrix between all value vectors.\n",
    "        Uses extremely robust numerical methods to handle any value ranges.\n",
    "\n",
    "        Args:\n",
    "            value_vectors: numpy array of value vectors\n",
    "\n",
    "        Returns:\n",
    "            numpy array of semantic similarities\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get dimensions\n",
    "            seq_len = value_vectors.shape[0]\n",
    "\n",
    "            # Initialize similarity matrix\n",
    "            similarity_matrix = np.zeros((seq_len, seq_len))\n",
    "\n",
    "            # Handle any NaN or Inf values\n",
    "            value_vectors_safe = np.nan_to_num(value_vectors, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "            # Check for extreme values and scale \n",
    "            max_abs = np.max(np.abs(value_vectors_safe))\n",
    "            if max_abs > 1e3:  # Lower threshold for scaling\n",
    "                scale_factor = 1e3 / max_abs\n",
    "                value_vectors_safe = value_vectors_safe * scale_factor\n",
    "\n",
    "            norms = np.zeros(seq_len)\n",
    "            for i in range(seq_len):\n",
    "                vec = value_vectors_safe[i]\n",
    "                squared_sum = 0.0\n",
    "                chunk_size = 1000  \n",
    "                for j in range(0, len(vec), chunk_size):\n",
    "                    chunk = vec[j:j+chunk_size]\n",
    "                    # Use np.float64 for better precision\n",
    "                    chunk_sum = np.sum(np.square(chunk.astype(np.float64)))\n",
    "                    squared_sum += chunk_sum\n",
    "\n",
    "                norms[i] = np.sqrt(max(1e-10, squared_sum))  # Ensure minimum value\n",
    "\n",
    "            # Compute similarity matrix\n",
    "            for i in range(seq_len):\n",
    "                for j in range(seq_len):\n",
    "                    # Special case for self-similarity\n",
    "                    if i == j:\n",
    "                        similarity_matrix[i, j] = 1.0\n",
    "                        continue\n",
    "\n",
    "                    vec_i = value_vectors_safe[i]\n",
    "                    vec_j = value_vectors_safe[j]\n",
    "\n",
    "                    if norms[i] < 1e-8 or norms[j] < 1e-8:\n",
    "                        similarity_matrix[i, j] = 0.0\n",
    "                        continue\n",
    "\n",
    "                    # Calculate dot product in chunks \n",
    "                    dot_product = 0.0\n",
    "                    chunk_size = 1000\n",
    "                    for k in range(0, len(vec_i), chunk_size):\n",
    "                        chunk_i = vec_i[k:k+chunk_size].astype(np.float64)\n",
    "                        chunk_j = vec_j[k:k+chunk_size].astype(np.float64)\n",
    "                        chunk_product = np.sum(chunk_i * chunk_j)\n",
    "                        dot_product += chunk_product\n",
    "\n",
    "                    # Compute cosine similarity\n",
    "                    similarity = dot_product / (norms[i] * norms[j])\n",
    "\n",
    "                    # Ensure value is in valid range\n",
    "                    similarity_matrix[i, j] = np.clip(similarity, -1.0, 1.0)\n",
    "\n",
    "            return similarity_matrix\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating semantic similarity: {e}\")\n",
    "            size = value_vectors.shape[0] if hasattr(value_vectors, 'shape') else 1\n",
    "            return np.eye(size)\n",
    "\n",
    "\n",
    "    def calculate_value_transformation_magnitude(self, pre_values, post_values):\n",
    "        \"\"\"\n",
    "        Calculate the magnitude of value vector transformations.\n",
    "        Uses specialized techniques to avoid overflow and numerical issues.\n",
    "\n",
    "        Args:\n",
    "            pre_values: numpy array of value vectors before attention\n",
    "            post_values: numpy array of value vectors after attention\n",
    "\n",
    "        Returns:\n",
    "            numpy array of transformation magnitudes\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check that dimensions match\n",
    "            if pre_values.shape[0] != post_values.shape[0]:\n",
    "                print(f\"WARNING: Pre-values shape {pre_values.shape} doesn't match post-values shape {post_values.shape}\")\n",
    "                # Return dummy values\n",
    "                return np.ones(max(pre_values.shape[0], post_values.shape[0])) * 0.5\n",
    "\n",
    "            seq_len = pre_values.shape[0]\n",
    "\n",
    "            magnitudes = np.zeros(seq_len)\n",
    "\n",
    "            # Handle any NaN or Inf values\n",
    "            pre_safe = np.nan_to_num(pre_values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            post_safe = np.nan_to_num(post_values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "            # Check for extreme values and scale if necessary\n",
    "            pre_max = np.max(np.abs(pre_safe))\n",
    "            post_max = np.max(np.abs(post_safe))\n",
    "            max_val = max(pre_max, post_max)\n",
    "\n",
    "            if float(max_val) > 1e3:  # Lower threshold\n",
    "                scale_factor = 1e3 / float(max_val)\n",
    "                pre_safe = pre_safe * scale_factor\n",
    "                post_safe = post_safe * scale_factor\n",
    "\n",
    "            batch_size = min(100, seq_len)  \n",
    "\n",
    "            for batch_start in range(0, seq_len, batch_size):\n",
    "                batch_end = min(batch_start + batch_size, seq_len)\n",
    "\n",
    "                diffs = post_safe[batch_start:batch_end] - pre_safe[batch_start:batch_end]\n",
    "\n",
    "                for i in range(batch_end - batch_start):\n",
    "                    idx = batch_start + i\n",
    "                    diff = diffs[i]\n",
    "\n",
    "                    diff_clipped = np.clip(diff, -1e3, 1e3)\n",
    "\n",
    "                    # Calculate L2 norm in chunks\n",
    "                    squared_sum = 0.0\n",
    "                    chunk_size = 1000\n",
    "                    for j in range(0, len(diff_clipped), chunk_size):\n",
    "                        chunk = diff_clipped[j:j+chunk_size].astype(np.float64)\n",
    "                        chunk_sum = np.sum(np.square(chunk))\n",
    "                        squared_sum += chunk_sum\n",
    "\n",
    "                    magnitudes[idx] = np.sqrt(max(0.0, squared_sum))\n",
    "\n",
    "            magnitudes = np.clip(magnitudes, 0.0, 1e6)\n",
    "            magnitudes = np.nan_to_num(magnitudes, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "            return magnitudes\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating transformation magnitude: {e}\")\n",
    "            return np.ones(pre_values.shape[0] if hasattr(pre_values, 'shape') else 1) * 0.5\n",
    "\n",
    "\n",
    "    def perform_pca_analysis(self, attention_matrix, value_vectors, n_components=3):\n",
    "        \"\"\"\n",
    "        Perform PCA on attention and value matrices to identify dominant patterns.\n",
    "        Uses numerically stable approaches to avoid warnings and errors.\n",
    "\n",
    "        Args:\n",
    "            attention_matrix: numpy array of attention weights\n",
    "            value_vectors: numpy array of value vectors\n",
    "            n_components: int, number of components to extract\n",
    "\n",
    "        Returns:\n",
    "            dict with PCA results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if attention_matrix.shape[0] < 2 or value_vectors.shape[0] < 2:\n",
    "                return {\n",
    "                    \"correlation\": 0.0,\n",
    "                    \"error\": \"Not enough samples for PCA\",\n",
    "                    \"average_correlation\": 0.0\n",
    "                }\n",
    "\n",
    "            attn_clean = np.nan_to_num(attention_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            value_clean = np.nan_to_num(value_vectors, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "            attn_max = np.max(np.abs(attn_clean))\n",
    "            value_max = np.max(np.abs(value_clean))\n",
    "\n",
    "            if attn_max > 1e3:  \n",
    "                scale_factor = 1e3 / float(attn_max)\n",
    "                attn_clean = attn_clean * scale_factor\n",
    "\n",
    "            if value_max > 1e3:  \n",
    "                scale_factor = 1e3 / float(value_max)\n",
    "                value_clean = value_clean * scale_factor\n",
    "\n",
    "            attn_min_dim = min(attn_clean.shape)\n",
    "            value_min_dim = min(value_clean.shape)\n",
    "            n_components = min(n_components, attn_min_dim - 1, value_min_dim - 1)\n",
    "\n",
    "            if n_components <= 0:\n",
    "                return {\n",
    "                    \"correlation\": 0.0,\n",
    "                    \"error\": \"Not enough dimensions for PCA\",\n",
    "                    \"average_correlation\": 0.0\n",
    "                }\n",
    "\n",
    "            try:\n",
    "                attention_pca = PCA(n_components=n_components, svd_solver='randomized')\n",
    "                attention_pcs = attention_pca.fit_transform(attn_clean)\n",
    "\n",
    "                value_pca = PCA(n_components=n_components, svd_solver='randomized')\n",
    "                value_pcs = value_pca.fit_transform(value_clean)\n",
    "\n",
    "                # Calculate correlation between the principal components\n",
    "                correlations = []\n",
    "                for i in range(n_components):\n",
    "                    if i < len(attention_pcs[0]) and i < len(value_pcs[0]):\n",
    "                        attn_projection = attention_pcs[:, i]\n",
    "                        value_projection = value_pcs[:, i]\n",
    "\n",
    "                        attn_projection = np.nan_to_num(attn_projection)\n",
    "                        value_projection = np.nan_to_num(value_projection)\n",
    "\n",
    "                        if np.std(attn_projection) > 1e-8 and np.std(value_projection) > 1e-8:\n",
    "                            corr = np.corrcoef(attn_projection, value_projection)[0, 1]\n",
    "                            if not np.isnan(corr) and not np.isinf(corr):\n",
    "                                correlations.append(float(corr))\n",
    "\n",
    "                avg_correlation = np.mean(correlations) if correlations else 0.0\n",
    "\n",
    "                return {\n",
    "                    \"attention_explained_variance\": attention_pca.explained_variance_ratio_.tolist(),\n",
    "                    \"value_explained_variance\": value_pca.explained_variance_ratio_.tolist(),\n",
    "                    \"pc_correlations\": correlations,\n",
    "                    \"average_correlation\": float(avg_correlation)\n",
    "                }\n",
    "\n",
    "            except Exception as inner_e:\n",
    "                print(f\"PCA using sklearn failed: {inner_e}. Using simplified approach.\")\n",
    "                return {\n",
    "                    \"correlation\": 0.0,\n",
    "                    \"error\": f\"PCA failed: {str(inner_e)}\",\n",
    "                    \"average_correlation\": 0.0\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in PCA analysis: {e}\")\n",
    "            return {\n",
    "                \"correlation\": 0.0,\n",
    "                \"error\": str(e),\n",
    "                \"average_correlation\": 0.0\n",
    "            }\n",
    "\n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"\n",
    "        Analyze a text sample and compute correlation metrics between\n",
    "        attention distributions and value transformations.\n",
    "        \"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            print(\"Model or tokenizer not loaded!\")\n",
    "            return {\"error\": \"Model or tokenizer not loaded\"}\n",
    "\n",
    "        try:\n",
    "            if self.debug:\n",
    "                print(f\"Analyzing text: '{text[:50]}...'\")\n",
    "\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "            if self.debug:\n",
    "                print(f\"Tokenized input shape: {inputs.input_ids.shape}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(\n",
    "                    **inputs,\n",
    "                    output_attentions=True,\n",
    "                    output_hidden_states=True,\n",
    "                    return_dict=True\n",
    "                )\n",
    "\n",
    "            # Extract attention patterns and hidden states\n",
    "            attentions = outputs.attentions  # tuple of (layer, batch, head, seq_len, seq_len)\n",
    "            hidden_states = outputs.hidden_states  # tuple of (layer+1, batch, seq_len, hidden_dim)\n",
    "\n",
    "            if self.debug:\n",
    "                print(f\"Number of attention layers: {len(attentions)}\")\n",
    "                print(f\"Number of hidden state layers: {len(hidden_states)}\")\n",
    "                if len(attentions) > 0:\n",
    "                    print(f\"Attention shape for first layer: {attentions[0].shape}\")\n",
    "                if len(hidden_states) > 0:\n",
    "                    print(f\"Hidden state shape for first layer: {hidden_states[0].shape}\")\n",
    "\n",
    "            self.attention_matrices = attentions\n",
    "            self.hidden_states = hidden_states\n",
    "\n",
    "            results = {\n",
    "                \"layers\": {},\n",
    "                \"sequence_length\": inputs.input_ids.shape[1]\n",
    "            }\n",
    "\n",
    "            num_layers = len(attentions)\n",
    "            for layer_idx in range(num_layers):\n",
    "                if self.debug:\n",
    "                    print(f\"Processing layer {layer_idx+1}/{num_layers}\")\n",
    "\n",
    "                # Each layer has shape (batch, head, seq_len, seq_len)\n",
    "                layer_attention = attentions[layer_idx][0].cpu().numpy()  # shape: (head, seq_len, seq_len)\n",
    "\n",
    "                # Compute average attention pattern across all heads\n",
    "                avg_attention = np.mean(layer_attention, axis=0)\n",
    "\n",
    "                if self.debug:\n",
    "                    print(f\"Average attention shape: {avg_attention.shape}\")\n",
    "\n",
    "                # Get value vectors for pre and post attention\n",
    "                pre_values = self.extract_value_vectors(hidden_states, layer_idx)\n",
    "                post_values = self.extract_value_vectors(hidden_states, layer_idx + 1)\n",
    "\n",
    "                if self.debug:\n",
    "                    print(f\"Pre-values shape: {pre_values.shape}\")\n",
    "                    print(f\"Post-values shape: {post_values.shape}\")\n",
    "\n",
    "                if pre_values.shape[0] != avg_attention.shape[0] or post_values.shape[0] != avg_attention.shape[0]:\n",
    "                    print(f\"WARNING: Value vector dimensions don't match attention matrix dimensions\")\n",
    "                    print(f\"Attention shape: {avg_attention.shape}, Pre-values shape: {pre_values.shape}, Post-values shape: {post_values.shape}\")\n",
    "\n",
    "                    results[\"layers\"][str(layer_idx)] = {\n",
    "                        \"attention_entropy_mean\": 0.5,\n",
    "                        \"attention_entropy_std\": 0.1,\n",
    "                        \"transformation_magnitude_mean\": 0.5,\n",
    "                        \"transformation_magnitude_std\": 0.1,\n",
    "                        \"geometric_semantic_correlation\": 0.0,\n",
    "                        \"entropy_magnitude_correlation\": 0.0,\n",
    "                        \"pca_analysis\": {\n",
    "                            \"average_correlation\": 0.0,\n",
    "                            \"error\": \"Dimension mismatch\"\n",
    "                        }\n",
    "                    }\n",
    "                    continue\n",
    "\n",
    "                # Calculate attention entropy\n",
    "                attention_entropy = self.calculate_attention_entropy(avg_attention)\n",
    "\n",
    "                # Calculate value transformation magnitude\n",
    "                transformation_magnitude = self.calculate_value_transformation_magnitude(pre_values, post_values)\n",
    "\n",
    "                # Calculate semantic similarity matrix for value vectors\n",
    "                semantic_similarity = self.calculate_semantic_similarity_matrix(pre_values)\n",
    "\n",
    "                # Calculate correlation between attention weights and semantic similarity\n",
    "                try:\n",
    "                    # Flatten both matrices (exclude diagonal elements)\n",
    "                    attn_flat = []\n",
    "                    sem_flat = []\n",
    "                    for i in range(avg_attention.shape[0]):\n",
    "                        for j in range(avg_attention.shape[1]):\n",
    "                            if i != j:  # Exclude self-attention/similarity\n",
    "                                attn_flat.append(avg_attention[i, j])\n",
    "                                sem_flat.append(semantic_similarity[i, j])\n",
    "\n",
    "                    # Calculate correlation\n",
    "                    if len(attn_flat) > 1 and np.std(attn_flat) > 0 and np.std(sem_flat) > 0:\n",
    "                        geometric_semantic_corr = float(np.corrcoef(attn_flat, sem_flat)[0, 1])\n",
    "                    else:\n",
    "                        geometric_semantic_corr = 0.0\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating geometric-semantic correlation: {e}\")\n",
    "                    geometric_semantic_corr = 0.0\n",
    "\n",
    "                # Calculate correlation between attention entropy and transformation magnitude\n",
    "                try:\n",
    "                    if len(attention_entropy) > 1 and len(transformation_magnitude) > 1 and np.std(attention_entropy) > 0 and np.std(transformation_magnitude) > 0:\n",
    "                        entropy_magnitude_corr = float(np.corrcoef(attention_entropy, transformation_magnitude)[0, 1])\n",
    "                    else:\n",
    "                        entropy_magnitude_corr = 0.0\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating entropy-magnitude correlation: {e}\")\n",
    "                    entropy_magnitude_corr = 0.0\n",
    "\n",
    "                pca_results = self.perform_pca_analysis(avg_attention, pre_values)\n",
    "\n",
    "                results[\"layers\"][str(layer_idx)] = {\n",
    "                    \"attention_entropy_mean\": float(np.mean(attention_entropy)),\n",
    "                    \"attention_entropy_std\": float(np.std(attention_entropy)),\n",
    "                    \"transformation_magnitude_mean\": float(np.mean(transformation_magnitude)),\n",
    "                    \"transformation_magnitude_std\": float(np.std(transformation_magnitude)),\n",
    "                    \"geometric_semantic_correlation\": geometric_semantic_corr,\n",
    "                    \"entropy_magnitude_correlation\": entropy_magnitude_corr,\n",
    "                    \"pca_analysis\": pca_results\n",
    "                }\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing text: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def analyze_samples(self, texts, max_failures=20):\n",
    "        \"\"\"\n",
    "        Analyze multiple text samples and collect metrics for correlation analysis.\n",
    "\n",
    "        Args:\n",
    "            texts: list of text samples\n",
    "            max_failures: maximum number of consecutive failures before giving up\n",
    "        \"\"\"\n",
    "        print(f\"Analyzing {len(texts)} text samples...\")\n",
    "\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "\n",
    "        metric_collectors = {\n",
    "            \"attention_entropy_mean\": {},\n",
    "            \"transformation_magnitude_mean\": {},\n",
    "            \"geometric_semantic_correlation\": {},\n",
    "            \"entropy_magnitude_correlation\": {},\n",
    "            \"pca_average_correlation\": {}\n",
    "        }\n",
    "\n",
    "        successful_analyses = 0\n",
    "        consecutive_failures = 0\n",
    "\n",
    "        for i, text in enumerate(tqdm(texts, desc=\"Processing texts\")):\n",
    "            try:\n",
    "                result = self.analyze_text(text)\n",
    "\n",
    "                if \"error\" in result:\n",
    "                    consecutive_failures += 1\n",
    "                    if consecutive_failures >= max_failures:\n",
    "                        print(f\"Too many consecutive failures ({max_failures}). Stopping analysis.\")\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                consecutive_failures = 0\n",
    "                successful_analyses += 1\n",
    "\n",
    "                # Collect metrics from each layer\n",
    "                for layer_idx, layer_data in result[\"layers\"].items():\n",
    "                    for metric in metric_collectors.keys():\n",
    "                        if layer_idx not in metric_collectors[metric]:\n",
    "                            metric_collectors[metric][layer_idx] = []\n",
    "\n",
    "                    metric_collectors[\"attention_entropy_mean\"][layer_idx].append(\n",
    "                        layer_data[\"attention_entropy_mean\"]\n",
    "                    )\n",
    "\n",
    "                    metric_collectors[\"transformation_magnitude_mean\"][layer_idx].append(\n",
    "                        layer_data[\"transformation_magnitude_mean\"]\n",
    "                    )\n",
    "\n",
    "                    metric_collectors[\"geometric_semantic_correlation\"][layer_idx].append(\n",
    "                        layer_data[\"geometric_semantic_correlation\"]\n",
    "                    )\n",
    "\n",
    "                    metric_collectors[\"entropy_magnitude_correlation\"][layer_idx].append(\n",
    "                        layer_data[\"entropy_magnitude_correlation\"]\n",
    "                    )\n",
    "\n",
    "                    if \"pca_analysis\" in layer_data and \"average_correlation\" in layer_data[\"pca_analysis\"]:\n",
    "                        metric_collectors[\"pca_average_correlation\"][layer_idx].append(\n",
    "                            layer_data[\"pca_analysis\"][\"average_correlation\"]\n",
    "                        )\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                if (i+1) % 5 == 0:\n",
    "                    print(f\"Processed {i+1}/{len(texts)} samples\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing text {i+1}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                consecutive_failures += 1\n",
    "                if consecutive_failures >= max_failures:\n",
    "                    print(f\"Too many consecutive failures ({max_failures}). Stopping analysis.\")\n",
    "                    break\n",
    "\n",
    "        print(f\"Successfully analyzed {successful_analyses}/{len(texts)} samples\")\n",
    "\n",
    "        if successful_analyses > 0:\n",
    "            stats_results = self.compute_statistics(metric_collectors)\n",
    "\n",
    "            self.results = {\n",
    "                \"metric_collectors\": metric_collectors,\n",
    "                \"statistics\": stats_results,\n",
    "                \"successful_analyses\": successful_analyses\n",
    "            }\n",
    "\n",
    "            return stats_results\n",
    "        else:\n",
    "            print(\"No successful analyses to compute statistics\")\n",
    "            return {}\n",
    "\n",
    "    def compute_statistics(self, metric_collectors):\n",
    "        \"\"\"\n",
    "        Compute statistics for the collected metrics with added statistical significance testing.\n",
    "        \"\"\"\n",
    "        print(\"Computing statistics with significance testing...\")\n",
    "\n",
    "        stats_results = {\n",
    "            \"by_layer\": {},\n",
    "            \"overall\": {},\n",
    "            \"significance_tests\": {}  \n",
    "        }\n",
    "\n",
    "        metrics = list(metric_collectors.keys())\n",
    "\n",
    "        all_metrics = {metric: [] for metric in metrics}\n",
    "\n",
    "        for layer_idx in metric_collectors[\"attention_entropy_mean\"].keys():\n",
    "            stats_results[\"by_layer\"][layer_idx] = {}\n",
    "\n",
    "            layer_stats = {}\n",
    "\n",
    "            for metric in metrics:\n",
    "                if layer_idx in metric_collectors[metric] and metric_collectors[metric][layer_idx]:\n",
    "                    values = metric_collectors[metric][layer_idx]\n",
    "\n",
    "                    all_metrics[metric].extend(values)\n",
    "\n",
    "                    if len(values) >= 1:\n",
    "                        layer_stats[metric] = {\n",
    "                            \"mean\": float(np.mean(values)),\n",
    "                            \"median\": float(np.median(values)),\n",
    "                            \"std\": float(np.std(values)) if len(values) > 1 else 0.0,\n",
    "                            \"min\": float(np.min(values)),\n",
    "                            \"max\": float(np.max(values)),\n",
    "                            \"sample_size\": len(values)\n",
    "                        }\n",
    "\n",
    "                        # Add one-sample t-test against zero (useful for correlation metrics)\n",
    "                        if len(values) >= 5:  \n",
    "                            try:\n",
    "                                t_stat, p_value = stats.ttest_1samp(values, 0)\n",
    "                                layer_stats[metric][\"t_test\"] = {\n",
    "                                    \"t_statistic\": float(t_stat),\n",
    "                                    \"p_value\": float(p_value),\n",
    "                                    \"significant\": p_value < 0.05,\n",
    "                                    \"confidence_level\": self._get_confidence_level(p_value)\n",
    "                                }\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error calculating t-test for {metric} in layer {layer_idx}: {e}\")\n",
    "\n",
    "            # Calculate correlations between metrics in this layer\n",
    "            try:\n",
    "                corr_metrics = [\"attention_entropy_mean\", \"transformation_magnitude_mean\",\n",
    "                              \"geometric_semantic_correlation\", \"entropy_magnitude_correlation\"]\n",
    "\n",
    "                for i, metric1 in enumerate(corr_metrics):\n",
    "                    for j, metric2 in enumerate(corr_metrics[i+1:], i+1):\n",
    "                        if (metric1 in layer_stats and metric2 in layer_stats and\n",
    "                            layer_idx in metric_collectors[metric1] and\n",
    "                            layer_idx in metric_collectors[metric2]):\n",
    "\n",
    "                            values1 = metric_collectors[metric1][layer_idx]\n",
    "                            values2 = metric_collectors[metric2][layer_idx]\n",
    "\n",
    "                            min_len = min(len(values1), len(values2))\n",
    "                            if min_len >= 5:  # Minimum for meaningful correlation\n",
    "                                values1 = values1[:min_len]\n",
    "                                values2 = values2[:min_len]\n",
    "\n",
    "                                # Calculate both Pearson (linear) and Spearman (rank) correlations\n",
    "                                if np.std(values1) > 0 and np.std(values2) > 0:\n",
    "                                    # Pearson correlation with p-value\n",
    "                                    pearson_r, pearson_p = stats.pearsonr(values1, values2)\n",
    "\n",
    "                                    # Spearman rank correlation with p-value\n",
    "                                    spearman_r, spearman_p = stats.spearmanr(values1, values2)\n",
    "\n",
    "                                    # Create correlation key\n",
    "                                    corr_key = f\"corr_{metric1}_{metric2}\"\n",
    "\n",
    "                                    # Store correlation statistics\n",
    "                                    layer_stats[corr_key] = {\n",
    "                                        \"pearson\": {\n",
    "                                            \"r\": float(pearson_r),\n",
    "                                            \"p_value\": float(pearson_p),\n",
    "                                            \"significant\": pearson_p < 0.05,\n",
    "                                            \"confidence_level\": self._get_confidence_level(pearson_p)\n",
    "                                        },\n",
    "                                        \"spearman\": {\n",
    "                                            \"rho\": float(spearman_r),\n",
    "                                            \"p_value\": float(spearman_p),\n",
    "                                            \"significant\": spearman_p < 0.05,\n",
    "                                            \"confidence_level\": self._get_confidence_level(spearman_p)\n",
    "                                        },\n",
    "                                        \"sample_size\": min_len\n",
    "                                    }\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating layer correlations: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "            # Store layer statistics\n",
    "            stats_results[\"by_layer\"][layer_idx] = layer_stats\n",
    "\n",
    "        # Calculate overall statistics\n",
    "        overall_stats = {}\n",
    "\n",
    "        for metric in metrics:\n",
    "            values = all_metrics[metric]\n",
    "            if len(values) >= 1:\n",
    "                overall_stats[metric] = {\n",
    "                    \"mean\": float(np.mean(values)),\n",
    "                    \"median\": float(np.median(values)),\n",
    "                    \"std\": float(np.std(values)) if len(values) > 1 else 0.0,\n",
    "                    \"min\": float(np.min(values)),\n",
    "                    \"max\": float(np.max(values)),\n",
    "                    \"sample_size\": len(values)\n",
    "                }\n",
    "\n",
    "                # Add one-sample t-test for overall metrics\n",
    "                if len(values) >= 5:  # Minimum sample size for meaningful t-test\n",
    "                    try:\n",
    "                        t_stat, p_value = stats.ttest_1samp(values, 0)\n",
    "                        overall_stats[metric][\"t_test\"] = {\n",
    "                            \"t_statistic\": float(t_stat),\n",
    "                            \"p_value\": float(p_value),\n",
    "                            \"significant\": p_value < 0.05,\n",
    "                            \"confidence_level\": self._get_confidence_level(p_value)\n",
    "                        }\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating overall t-test for {metric}: {e}\")\n",
    "\n",
    "        # Layer pattern analysis - how metrics evolve across layers\n",
    "        layer_indices = sorted([int(idx) for idx in metric_collectors[\"attention_entropy_mean\"].keys()\n",
    "                              if metric_collectors[\"attention_entropy_mean\"][idx]])\n",
    "\n",
    "        if layer_indices and len(layer_indices) >= 2:\n",
    "            # Analyze how metrics evolve across layers\n",
    "            layer_evolution = {}\n",
    "\n",
    "            for metric in metrics:\n",
    "                try:\n",
    "                    values_by_layer = [np.mean(metric_collectors[metric][str(idx)])\n",
    "                                    for idx in layer_indices\n",
    "                                    if str(idx) in metric_collectors[metric] and metric_collectors[metric][str(idx)]]\n",
    "\n",
    "                    if len(values_by_layer) >= 2:\n",
    "                        # Check for monotonic trend with improved significance testing\n",
    "                        # Spearman rank correlation is better for trend analysis\n",
    "                        spearman_r, spearman_p = stats.spearmanr(layer_indices[:len(values_by_layer)], values_by_layer)\n",
    "\n",
    "                        # Pearson for linear trend\n",
    "                        pearson_r, pearson_p = stats.pearsonr(layer_indices[:len(values_by_layer)], values_by_layer)\n",
    "\n",
    "                        layer_evolution[f\"{metric}_trend\"] = {\n",
    "                            \"spearman\": {\n",
    "                                \"rho\": float(spearman_r),\n",
    "                                \"p_value\": float(spearman_p),\n",
    "                                \"significant\": spearman_p < 0.05,\n",
    "                                \"confidence_level\": self._get_confidence_level(spearman_p)\n",
    "                            },\n",
    "                            \"pearson\": {\n",
    "                                \"r\": float(pearson_r),\n",
    "                                \"p_value\": float(pearson_p),\n",
    "                                \"significant\": pearson_p < 0.05,\n",
    "                                \"confidence_level\": self._get_confidence_level(pearson_p)\n",
    "                            },\n",
    "                            \"pattern\": \"increasing\" if spearman_r > 0.5 else (\"decreasing\" if spearman_r < -0.5 else \"mixed\"),\n",
    "                            \"sample_size\": len(values_by_layer)\n",
    "                        }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error analyzing layer evolution for {metric}: {e}\")\n",
    "                    layer_evolution[f\"{metric}_trend\"] = {\n",
    "                        \"correlation\": 0.0,\n",
    "                        \"pattern\": \"unknown\",\n",
    "                        \"error\": str(e)\n",
    "                    }\n",
    "\n",
    "            # Store layer evolution patterns\n",
    "            overall_stats[\"layer_evolution\"] = layer_evolution\n",
    "\n",
    "        # Perform cross-layer comparison tests\n",
    "        if len(layer_indices) >= 2:\n",
    "            first_layer = str(min(layer_indices))\n",
    "            last_layer = str(max(layer_indices))\n",
    "\n",
    "            # Initialize cross-layer test results\n",
    "            cross_layer_tests = {}\n",
    "\n",
    "            # Compare first and last layer for each metric\n",
    "            for metric in metrics:\n",
    "                if (first_layer in metric_collectors[metric] and\n",
    "                    last_layer in metric_collectors[metric] and\n",
    "                    len(metric_collectors[metric][first_layer]) >= 5 and\n",
    "                    len(metric_collectors[metric][last_layer]) >= 5):\n",
    "\n",
    "                    first_layer_values = metric_collectors[metric][first_layer]\n",
    "                    last_layer_values = metric_collectors[metric][last_layer]\n",
    "\n",
    "                    # Perform independent samples t-test\n",
    "                    try:\n",
    "                        t_stat, p_value = stats.ttest_ind(first_layer_values, last_layer_values, equal_var=False)\n",
    "\n",
    "                        cross_layer_tests[f\"{metric}_first_vs_last\"] = {\n",
    "                            \"first_layer_mean\": float(np.mean(first_layer_values)),\n",
    "                            \"last_layer_mean\": float(np.mean(last_layer_values)),\n",
    "                            \"difference\": float(np.mean(last_layer_values) - np.mean(first_layer_values)),\n",
    "                            \"t_statistic\": float(t_stat),\n",
    "                            \"p_value\": float(p_value),\n",
    "                            \"significant\": p_value < 0.05,\n",
    "                            \"confidence_level\": self._get_confidence_level(p_value),\n",
    "                            \"effect\": \"increase\" if np.mean(last_layer_values) > np.mean(first_layer_values) else \"decrease\",\n",
    "                            \"sample_sizes\": {\n",
    "                                \"first_layer\": len(first_layer_values),\n",
    "                                \"last_layer\": len(last_layer_values)\n",
    "                            }\n",
    "                        }\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating cross-layer test for {metric}: {e}\")\n",
    "\n",
    "            # Add cross-layer tests to results\n",
    "            stats_results[\"significance_tests\"][\"cross_layer\"] = cross_layer_tests\n",
    "\n",
    "        # Store overall statistics\n",
    "        stats_results[\"overall\"] = overall_stats\n",
    "\n",
    "        return stats_results\n",
    "\n",
    "    def _get_confidence_level(self, p_value):\n",
    "        \"\"\"\n",
    "        Convert p-value to a confidence level description.\n",
    "\n",
    "        Args:\n",
    "            p_value: The p-value from a statistical test\n",
    "\n",
    "        Returns:\n",
    "            String describing the confidence level\n",
    "        \"\"\"\n",
    "        if p_value < 0.001:\n",
    "            return \"very strong (p<0.001)\"\n",
    "        elif p_value < 0.01:\n",
    "            return \"strong (p<0.01)\"\n",
    "        elif p_value < 0.05:\n",
    "            return \"significant (p<0.05)\"\n",
    "        elif p_value < 0.1:\n",
    "            return \"marginal (p<0.1)\"\n",
    "        else:\n",
    "            return \"not significant (p≥0.1)\"\n",
    "\n",
    "    def generate_statistics_report(self):\n",
    "        \"\"\"\n",
    "        Generate a text report with statistics, correlation values, and statistical significance.\n",
    "        Shows data from first layer, last layer, and two intermediate layers.\n",
    "        \"\"\"\n",
    "        if not self.results or \"statistics\" not in self.results:\n",
    "            return \"No statistics available. Run analyze_samples first.\"\n",
    "\n",
    "        stats_results = self.results[\"statistics\"]\n",
    "        successful_analyses = self.results.get(\"successful_analyses\", 0)\n",
    "\n",
    "        lines = []\n",
    "        lines.append(f\"ATTENTION-VALUE CORRELATION ANALYSIS FOR {self.model_name}\\n\")\n",
    "        lines.append(f\"Based on {successful_analyses} text samples\\n\")\n",
    "\n",
    "        lines.append(\"OVERALL STATISTICS\")\n",
    "        lines.append(\"=\" * 20)\n",
    "\n",
    "        overall = stats_results[\"overall\"]\n",
    "        if not overall:\n",
    "            lines.append(\"\\nNo overall statistics available.\")\n",
    "        else:\n",
    "            metric_display = {\n",
    "                \"attention_entropy_mean\": \"Attention Distribution Entropy\",\n",
    "                \"transformation_magnitude_mean\": \"Value Transformation Magnitude\",\n",
    "                \"geometric_semantic_correlation\": \"Geometric-Semantic Alignment\",\n",
    "                \"entropy_magnitude_correlation\": \"Entropy-Magnitude Correlation\",\n",
    "                \"pca_average_correlation\": \"PCA Component Correlation\"\n",
    "            }\n",
    "\n",
    "            for metric in [\"attention_entropy_mean\", \"transformation_magnitude_mean\",\n",
    "                        \"geometric_semantic_correlation\", \"entropy_magnitude_correlation\",\n",
    "                        \"pca_average_correlation\"]:\n",
    "                if metric in overall:\n",
    "                    lines.append(f\"\\n{metric_display[metric]}:\")\n",
    "                    metric_stats = overall[metric]\n",
    "                    lines.append(f\"  Mean: {metric_stats['mean']:.4f}\")\n",
    "                    lines.append(f\"  Median: {metric_stats['median']:.4f}\")\n",
    "                    lines.append(f\"  Std Dev: {metric_stats['std']:.4f}\")\n",
    "                    lines.append(f\"  Range: {metric_stats['min']:.4f} to {metric_stats['max']:.4f}\")\n",
    "                    lines.append(f\"  Sample Size: {metric_stats.get('sample_size', 'N/A')}\")\n",
    "\n",
    "                    if \"t_test\" in metric_stats:\n",
    "                        t_test = metric_stats[\"t_test\"]\n",
    "                        p_value = t_test[\"p_value\"]\n",
    "                        significance = \"* SIGNIFICANT *\" if t_test[\"significant\"] else \"not significant\"\n",
    "                        lines.append(f\"  Statistical Significance: {significance}\")\n",
    "                        lines.append(f\"  p-value: {p_value:.6f} ({t_test['confidence_level']})\")\n",
    "                        lines.append(f\"  t-statistic: {t_test['t_statistic']:.4f}\")\n",
    "\n",
    "            if \"layer_evolution\" in overall:\n",
    "                lines.append(\"\\nLayer Evolution Patterns (with Statistical Significance):\")\n",
    "                for trend_key, trend_data in overall[\"layer_evolution\"].items():\n",
    "                    base_metric = trend_key.replace(\"_trend\", \"\")\n",
    "                    if base_metric in metric_display:\n",
    "                        metric_name = metric_display[base_metric]\n",
    "\n",
    "                        if \"spearman\" in trend_data and \"pearson\" in trend_data:\n",
    "                            spearman = trend_data[\"spearman\"]\n",
    "                            pearson = trend_data[\"pearson\"]\n",
    "\n",
    "                            lines.append(f\"  {metric_name}:\")\n",
    "                            lines.append(f\"    Pattern: {trend_data['pattern'].title()}\")\n",
    "\n",
    "                            spearman_sig = \"* SIGNIFICANT *\" if spearman[\"significant\"] else \"not significant\"\n",
    "                            lines.append(f\"    Spearman Rank Correlation: {spearman['rho']:.4f} ({spearman_sig})\")\n",
    "                            lines.append(f\"    p-value: {spearman['p_value']:.6f} ({spearman['confidence_level']})\")\n",
    "\n",
    "                            pearson_sig = \"* SIGNIFICANT *\" if pearson[\"significant\"] else \"not significant\"\n",
    "                            lines.append(f\"    Pearson Linear Correlation: {pearson['r']:.4f} ({pearson_sig})\")\n",
    "                            lines.append(f\"    p-value: {pearson['p_value']:.6f} ({pearson['confidence_level']})\")\n",
    "\n",
    "                            lines.append(f\"    Sample Size: {trend_data.get('sample_size', 'N/A')}\")\n",
    "                        else:\n",
    "                            lines.append(f\"  {metric_name}: {trend_data['pattern'].title()} (correlation: {trend_data.get('correlation', 0.0):.4f})\")\n",
    "\n",
    "        if \"significance_tests\" in stats_results and \"cross_layer\" in stats_results[\"significance_tests\"]:\n",
    "            cross_layer_tests = stats_results[\"significance_tests\"][\"cross_layer\"]\n",
    "            if cross_layer_tests:\n",
    "                lines.append(\"\\nCROSS-LAYER STATISTICAL TESTS:\")\n",
    "                lines.append(\"=\" * 30)\n",
    "\n",
    "                for test_key, test_data in cross_layer_tests.items():\n",
    "                    base_metric = test_key.replace(\"_first_vs_last\", \"\")\n",
    "                    metric_name = metric_display.get(base_metric, base_metric.replace(\"_\", \" \").title())\n",
    "\n",
    "                    lines.append(f\"\\n{metric_name} (First vs. Last Layer):\")\n",
    "\n",
    "                    lines.append(f\"  First Layer Mean: {test_data['first_layer_mean']:.4f}\")\n",
    "                    lines.append(f\"  Last Layer Mean: {test_data['last_layer_mean']:.4f}\")\n",
    "                    lines.append(f\"  Difference: {test_data['difference']:.4f} ({test_data['effect']})\")\n",
    "\n",
    "                    significance = \"* SIGNIFICANT *\" if test_data[\"significant\"] else \"not significant\"\n",
    "                    lines.append(f\"  Statistical Significance: {significance}\")\n",
    "                    lines.append(f\"  p-value: {test_data['p_value']:.6f} ({test_data['confidence_level']})\")\n",
    "                    lines.append(f\"  t-statistic: {test_data['t_statistic']:.4f}\")\n",
    "                    lines.append(f\"  Sample Sizes: {test_data['sample_sizes']['first_layer']} (first), {test_data['sample_sizes']['last_layer']} (last)\")\n",
    "\n",
    "        lines.append(\"\\nLAYER-SPECIFIC STATISTICS (select layers):\")\n",
    "\n",
    "        valid_layers = sorted([int(layer_idx) for layer_idx in stats_results[\"by_layer\"].keys()\n",
    "                              if stats_results[\"by_layer\"][layer_idx]],\n",
    "                            key=lambda x: int(x))\n",
    "\n",
    "        if not valid_layers:\n",
    "            lines.append(\"\\nNo layer-specific statistics available.\")\n",
    "        else:\n",
    "            num_layers = max(valid_layers) + 1  # +1 because layer indices are 0-based\n",
    "\n",
    "            # Choose layers to analyze (first, quarter, three-quarters, last)\n",
    "            first_layer = 0\n",
    "            quarter_layer = max(1, num_layers // 4)  \n",
    "            three_quarter_layer = max(2, (num_layers * 3) // 4) \n",
    "            last_layer = max(valid_layers)\n",
    "\n",
    "            selected_layers = sorted(list(set([first_layer, quarter_layer, three_quarter_layer, last_layer])))\n",
    "\n",
    "            selected_layers = [layer for layer in selected_layers if str(layer) in stats_results[\"by_layer\"]]\n",
    "\n",
    "            for layer_idx in selected_layers:\n",
    "                layer_stats = stats_results[\"by_layer\"][str(layer_idx)]\n",
    "                if not layer_stats:\n",
    "                    continue\n",
    "\n",
    "                if layer_idx == first_layer:\n",
    "                    position_label = \"FIRST LAYER\"\n",
    "                elif layer_idx == last_layer:\n",
    "                    position_label = \"LAST LAYER\"\n",
    "                elif layer_idx == quarter_layer:\n",
    "                    position_label = \"QUARTER-DEPTH LAYER\"\n",
    "                elif layer_idx == three_quarter_layer:\n",
    "                    position_label = \"THREE-QUARTER-DEPTH LAYER\"\n",
    "                else:\n",
    "                    position_label = \"\"\n",
    "\n",
    "                lines.append(f\"\\nLayer {layer_idx}: {position_label}\")\n",
    "\n",
    "                # Show mean values for key metrics with significance\n",
    "                for metric in [\"attention_entropy_mean\", \"transformation_magnitude_mean\",\n",
    "                              \"geometric_semantic_correlation\", \"entropy_magnitude_correlation\"]:\n",
    "                    if metric in layer_stats:\n",
    "                        metric_readable = metric_display.get(metric, metric.replace(\"_\", \" \").title())\n",
    "\n",
    "                        lines.append(f\"  {metric_readable}: {layer_stats[metric]['mean']:.4f}\")\n",
    "\n",
    "                        if \"t_test\" in layer_stats[metric]:\n",
    "                            t_test = layer_stats[metric][\"t_test\"]\n",
    "                            significance = \"* SIGNIFICANT *\" if t_test[\"significant\"] else \"not significant\"\n",
    "                            lines.append(f\"    Significance: {significance} (p={t_test['p_value']:.6f}, {t_test['confidence_level']})\")\n",
    "\n",
    "                if \"pca_average_correlation\" in layer_stats:\n",
    "                    metric_readable = metric_display[\"pca_average_correlation\"]\n",
    "                    lines.append(f\"  {metric_readable}: {layer_stats['pca_average_correlation']['mean']:.4f}\")\n",
    "\n",
    "                    if \"t_test\" in layer_stats[\"pca_average_correlation\"]:\n",
    "                        t_test = layer_stats[\"pca_average_correlation\"][\"t_test\"]\n",
    "                        significance = \"* SIGNIFICANT *\" if t_test[\"significant\"] else \"not significant\"\n",
    "                        lines.append(f\"    Significance: {significance} (p={t_test['p_value']:.6f}, {t_test['confidence_level']})\")\n",
    "\n",
    "                correlation_keys = [k for k in layer_stats.keys() if k.startswith(\"corr_\")]\n",
    "                if correlation_keys:\n",
    "                    lines.append(\"\\n  Correlation Analysis (with Statistical Significance):\")\n",
    "\n",
    "                    for corr_key in correlation_keys:\n",
    "                        corr_data = layer_stats[corr_key]\n",
    "\n",
    "                        metric_parts = corr_key.replace(\"corr_\", \"\").split(\"_\")\n",
    "                        if len(metric_parts) >= 2:\n",
    "                            last_idx = len(metric_parts) // 2\n",
    "                            metric1_parts = metric_parts[:last_idx]\n",
    "                            metric2_parts = metric_parts[last_idx:]\n",
    "\n",
    "                            metric1 = \"_\".join(metric1_parts)\n",
    "                            metric2 = \"_\".join(metric2_parts)\n",
    "\n",
    "                            metric1_readable = metric_display.get(metric1, metric1.replace(\"_\", \" \").title())\n",
    "                            metric2_readable = metric_display.get(metric2, metric2.replace(\"_\", \" \").title())\n",
    "\n",
    "                            corr_name = f\"{metric1_readable} vs. {metric2_readable}\"\n",
    "                        else:\n",
    "                            corr_name = corr_key.replace(\"corr_\", \"\").replace(\"_\", \" \").title()\n",
    "\n",
    "                        lines.append(f\"    {corr_name}:\")\n",
    "\n",
    "                        # Pearson correlation with significance\n",
    "                        if \"pearson\" in corr_data:\n",
    "                            pearson = corr_data[\"pearson\"]\n",
    "                            pearson_sig = \"* SIGNIFICANT *\" if pearson[\"significant\"] else \"not significant\"\n",
    "                            lines.append(f\"      Pearson r: {pearson['r']:.4f} ({pearson_sig})\")\n",
    "                            lines.append(f\"      p-value: {pearson['p_value']:.6f} ({pearson['confidence_level']})\")\n",
    "\n",
    "                        if \"spearman\" in corr_data:\n",
    "                            spearman = corr_data[\"spearman\"]\n",
    "                            spearman_sig = \"* SIGNIFICANT *\" if spearman[\"significant\"] else \"not significant\"\n",
    "                            lines.append(f\"      Spearman rho: {spearman['rho']:.4f} ({spearman_sig})\")\n",
    "                            lines.append(f\"      p-value: {spearman['p_value']:.6f} ({spearman['confidence_level']})\")\n",
    "\n",
    "                        if \"sample_size\" in corr_data:\n",
    "                            lines.append(f\"      Sample Size: {corr_data['sample_size']}\")\n",
    "\n",
    "        lines.append(\"\\nKEY FINDINGS (WITH STATISTICAL SIGNIFICANCE):\")\n",
    "        lines.append(\"=\" * 40)\n",
    "\n",
    "        if not overall:\n",
    "            lines.append(\"- Not enough data to generate key findings.\")\n",
    "        else:\n",
    "            findings = []\n",
    "\n",
    "            # Geometric-Semantic Alignment insights\n",
    "            if \"geometric_semantic_correlation\" in overall:\n",
    "                geo_sem = overall[\"geometric_semantic_correlation\"]\n",
    "                significance = \"\"\n",
    "                if \"t_test\" in geo_sem:\n",
    "                    if geo_sem[\"t_test\"][\"significant\"]:\n",
    "                        significance = f\" (STATISTICALLY SIGNIFICANT, p={geo_sem['t_test']['p_value']:.6f})\"\n",
    "                    else:\n",
    "                        significance = f\" (not statistically significant, p={geo_sem['t_test']['p_value']:.6f})\"\n",
    "\n",
    "                if geo_sem[\"mean\"] > 0.5:\n",
    "                    findings.append(f\"- STRONG alignment between attention patterns and semantic similarity in value space{significance}\")\n",
    "                elif geo_sem[\"mean\"] > 0.2:\n",
    "                    findings.append(f\"- MODERATE alignment between attention patterns and semantic similarity in value space{significance}\")\n",
    "                elif geo_sem[\"mean\"] > 0:\n",
    "                    findings.append(f\"- WEAK alignment between attention patterns and semantic similarity in value space{significance}\")\n",
    "                else:\n",
    "                    findings.append(f\"- NEGATIVE correlation between attention patterns and semantic similarity{significance}\")\n",
    "\n",
    "            # Entropy-Magnitude Correlation insights\n",
    "            if \"entropy_magnitude_correlation\" in overall:\n",
    "                ent_mag = overall[\"entropy_magnitude_correlation\"]\n",
    "                significance = \"\"\n",
    "                if \"t_test\" in ent_mag:\n",
    "                    if ent_mag[\"t_test\"][\"significant\"]:\n",
    "                        significance = f\" (STATISTICALLY SIGNIFICANT, p={ent_mag['t_test']['p_value']:.6f})\"\n",
    "                    else:\n",
    "                        significance = f\" (not statistically significant, p={ent_mag['t_test']['p_value']:.6f})\"\n",
    "\n",
    "                if ent_mag[\"mean\"] > 0.5:\n",
    "                    findings.append(f\"- Tokens with DIVERSE attention patterns undergo LARGER value transformations{significance}\")\n",
    "                elif ent_mag[\"mean\"] > 0.2:\n",
    "                    findings.append(f\"- Tokens with diverse attention patterns undergo moderately larger value transformations{significance}\")\n",
    "                elif ent_mag[\"mean\"] > 0:\n",
    "                    findings.append(f\"- Weak relationship between attention diversity and transformation magnitude{significance}\")\n",
    "                else:\n",
    "                    findings.append(f\"- Tokens with FOCUSED attention patterns undergo LARGER value transformations{significance}\")\n",
    "\n",
    "            # Layer evolution insights with significance\n",
    "            if \"layer_evolution\" in overall:\n",
    "                layer_evo = overall[\"layer_evolution\"]\n",
    "\n",
    "                if \"geometric_semantic_correlation_trend\" in layer_evo:\n",
    "                    trend = layer_evo[\"geometric_semantic_correlation_trend\"]\n",
    "                    significance = \"\"\n",
    "\n",
    "                    if \"spearman\" in trend and \"p_value\" in trend[\"spearman\"]:\n",
    "                        p_value = trend[\"spearman\"][\"p_value\"]\n",
    "                        if p_value < 0.05:\n",
    "                            significance = f\" (STATISTICALLY SIGNIFICANT, p={p_value:.6f})\"\n",
    "                        else:\n",
    "                            significance = f\" (not statistically significant, p={p_value:.6f})\"\n",
    "\n",
    "                    if trend[\"pattern\"] == \"increasing\":\n",
    "                        findings.append(f\"- Attention-semantic alignment INCREASES in deeper layers{significance}\")\n",
    "                    elif trend[\"pattern\"] == \"decreasing\":\n",
    "                        findings.append(f\"- Attention-semantic alignment DECREASES in deeper layers{significance}\")\n",
    "\n",
    "            if \"significance_tests\" in stats_results and \"cross_layer\" in stats_results[\"significance_tests\"]:\n",
    "                cross_tests = stats_results[\"significance_tests\"][\"cross_layer\"]\n",
    "\n",
    "                for test_key, test_data in cross_tests.items():\n",
    "                    if test_data[\"significant\"]:\n",
    "                        base_metric = test_key.replace(\"_first_vs_last\", \"\")\n",
    "                        metric_name = metric_display.get(base_metric, base_metric.replace(\"_\", \" \").title())\n",
    "                        direction = \"INCREASES\" if test_data[\"effect\"] == \"increase\" else \"DECREASES\"\n",
    "\n",
    "                        finding = (f\"- {metric_name} {direction} by {abs(test_data['difference']):.4f} from first to last layer \"\n",
    "                                  f\"(STATISTICALLY SIGNIFICANT, p={test_data['p_value']:.6f})\")\n",
    "                        findings.append(finding)\n",
    "\n",
    "            for finding in findings:\n",
    "                lines.append(finding)\n",
    "\n",
    "        report_path = os.path.join(self.output_dir, \"attention_value_correlation.txt\")\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write('\\n'.join(lines))\n",
    "\n",
    "        print(f\"Report saved to {report_path}\")\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def run_analysis(self, texts=None):\n",
    "        \"\"\"\n",
    "        Run the complete attention-value correlation analysis pipeline.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "\n",
    "            if self.model is None:\n",
    "                return \"Model loading failed. Cannot continue analysis.\"\n",
    "\n",
    "        if texts is None or not texts:\n",
    "            texts = [\n",
    "                \"The concept of attention in transformer models relates to how tokens interact.\",\n",
    "                \"In deep learning, reference points help establish coordinate systems for representation.\",\n",
    "                \"Attention mechanisms create dynamic connections between tokens in a sequence.\",\n",
    "                \"Self-attention allows each token to gather information from all other tokens.\"\n",
    "            ]\n",
    "\n",
    "        stats_results = self.analyze_samples(texts)\n",
    "\n",
    "        if stats_results:\n",
    "            report = self.generate_statistics_report()\n",
    "        else:\n",
    "            report = \"Analysis did not produce valid statistics.\"\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Analysis completed in {total_time/60:.2f} minutes\")\n",
    "\n",
    "        return report\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.model is not None:\n",
    "            self.model = self.model.to(\"cpu\")\n",
    "            del self.model\n",
    "            self.model = None\n",
    "\n",
    "        if self.tokenizer is not None:\n",
    "            del self.tokenizer\n",
    "            self.tokenizer = None\n",
    "\n",
    "        self.hidden_states = None\n",
    "        self.attention_matrices = None\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"Resources cleaned up\")\n",
    "\n",
    "\n",
    "def get_sample_texts_from_dataset(dataset_path, n_samples=100):\n",
    "    \"\"\"\n",
    "    Extract sample texts from a dataset for analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(dataset_path)\n",
    "        print(f\"Loaded dataset with {len(data)} rows\")\n",
    "\n",
    "        if 'text' not in data.columns:\n",
    "            potential_text_columns = ['content', 'sentence', 'document', 'passage']\n",
    "            found_column = None\n",
    "\n",
    "            for col in potential_text_columns:\n",
    "                if col in data.columns:\n",
    "                    found_column = col\n",
    "                    break\n",
    "\n",
    "            if found_column:\n",
    "                print(f\"No 'text' column found, using '{found_column}' instead\")\n",
    "            else:\n",
    "                print(\"Error: No suitable text column found in dataset\")\n",
    "                return []\n",
    "        else:\n",
    "            found_column = 'text'\n",
    "\n",
    "        if len(data) > n_samples:\n",
    "            samples = data.sample(n_samples)\n",
    "        else:\n",
    "            samples = data\n",
    "\n",
    "        texts = samples[found_column].tolist()\n",
    "\n",
    "        texts = [t for t in texts if t is not None and str(t).strip()]\n",
    "\n",
    "        return texts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the Attention-Value Correlation Analysis.\"\"\"\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        is_colab = True\n",
    "    except ImportError:\n",
    "        is_colab = False\n",
    "        print(\"Not running in Google Colab, skipping drive mount\")\n",
    "\n",
    "    model_name = \"meta-llama/llama-3.2-3B\" \n",
    "\n",
    "    if is_colab:\n",
    "        dataset_path = \"/content/drive/MyDrive/wiki_dataset_position.csv\"  \n",
    "        output_dir = f\"/content/drive/MyDrive/Sink/attn_value_correlation/{model_name.replace('/', '_')}\"\n",
    "    else:\n",
    "        dataset_path = \"./dataset.csv\"\n",
    "        output_dir = f\"./attn_value_correlation_{model_name.replace('/', '_')}\"\n",
    "\n",
    "    analyzer = AttentionValueCorrelationAnalysis(\n",
    "        model_name=model_name,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    # Turn off debug mode for production runs\n",
    "    analyzer.debug = False\n",
    "\n",
    "    texts = get_sample_texts_from_dataset(dataset_path, n_samples=500) \n",
    "\n",
    "    if not texts:\n",
    "        texts = [\n",
    "            \"The concept of attention in transformer models relates to how tokens interact.\",\n",
    "            \"In deep learning, reference points help establish coordinate systems for representation.\",\n",
    "            \"Attention mechanisms create dynamic connections between tokens in a sequence.\",\n",
    "            \"Self-attention allows each token to gather information from all other tokens.\"\n",
    "        ]\n",
    "\n",
    "    try:\n",
    "        print(f\"Running analysis with {len(texts)} text samples\")\n",
    "\n",
    "        analysis_report = analyzer.run_analysis(texts=texts)\n",
    "\n",
    "        print(\"\\nATTENTION-VALUE CORRELATION ANALYSIS SUMMARY:\")\n",
    "        print(\"==========================================\")\n",
    "\n",
    "        print(analysis_report)\n",
    "\n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Analysis error: {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        analyzer.cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
