{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe241848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "class FiedlerCorrelationAnalysis:\n",
    "    \"\"\"\n",
    "    Analyzes correlations between Fiedler values (algebraic connectivity) and\n",
    "    other attention graph properties in transformer models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, output_dir=\"./fiedler_correlation_analysis\"):\n",
    "        \"\"\"Initialize the analysis with model name and output directory.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = output_dir\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.results = {}\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def load_model(self, use_4bit=True):\n",
    "        \"\"\"Load the model and tokenizer with robust error handling.\"\"\"\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "\n",
    "            # try 4-bit quantization\n",
    "            if use_4bit:\n",
    "                try:\n",
    "                    quantization_config = BitsAndBytesConfig(\n",
    "                        load_in_4bit=True,\n",
    "                        bnb_4bit_compute_dtype=torch.float16\n",
    "                    )\n",
    "\n",
    "                    self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                        self.model_name,\n",
    "                        torch_dtype=torch.float16,\n",
    "                        quantization_config=quantization_config,\n",
    "                        device_map=\"auto\",\n",
    "                        trust_remote_code=True\n",
    "                    )\n",
    "                    print(\"Model loaded with 4-bit quantization\")\n",
    "                except (ImportError, ModuleNotFoundError) as e:\n",
    "                    print(f\"BitsAndBytes not available: {e}\")\n",
    "                    print(\"Falling back to standard loading...\")\n",
    "                    self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                        self.model_name,\n",
    "                        torch_dtype=torch.float16,\n",
    "                        device_map=\"auto\",\n",
    "                        trust_remote_code=True\n",
    "                    )\n",
    "            else:\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"This might be due to an unsupported model architecture or outdated transformers library.\")\n",
    "            print(\"Try updating with: pip install --upgrade transformers\")\n",
    "            print(\"Or install from source: pip install git+https://github.com/huggingface/transformers.git\")\n",
    "            print(\"Using a dummy model for testing...\")\n",
    "\n",
    "            # Create a dummy model for testing purposes\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "            # Try to load a simpler model that's likely to work\n",
    "            fallback_model = \"gpt2\"\n",
    "            print(f\"Loading fallback model: {fallback_model}\")\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(fallback_model)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    fallback_model,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "            except Exception as e2:\n",
    "                print(f\"Error loading fallback model: {e2}\")\n",
    "                return None, None\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Model loaded successfully\")\n",
    "        return self.model, self.tokenizer\n",
    "\n",
    "    def compute_laplacian_eigenvalues(self, attention_matrix, threshold=0.01):\n",
    "        \"\"\"\n",
    "        Compute Laplacian matrix eigenvalues from an attention matrix.\n",
    "        \"\"\"\n",
    "        # Add safety checks for attention matrix\n",
    "        if attention_matrix is None or attention_matrix.size == 0:\n",
    "            return {\n",
    "                \"fiedler_value\": 0.0,\n",
    "                \"star_likeness\": 0.0,\n",
    "                \"degree_centralization\": 0.0,\n",
    "                \"error\": \"Empty attention matrix\"\n",
    "            }\n",
    "\n",
    "        # Ensure attention matrix has at least 2x2 dimensions for meaningful analysis\n",
    "        if min(attention_matrix.shape) < 2:\n",
    "            return {\n",
    "                \"fiedler_value\": 0.0,\n",
    "                \"star_likeness\": 0.0,\n",
    "                \"error\": \"Matrix too small for analysis\"\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Binarize the attention matrix based on threshold to create adjacency matrix\n",
    "            adjacency_matrix = (attention_matrix > threshold).astype(float)\n",
    "\n",
    "            connection_count = np.sum(adjacency_matrix)\n",
    "            connection_density = float(connection_count/adjacency_matrix.size)\n",
    "\n",
    "            if connection_count < 2:\n",
    "                return {\n",
    "                    \"fiedler_value\": 0.0,\n",
    "                    \"star_likeness\": 0.0,\n",
    "                    \"error\": \"Too few connections after thresholding\"\n",
    "                }\n",
    "\n",
    "            # Create a graph to analyze the structure\n",
    "            G = nx.DiGraph()\n",
    "            rows, cols = np.where(adjacency_matrix > 0)\n",
    "            edges = list(zip(rows.tolist(), cols.tolist()))\n",
    "            G.add_edges_from(edges)\n",
    "\n",
    "            # Compute degree metrics\n",
    "            in_degrees = np.array([d for _, d in G.in_degree()])\n",
    "\n",
    "            # Compute star-likeness metrics\n",
    "            total_nodes = len(in_degrees)\n",
    "            max_in_degree = np.max(in_degrees) if len(in_degrees) > 0 else 0\n",
    "\n",
    "            # Basic star-likeness\n",
    "            star_likeness = max_in_degree / max(1, total_nodes - 1)\n",
    "\n",
    "            # Calculate degree centralization\n",
    "            max_possible_diff = (total_nodes - 1) * (total_nodes - 1)\n",
    "            if max_possible_diff > 0 and max_in_degree > 0:\n",
    "                sum_diff = sum(max_in_degree - d for d in in_degrees)\n",
    "                degree_centralization = sum_diff / max_possible_diff\n",
    "            else:\n",
    "                degree_centralization = 0.0\n",
    "\n",
    "            # Calculate degree variance\n",
    "            degree_variance = np.var(in_degrees) if len(in_degrees) > 1 else 0.0\n",
    "\n",
    "            # Symmetrized adjacency for a sensible Laplacian\n",
    "            adj_sym = (adjacency_matrix + adjacency_matrix.T) / 2.0\n",
    "\n",
    "            # Compute the Laplacian\n",
    "            degree_matrix = np.diag(np.sum(adj_sym, axis=1))\n",
    "            laplacian_matrix = degree_matrix - adj_sym\n",
    "\n",
    "            # Compute eigenvalues of the Laplacian\n",
    "            eigenvalues = np.linalg.eigvalsh(laplacian_matrix)\n",
    "            eigenvalues = np.sort(eigenvalues)  # Sort in ascending order\n",
    "\n",
    "            # Second eigenvalue (algebraic connectivity)\n",
    "            fiedler_value = float(eigenvalues[1]) if len(eigenvalues) > 1 else 0.0\n",
    "\n",
    "            return {\n",
    "                \"fiedler_value\": fiedler_value,\n",
    "                \"star_likeness\": float(star_likeness),\n",
    "                \"degree_centralization\": float(degree_centralization),\n",
    "                \"degree_variance\": float(degree_variance),\n",
    "                \"connection_density\": connection_density\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"fiedler_value\": 0.0,\n",
    "                \"star_likeness\": 0.0,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    def analyze_text(self, text, thresholds=[0.01, 0.05]):\n",
    "        \"\"\"\n",
    "        Analyze a text sample and compute metrics for each layer's attention matrix.\n",
    "        \"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            print(\"Model or tokenizer not loaded!\")\n",
    "            return {\"error\": \"Model or tokenizer not loaded\"}\n",
    "\n",
    "        try:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(\n",
    "                    **inputs,\n",
    "                    output_attentions=True,\n",
    "                    return_dict=True\n",
    "                )\n",
    "\n",
    "            # Extract attention patterns\n",
    "            attentions = outputs.attentions  # tuple of (layer, batch, head, seq_len, seq_len)\n",
    "\n",
    "            # Initialize results structure\n",
    "            results = {\n",
    "                \"thresholds\": thresholds,\n",
    "                \"layers\": {}\n",
    "            }\n",
    "\n",
    "            # Process each layer\n",
    "            for layer_idx, layer_attention in enumerate(attentions):\n",
    "                # Each layer has shape (batch, head, seq_len, seq_len)\n",
    "                layer_attention = layer_attention[0].cpu().numpy()  # shape: (head, seq_len, seq_len)\n",
    "\n",
    "                # Compute average attention pattern across all heads\n",
    "                avg_attention = np.mean(layer_attention, axis=0)\n",
    "\n",
    "                # Store layer results\n",
    "                results[\"layers\"][str(layer_idx)] = {}\n",
    "\n",
    "                # Analyze for each threshold\n",
    "                for threshold in thresholds:\n",
    "                    # Compute Laplacian eigenvalues and graph metrics\n",
    "                    metrics = self.compute_laplacian_eigenvalues(\n",
    "                        avg_attention,\n",
    "                        threshold=threshold\n",
    "                    )\n",
    "\n",
    "                    results[\"layers\"][str(layer_idx)][str(threshold)] = metrics\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing text: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def analyze_samples(self, texts, thresholds=[0.01, 0.05]):\n",
    "        \"\"\"\n",
    "        Analyze multiple text samples and collect metrics for correlation analysis.\n",
    "        \"\"\"\n",
    "        print(f\"Analyzing {len(texts)} text samples...\")\n",
    "\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "\n",
    "        # Initialize data collectors for each threshold\n",
    "        metric_collectors = {}\n",
    "        for threshold in thresholds:\n",
    "            threshold_key = str(threshold)\n",
    "            metric_collectors[threshold_key] = {}\n",
    "\n",
    "        # Analyze each text\n",
    "        successful_analyses = 0\n",
    "        for i, text in enumerate(tqdm(texts, desc=\"Processing texts\")):\n",
    "            try:\n",
    "                # Process text\n",
    "                result = self.analyze_text(text, thresholds)\n",
    "\n",
    "                if \"error\" in result:\n",
    "                    continue\n",
    "\n",
    "                successful_analyses += 1\n",
    "\n",
    "                for layer_idx, layer_data in result[\"layers\"].items():\n",
    "                    if layer_idx not in metric_collectors[str(thresholds[0])]:\n",
    "                        for threshold in thresholds:\n",
    "                            threshold_key = str(threshold)\n",
    "                            metric_collectors[threshold_key][layer_idx] = {\n",
    "                                \"fiedler_values\": [],\n",
    "                                \"star_likeness\": [],\n",
    "                                \"degree_centralization\": [],\n",
    "                                \"degree_variance\": [],\n",
    "                                \"connection_densities\": []\n",
    "                            }\n",
    "\n",
    "                    for threshold in thresholds:\n",
    "                        threshold_key = str(threshold)\n",
    "                        if threshold_key in layer_data:\n",
    "                            metrics = layer_data[threshold_key]\n",
    "\n",
    "                            collectors = metric_collectors[threshold_key][layer_idx]\n",
    "\n",
    "                            if \"fiedler_value\" in metrics:\n",
    "                                collectors[\"fiedler_values\"].append(metrics[\"fiedler_value\"])\n",
    "\n",
    "                            if \"star_likeness\" in metrics:\n",
    "                                collectors[\"star_likeness\"].append(metrics[\"star_likeness\"])\n",
    "\n",
    "                            if \"degree_centralization\" in metrics:\n",
    "                                collectors[\"degree_centralization\"].append(metrics[\"degree_centralization\"])\n",
    "\n",
    "                            if \"degree_variance\" in metrics:\n",
    "                                collectors[\"degree_variance\"].append(metrics[\"degree_variance\"])\n",
    "\n",
    "                            if \"connection_density\" in metrics:\n",
    "                                collectors[\"connection_densities\"].append(metrics[\"connection_density\"])\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                if (i+1) % 5 == 0:\n",
    "                    print(f\"Processed {i+1}/{len(texts)} samples\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing text {i+1}: {str(e)}\")\n",
    "\n",
    "        print(f\"Successfully analyzed {successful_analyses}/{len(texts)} samples\")\n",
    "\n",
    "        if successful_analyses > 2:\n",
    "            correlation_results = self.compute_correlations(metric_collectors, thresholds)\n",
    "\n",
    "            self.results = {\n",
    "                \"metric_collectors\": metric_collectors,\n",
    "                \"correlation_results\": correlation_results,\n",
    "                \"thresholds\": thresholds\n",
    "            }\n",
    "\n",
    "            return correlation_results\n",
    "        else:\n",
    "            print(\"Not enough successful analyses to compute correlations\")\n",
    "            return {}\n",
    "\n",
    "    def compute_correlations(self, metric_collectors, thresholds):\n",
    "        \"\"\"\n",
    "        Compute correlations between Fiedler values and other graph metrics,\n",
    "        including p-values for statistical significance testing.\n",
    "        \"\"\"\n",
    "        print(\"Computing correlations...\")\n",
    "\n",
    "        correlation_results = {}\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            threshold_key = str(threshold)\n",
    "            correlation_results[threshold_key] = {\n",
    "                \"by_layer\": {},\n",
    "                \"overall\": {\n",
    "                    \"fiedler_vs_star\": {\"corr\": None, \"p_value\": None},\n",
    "                    \"fiedler_vs_centralization\": {\"corr\": None, \"p_value\": None},\n",
    "                    \"fiedler_vs_variance\": {\"corr\": None, \"p_value\": None},\n",
    "                    \"fiedler_vs_density\": {\"corr\": None, \"p_value\": None}\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Process each layer\n",
    "            for layer_idx, layer_data in metric_collectors[threshold_key].items():\n",
    "                correlation_results[threshold_key][\"by_layer\"][layer_idx] = {}\n",
    "\n",
    "                # Calculate correlations for this layer if we have enough data\n",
    "                fiedler_values = layer_data[\"fiedler_values\"]\n",
    "\n",
    "                # We need at least 3 samples for meaningful correlation\n",
    "                if len(fiedler_values) >= 3:\n",
    "                    correlations = {}\n",
    "\n",
    "                    # Correlation with star-likeness\n",
    "                    if len(layer_data[\"star_likeness\"]) >= 3:\n",
    "                        if len(fiedler_values) != len(layer_data[\"star_likeness\"]):\n",
    "                            min_length = min(len(fiedler_values), len(layer_data[\"star_likeness\"]))\n",
    "                            f_values = fiedler_values[:min_length]\n",
    "                            s_values = layer_data[\"star_likeness\"][:min_length]\n",
    "                        else:\n",
    "                            f_values = fiedler_values\n",
    "                            s_values = layer_data[\"star_likeness\"]\n",
    "\n",
    "                        if np.std(f_values) > 0 and np.std(s_values) > 0:\n",
    "                            # Use scipy.stats.pearsonr instead of np.corrcoef to get p-value\n",
    "                            corr, p_value = stats.pearsonr(f_values, s_values)\n",
    "                            correlations[\"fiedler_vs_star\"] = {\n",
    "                                \"corr\": float(corr),\n",
    "                                \"p_value\": float(p_value)\n",
    "                            }\n",
    "\n",
    "                    # Correlation with degree centralization\n",
    "                    if len(layer_data[\"degree_centralization\"]) >= 3:\n",
    "                        if len(fiedler_values) != len(layer_data[\"degree_centralization\"]):\n",
    "                            min_length = min(len(fiedler_values), len(layer_data[\"degree_centralization\"]))\n",
    "                            f_values = fiedler_values[:min_length]\n",
    "                            c_values = layer_data[\"degree_centralization\"][:min_length]\n",
    "                        else:\n",
    "                            f_values = fiedler_values\n",
    "                            c_values = layer_data[\"degree_centralization\"]\n",
    "\n",
    "                        if np.std(f_values) > 0 and np.std(c_values) > 0:\n",
    "                            corr, p_value = stats.pearsonr(f_values, c_values)\n",
    "                            correlations[\"fiedler_vs_centralization\"] = {\n",
    "                                \"corr\": float(corr),\n",
    "                                \"p_value\": float(p_value)\n",
    "                            }\n",
    "\n",
    "                    # Correlation with degree variance\n",
    "                    if len(layer_data[\"degree_variance\"]) >= 3:\n",
    "                        if len(fiedler_values) != len(layer_data[\"degree_variance\"]):\n",
    "                            min_length = min(len(fiedler_values), len(layer_data[\"degree_variance\"]))\n",
    "                            f_values = fiedler_values[:min_length]\n",
    "                            v_values = layer_data[\"degree_variance\"][:min_length]\n",
    "                        else:\n",
    "                            f_values = fiedler_values\n",
    "                            v_values = layer_data[\"degree_variance\"]\n",
    "\n",
    "                        if np.std(f_values) > 0 and np.std(v_values) > 0:\n",
    "                            corr, p_value = stats.pearsonr(f_values, v_values)\n",
    "                            correlations[\"fiedler_vs_variance\"] = {\n",
    "                                \"corr\": float(corr),\n",
    "                                \"p_value\": float(p_value)\n",
    "                            }\n",
    "\n",
    "                    # Correlation with connection density\n",
    "                    if len(layer_data[\"connection_densities\"]) >= 3:\n",
    "                        if len(fiedler_values) != len(layer_data[\"connection_densities\"]):\n",
    "                            min_length = min(len(fiedler_values), len(layer_data[\"connection_densities\"]))\n",
    "                            f_values = fiedler_values[:min_length]\n",
    "                            d_values = layer_data[\"connection_densities\"][:min_length]\n",
    "                        else:\n",
    "                            f_values = fiedler_values\n",
    "                            d_values = layer_data[\"connection_densities\"]\n",
    "\n",
    "                        if np.std(f_values) > 0 and np.std(d_values) > 0:\n",
    "                            corr, p_value = stats.pearsonr(f_values, d_values)\n",
    "                            correlations[\"fiedler_vs_density\"] = {\n",
    "                                \"corr\": float(corr),\n",
    "                                \"p_value\": float(p_value)\n",
    "                            }\n",
    "\n",
    "                    # Store layer-specific correlations\n",
    "                    correlation_results[threshold_key][\"by_layer\"][layer_idx] = correlations\n",
    "\n",
    "            # Calculate overall correlations for this threshold\n",
    "            overall = correlation_results[threshold_key][\"overall\"]\n",
    "\n",
    "            star_corrs = []\n",
    "            star_p_values = []\n",
    "            central_corrs = []\n",
    "            central_p_values = []\n",
    "            var_corrs = []\n",
    "            var_p_values = []\n",
    "            density_corrs = []\n",
    "            density_p_values = []\n",
    "\n",
    "            for layer_corrs in correlation_results[threshold_key][\"by_layer\"].values():\n",
    "                if \"fiedler_vs_star\" in layer_corrs:\n",
    "                    star_corrs.append(layer_corrs[\"fiedler_vs_star\"][\"corr\"])\n",
    "                    star_p_values.append(layer_corrs[\"fiedler_vs_star\"][\"p_value\"])\n",
    "                if \"fiedler_vs_centralization\" in layer_corrs:\n",
    "                    central_corrs.append(layer_corrs[\"fiedler_vs_centralization\"][\"corr\"])\n",
    "                    central_p_values.append(layer_corrs[\"fiedler_vs_centralization\"][\"p_value\"])\n",
    "                if \"fiedler_vs_variance\" in layer_corrs:\n",
    "                    var_corrs.append(layer_corrs[\"fiedler_vs_variance\"][\"corr\"])\n",
    "                    var_p_values.append(layer_corrs[\"fiedler_vs_variance\"][\"p_value\"])\n",
    "                if \"fiedler_vs_density\" in layer_corrs:\n",
    "                    density_corrs.append(layer_corrs[\"fiedler_vs_density\"][\"corr\"])\n",
    "                    density_p_values.append(layer_corrs[\"fiedler_vs_density\"][\"p_value\"])\n",
    "\n",
    "            if len(star_corrs) > 0:\n",
    "                overall[\"fiedler_vs_star\"] = {\n",
    "                    \"corr\": float(np.mean(star_corrs)),\n",
    "                    \"p_value\": float(np.mean(star_p_values))\n",
    "                }\n",
    "            if len(central_corrs) > 0:\n",
    "                overall[\"fiedler_vs_centralization\"] = {\n",
    "                    \"corr\": float(np.mean(central_corrs)),\n",
    "                    \"p_value\": float(np.mean(central_p_values))\n",
    "                }\n",
    "            if len(var_corrs) > 0:\n",
    "                overall[\"fiedler_vs_variance\"] = {\n",
    "                    \"corr\": float(np.mean(var_corrs)),\n",
    "                    \"p_value\": float(np.mean(var_p_values))\n",
    "                }\n",
    "            if len(density_corrs) > 0:\n",
    "                overall[\"fiedler_vs_density\"] = {\n",
    "                    \"corr\": float(np.mean(density_corrs)),\n",
    "                    \"p_value\": float(np.mean(density_p_values))\n",
    "                }\n",
    "\n",
    "        return correlation_results\n",
    "\n",
    "    def generate_correlation_report(self):\n",
    "        \"\"\"\n",
    "        Generate a simple text report with correlation values and p-values.\n",
    "        \"\"\"\n",
    "        if not self.results or \"correlation_results\" not in self.results:\n",
    "            return \"No correlation results available. Run analyze_samples first.\"\n",
    "\n",
    "        correlation_results = self.results[\"correlation_results\"]\n",
    "        thresholds = self.results[\"thresholds\"]\n",
    "\n",
    "        lines = []\n",
    "        lines.append(f\"FIEDLER VALUE CORRELATION ANALYSIS FOR {self.model_name}\\n\")\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            threshold_key = str(threshold)\n",
    "            if threshold_key not in correlation_results:\n",
    "                continue\n",
    "\n",
    "            lines.append(f\"THRESHOLD: {threshold}\")\n",
    "            lines.append(\"=\" * 20)\n",
    "\n",
    "            # Overall correlations\n",
    "            overall = correlation_results[threshold_key][\"overall\"]\n",
    "            lines.append(\"\\nOVERALL CORRELATIONS:\")\n",
    "\n",
    "            for corr_name, corr_data in overall.items():\n",
    "                if corr_data is not None and isinstance(corr_data, dict) and \"corr\" in corr_data:\n",
    "                    corr_value = corr_data[\"corr\"]\n",
    "                    p_value = corr_data.get(\"p_value\", None)\n",
    "\n",
    "                    if p_value is not None:\n",
    "                        significant = \"**\" if p_value < 0.05 else \"\"\n",
    "                        lines.append(f\"{corr_name}: {corr_value:.4f} (p={p_value:.4f}){significant}\")\n",
    "                    else:\n",
    "                        lines.append(f\"{corr_name}: {corr_value:.4f}\")\n",
    "                elif corr_data is not None:  # Handle legacy format where corr_data is just a float\n",
    "                    lines.append(f\"{corr_name}: {corr_data:.4f}\")\n",
    "\n",
    "            lines.append(\"\\nLAYER-SPECIFIC CORRELATIONS (sample):\")\n",
    "            lines.append(\"(** indicates p < 0.05, statistically significant)\")\n",
    "\n",
    "            # Just show first few layers as samples\n",
    "            layer_count = 0\n",
    "            for layer_idx, layer_corrs in correlation_results[threshold_key][\"by_layer\"].items():\n",
    "                if not layer_corrs or layer_count >= 5:  \n",
    "                    continue\n",
    "\n",
    "                lines.append(f\"\\nLayer {layer_idx}:\")\n",
    "                for corr_name, corr_data in layer_corrs.items():\n",
    "                    if isinstance(corr_data, dict) and \"corr\" in corr_data:\n",
    "                        corr_value = corr_data[\"corr\"]\n",
    "                        p_value = corr_data.get(\"p_value\", None)\n",
    "\n",
    "                        if p_value is not None:\n",
    "                            significant = \"**\" if p_value < 0.05 else \"\"\n",
    "                            lines.append(f\"  {corr_name}: {corr_value:.4f} (p={p_value:.4f}){significant}\")\n",
    "                        else:\n",
    "                            lines.append(f\"  {corr_name}: {corr_value:.4f}\")\n",
    "                    else:  # Handle legacy format\n",
    "                        lines.append(f\"  {corr_name}: {corr_data:.4f}\")\n",
    "\n",
    "                layer_count += 1\n",
    "\n",
    "            if layer_count == 3:\n",
    "                lines.append(\"\\n(Additional layers omitted for brevity)\")\n",
    "\n",
    "            lines.append(\"\\n\")\n",
    "\n",
    "        report_path = os.path.join(self.output_dir, \"fiedler_correlations.txt\")\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write('\\n'.join(lines))\n",
    "\n",
    "        print(f\"Report saved to {report_path}\")\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def run_analysis(self, texts=None, thresholds=[0.01, 0.05]):\n",
    "        \"\"\"\n",
    "        Run the complete correlation analysis pipeline.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "\n",
    "            if self.model is None:\n",
    "                return \"Model loading failed. Cannot continue analysis.\"\n",
    "\n",
    "        # Use example texts if none provided\n",
    "        if texts is None or not texts:\n",
    "            texts = [\n",
    "                \"The concept of eigenvalues relates to how transformations affect spaces.\",\n",
    "                \"Algebraic connectivity measures how well-connected a graph is.\",\n",
    "                \"Star graphs have one central node connected to all other nodes.\",\n",
    "                \"In transformers, attention mechanisms create dynamic connections between tokens.\"\n",
    "            ]\n",
    "\n",
    "        correlation_results = self.analyze_samples(texts, thresholds)\n",
    "\n",
    "        if correlation_results:\n",
    "            report = self.generate_correlation_report()\n",
    "        else:\n",
    "            report = \"Analysis did not produce valid correlation results.\"\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Analysis completed in {total_time/60:.2f} minutes\")\n",
    "\n",
    "        return report\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources safely.\"\"\"\n",
    "        if self.model is not None:\n",
    "            try:\n",
    "                has_meta_params = False\n",
    "                for param in self.model.parameters():\n",
    "                    if hasattr(param, 'device') and param.device == torch.device(\"meta\"):\n",
    "                        has_meta_params = True\n",
    "                        break\n",
    "\n",
    "                if not has_meta_params:\n",
    "                    self.model = self.model.to(\"cpu\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not move model to CPU: {e}\")\n",
    "                print(\"This is normal for models with parameters offloaded to disk\")\n",
    "\n",
    "            del self.model\n",
    "            self.model = None\n",
    "\n",
    "        if self.tokenizer is not None:\n",
    "            del self.tokenizer\n",
    "            self.tokenizer = None\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"Resources cleaned up\")\n",
    "\n",
    "\n",
    "def get_sample_texts_from_dataset(dataset_path, n_samples=10):\n",
    "    \"\"\"\n",
    "    Extract sample texts from a dataset for analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(dataset_path)\n",
    "        print(f\"Loaded dataset with {len(data)} rows\")\n",
    "\n",
    "        if 'text' not in data.columns:\n",
    "            print(\"Error: No 'text' column found in dataset\")\n",
    "            return []\n",
    "\n",
    "        if len(data) > n_samples:\n",
    "            samples = data.sample(n_samples)\n",
    "        else:\n",
    "            samples = data\n",
    "\n",
    "        texts = samples['text'].tolist()\n",
    "        return texts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a01b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the Fiedler value correlation analysis.\"\"\"\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        # Mount Google Drive if in Colab\n",
    "        drive.mount('/content/drive')\n",
    "        is_colab = True\n",
    "    except ImportError:\n",
    "        is_colab = False\n",
    "        print(\"Not running in Google Colab, skipping drive mount\")\n",
    "\n",
    "    model_name = \"EleutherAI/pythia-12b\"  \n",
    "    print(f\"Using model: {model_name}\")\n",
    "\n",
    "    if is_colab:\n",
    "        dataset_path = \"/content/drive/MyDrive/wiki_dataset_position.csv\"\n",
    "        output_dir = f\"/content/drive/MyDrive/Sink/fiedler/{model_name.split('/')[-1]}\"\n",
    "    else:\n",
    "        dataset_path = \"./dataset.csv\"\n",
    "        output_dir = f\"./fiedler_corr_{model_name.split('/')[-1]}\"\n",
    "\n",
    "    analyzer = FiedlerCorrelationAnalysis(\n",
    "        model_name=model_name,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    # Get sample texts from dataset\n",
    "    n_samples = 500  \n",
    "    texts = get_sample_texts_from_dataset(dataset_path, n_samples=n_samples)\n",
    "\n",
    "    # If no texts could be loaded, use some default examples\n",
    "    if not texts:\n",
    "        texts = [\n",
    "            \"The concept of eigenvalues relates to how transformations affect spaces.\",\n",
    "            \"Algebraic connectivity measures how well-connected a graph is.\",\n",
    "            \"Star graphs have one central node connected to all other nodes.\",\n",
    "            \"In transformers, attention mechanisms create dynamic connections between tokens.\"\n",
    "        ]\n",
    "\n",
    "    # Run analysis with specified thresholds\n",
    "    try:\n",
    "        thresholds = [0.001, 0.005, 0.01, 0.02, 0.5, 0.1, 0.2]\n",
    "        print(f\"Running analysis with thresholds: {thresholds}\")\n",
    "\n",
    "        correlation_report = analyzer.run_analysis(texts=texts, thresholds=thresholds)\n",
    "\n",
    "        print(\"\\nSIMPLE CORRELATION SUMMARY:\")\n",
    "        print(\"==========================\")\n",
    "\n",
    "        print(correlation_report)\n",
    "\n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Analysis error: {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        analyzer.cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
