{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed779eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy, pearsonr, spearmanr\n",
    "import traceback\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import tabulate\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8320427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class UltraFastAttentionSinkKLAnalysis:\n",
    "    \"\"\"\n",
    "    Ultra-optimized version of the attention sink KL analysis designed for speed.\n",
    "    Focuses only on generating the final summary report with minimal computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, output_dir=\"./attention_sink_kl_analysis\",\n",
    "                max_seq_length=128, verbose=False, skip_layers=True, optimize_calcs=True):\n",
    "        \"\"\"Initialize the analysis with model name and output directory.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = output_dir\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.aggregated_results = {}\n",
    "        self.sample_metrics = []\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Speed optimization parameters\n",
    "        self.max_seq_length = max_seq_length  # Truncate sequences to this length\n",
    "        self.skip_layers = skip_layers        # Whether to analyze only every other layer\n",
    "        self.optimize_calcs = optimize_calcs  # Whether to optimize KL calculations\n",
    "\n",
    "        self.layer_count = 0\n",
    "\n",
    "        self.peak_memory = 0\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Optimization settings:\")\n",
    "            print(f\" - Max sequence length: {self.max_seq_length}\")\n",
    "            print(f\" - Skip layers: {self.skip_layers}\")\n",
    "            print(f\" - Optimize calculations: {self.optimize_calcs}\")\n",
    "\n",
    "    def log(self, message, force=False):\n",
    "        \"\"\"Log messages only when verbose is enabled or forced.\"\"\"\n",
    "        if self.verbose or force:\n",
    "            print(message)\n",
    "\n",
    "    def _track_memory(self):\n",
    "        \"\"\"Track GPU memory usage.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        current = torch.cuda.memory_allocated() / (1024 ** 3)  # GB\n",
    "        peak = torch.cuda.max_memory_allocated() / (1024 ** 3)  # GB\n",
    "\n",
    "        if peak > self.peak_memory:\n",
    "            self.peak_memory = peak\n",
    "\n",
    "    def load_model(self, use_4bit=True):\n",
    "        \"\"\"Load the decoder-only model and tokenizer.\"\"\"\n",
    "        self.log(f\"Loading model: {self.model_name}\", force=True)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.tokenizer.model_max_length = self.max_seq_length\n",
    "\n",
    "        if use_4bit:\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "\n",
    "                quantization_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_compute_dtype=torch.float16\n",
    "                )\n",
    "\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    quantization_config=quantization_config,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                self.log(f\"Error with 4-bit quantization: {e}\", force=True)\n",
    "                self.log(\"Falling back to standard loading...\", force=True)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "\n",
    "        # Get model layer count\n",
    "        self.layer_count = self._get_model_layer_count()\n",
    "        self.log(f\"Model has {self.layer_count} layers\", force=True)\n",
    "\n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        self._track_memory()\n",
    "\n",
    "        self.log(f\"Model loaded: {self.model_name}\", force=True)\n",
    "        return self.model, self.tokenizer\n",
    "\n",
    "    def _get_model_layer_count(self):\n",
    "        \"\"\"Get the number of layers in the model.\"\"\"\n",
    "        if not self.model:\n",
    "            return 0\n",
    "\n",
    "        try:\n",
    "            # Check for config attribute with num_hidden_layers (most common)\n",
    "            if hasattr(self.model, 'config') and hasattr(self.model.config, 'num_hidden_layers'):\n",
    "                return self.model.config.num_hidden_layers\n",
    "\n",
    "            # For BERT-like models\n",
    "            elif hasattr(self.model, 'encoder') and hasattr(self.model.encoder, 'layer'):\n",
    "                return len(self.model.encoder.layer)\n",
    "\n",
    "            # For GPT-like models\n",
    "            elif hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'h'):\n",
    "                return len(self.model.transformer.h)\n",
    "\n",
    "            # For LLaMA, Qwen models\n",
    "            elif hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):\n",
    "                return len(self.model.model.layers)\n",
    "\n",
    "            # Direct layers attribute (some newer models)\n",
    "            elif hasattr(self.model, 'layers'):\n",
    "                return len(self.model.layers)\n",
    "\n",
    "            # Fallback to common layer counts\n",
    "            else:\n",
    "                return 32  \n",
    "\n",
    "        except Exception as e:\n",
    "            self.log(f\"Error detecting layer count: {e}. Using default value 32.\", force=True)\n",
    "            return 32  # Fallback to a reasonable default\n",
    "\n",
    "    def identify_attention_sinks(self, attention_matrix, threshold=0.9):\n",
    "        \"\"\"\n",
    "        Optimized function to identify attention sinks in an attention matrix.\n",
    "        \"\"\"\n",
    "        # Get dimensions\n",
    "        num_heads, seq_len, _ = attention_matrix.shape\n",
    "\n",
    "        # Calculate incoming attention for each token across all heads\n",
    "        # Sum across dim=0 (heads) and dim=1 (source tokens)\n",
    "        total_incoming_attention = attention_matrix.sum(axis=(0, 1))\n",
    "\n",
    "        # Calculate threshold value based on percentile\n",
    "        threshold_value = np.percentile(total_incoming_attention, threshold * 100)\n",
    "\n",
    "        # Identify sinks (tokens with incoming attention above threshold)\n",
    "        sink_indices = np.where(total_incoming_attention >= threshold_value)[0]\n",
    "\n",
    "        # Calculate what percentage of total attention goes to sinks\n",
    "        total_attention = total_incoming_attention.sum()\n",
    "        sink_attention = total_incoming_attention[sink_indices].sum()\n",
    "        sink_attention_percentage = (sink_attention / total_attention) * 100 if total_attention > 0 else 0\n",
    "\n",
    "        if not self.optimize_calcs:\n",
    "            # Calculate incoming attention per head for each token\n",
    "            per_head_incoming = np.zeros((num_heads, seq_len))\n",
    "            for h in range(num_heads):\n",
    "                per_head_incoming[h] = attention_matrix[h].sum(axis=0)  # Sum over source tokens\n",
    "\n",
    "            # Calculate attention concentration\n",
    "            head_sink_concentration = np.zeros(num_heads)\n",
    "            for h in range(num_heads):\n",
    "                head_total = per_head_incoming[h].sum()\n",
    "                head_to_sinks = per_head_incoming[h][sink_indices].sum()\n",
    "                head_sink_concentration[h] = (head_to_sinks / head_total) if head_total > 0 else 0\n",
    "\n",
    "            avg_sink_concentration = np.mean(head_sink_concentration)\n",
    "        else:\n",
    "            avg_sink_concentration = 0\n",
    "\n",
    "            if len(sink_indices) > 0:\n",
    "                # Take a sample of attention to estimate concentration\n",
    "                sample_heads = min(num_heads, 4)  \n",
    "                sample_concentration = 0\n",
    "\n",
    "                for h in range(sample_heads):\n",
    "                    head_attention = attention_matrix[h]\n",
    "                    head_total = head_attention.sum()\n",
    "                    if head_total > 0:\n",
    "                        head_to_sinks = head_attention[:, sink_indices].sum()\n",
    "                        sample_concentration += (head_to_sinks / head_total)\n",
    "\n",
    "                avg_sink_concentration = sample_concentration / sample_heads\n",
    "\n",
    "        return {\n",
    "            \"sink_indices\": sink_indices.tolist(),\n",
    "            \"sink_count\": len(sink_indices),\n",
    "            \"sink_attention_percentage\": float(sink_attention_percentage),\n",
    "            \"avg_sink_concentration\": float(avg_sink_concentration)\n",
    "        }\n",
    "\n",
    "    def fast_kl_divergence(self, attn_matrix, sink_indices=None):\n",
    "        \"\"\"\n",
    "        Ultra-optimized KL divergence calculation focusing only on essential metrics.\n",
    "        Now with added statistical significance (p-value) testing.\n",
    "        \"\"\"\n",
    "        num_heads, seq_len, _ = attn_matrix.shape\n",
    "\n",
    "        # Ensure probabilities sum to 1 for each source token - using proper broadcasting\n",
    "        attn_probs = attn_matrix.copy()\n",
    "\n",
    "        # Handle row normalization properly \n",
    "        for h in range(num_heads):\n",
    "            for i in range(seq_len):\n",
    "                row_sum = attn_probs[h, i].sum()\n",
    "                if row_sum > 0:\n",
    "                    attn_probs[h, i] = attn_probs[h, i] / row_sum\n",
    "                else:\n",
    "                    # Set uniform distribution for zero rows\n",
    "                    attn_probs[h, i] = np.ones(seq_len) / seq_len\n",
    "\n",
    "        # 1. Calculate average attention distribution across heads\n",
    "        avg_attn = np.mean(attn_probs, axis=0)  # (seq_len, seq_len)\n",
    "\n",
    "        # 2. Calculate KL divergence between each head and the average\n",
    "        if self.optimize_calcs and seq_len > 20:\n",
    "            # For long sequences, sample only a subset of tokens\n",
    "            sample_tokens = min(20, seq_len)  # Use at most 20 tokens\n",
    "            sample_indices = np.linspace(0, seq_len-1, sample_tokens, dtype=int)\n",
    "\n",
    "            kl_to_avg = np.zeros(num_heads)\n",
    "            for h in range(num_heads):\n",
    "                head_kl = np.zeros(sample_tokens)\n",
    "                for i, idx in enumerate(sample_indices):\n",
    "                    head_kl[i] = entropy(attn_probs[h, idx], avg_attn[idx])\n",
    "                kl_to_avg[h] = np.mean(head_kl)\n",
    "        else:\n",
    "            # For shorter sequences, compute full KL divergence\n",
    "            kl_to_avg = np.zeros(num_heads)\n",
    "            for h in range(num_heads):\n",
    "                head_kl = np.zeros(seq_len)\n",
    "                for i in range(seq_len):\n",
    "                    head_kl[i] = entropy(attn_probs[h, i], avg_attn[i])\n",
    "                kl_to_avg[h] = np.mean(head_kl)\n",
    "\n",
    "        # Compute metrics for sink tokens\n",
    "        sink_metrics = {}\n",
    "        if sink_indices is not None and len(sink_indices) > 0:\n",
    "            # Create masked attention distributions without sinks\n",
    "            masked_attn = attn_probs.copy()\n",
    "\n",
    "            # Zero out attention to sinks and renormalize\n",
    "            for h in range(num_heads):\n",
    "                for i in range(seq_len):\n",
    "                    # Zero out attention to sinks\n",
    "                    for sink in sink_indices:\n",
    "                        if sink < seq_len:  # Make sure sink index is valid\n",
    "                            masked_attn[h, i, sink] = 0\n",
    "\n",
    "                    # Renormalize\n",
    "                    row_sum = masked_attn[h, i].sum()\n",
    "                    if row_sum > 0:\n",
    "                        masked_attn[h, i] = masked_attn[h, i] / row_sum\n",
    "                    else:\n",
    "                        # If all attention went to sinks, distribute uniformly over non-sinks\n",
    "                        non_sink_indices = [j for j in range(seq_len) if j not in sink_indices]\n",
    "                        if non_sink_indices:\n",
    "                            for j in non_sink_indices:\n",
    "                                masked_attn[h, i, j] = 1.0 / len(non_sink_indices)\n",
    "\n",
    "            # Calculate average masked attention\n",
    "            avg_masked_attn = np.mean(masked_attn, axis=0)\n",
    "\n",
    "            # KL divergence with and without sinks \n",
    "            if self.optimize_calcs and seq_len > 20:\n",
    "                sample_tokens = min(20, seq_len)\n",
    "                sample_indices = np.linspace(0, seq_len-1, sample_tokens, dtype=int)\n",
    "\n",
    "                kl_without_sinks = np.zeros(num_heads)\n",
    "                for h in range(num_heads):\n",
    "                    head_kl = np.zeros(sample_tokens)\n",
    "                    for i, idx in enumerate(sample_indices):\n",
    "                        head_kl[i] = entropy(masked_attn[h, idx], avg_masked_attn[idx])\n",
    "                    kl_without_sinks[h] = np.mean(head_kl)\n",
    "            else:\n",
    "                kl_without_sinks = np.zeros(num_heads)\n",
    "                for h in range(num_heads):\n",
    "                    head_kl = np.zeros(seq_len)\n",
    "                    for i in range(seq_len):\n",
    "                        head_kl[i] = entropy(masked_attn[h, i], avg_masked_attn[i])\n",
    "                    kl_without_sinks[h] = np.mean(head_kl)\n",
    "\n",
    "            # Compare KL divergence with and without sinks\n",
    "            kl_reduction = kl_to_avg - kl_without_sinks\n",
    "\n",
    "            # Add p-value testing for KL reduction using paired t-test\n",
    "            if not self.optimize_calcs and len(kl_to_avg) >= 2:\n",
    "                from scipy import stats\n",
    "                t_stat, p_value = stats.ttest_rel(kl_to_avg, kl_without_sinks)\n",
    "                kl_reduction_significant = bool(p_value < 0.05)  # Cast to bool for proper JSON serialization\n",
    "            else:\n",
    "                p_value = 1.0\n",
    "                kl_reduction_significant = False\n",
    "\n",
    "            # Calculate correlation between sink attention and KL divergence\n",
    "            if self.optimize_calcs:\n",
    "                sink_kl_correlation = 0.0  \n",
    "                correlation_p_value = 1.0\n",
    "                correlation_significant = False\n",
    "            else:\n",
    "                # Calculate correlation between sink attention and KL divergence\n",
    "                sink_attn_per_head = np.zeros((num_heads, len(sink_indices)))\n",
    "                for h in range(num_heads):\n",
    "                    for i, sink in enumerate(sink_indices):\n",
    "                        if sink < seq_len:  # Make sure sink index is valid\n",
    "                            sink_attn_per_head[h, i] = np.mean(attn_probs[h, :, sink])\n",
    "\n",
    "                sink_attention_avg = np.mean(sink_attn_per_head, axis=1)\n",
    "                if np.std(sink_attention_avg) > 1e-10 and np.std(kl_to_avg) > 1e-10:\n",
    "                    # Only calculate correlation if we have non-constant data\n",
    "                    try:\n",
    "                        from scipy import stats\n",
    "                        sink_kl_correlation, correlation_p_value = stats.pearsonr(sink_attention_avg, kl_to_avg)\n",
    "                        correlation_significant = bool(p_value < 0.05)  # Cast to bool for proper JSON serialization\n",
    "                    except:\n",
    "                        sink_kl_correlation = 0.0\n",
    "                        correlation_p_value = 1.0\n",
    "                        correlation_significant = False\n",
    "                else:\n",
    "                    sink_kl_correlation = np.nan\n",
    "                    correlation_p_value = 1.0\n",
    "                    correlation_significant = False\n",
    "\n",
    "            sink_metrics = {\n",
    "                \"kl_without_sinks\": kl_without_sinks.tolist(),\n",
    "                \"kl_reduction\": kl_reduction.tolist(),\n",
    "                \"avg_kl_reduction\": float(np.mean(kl_reduction)),\n",
    "                \"kl_reduction_p_value\": float(p_value) if not self.optimize_calcs else 1.0,\n",
    "                \"kl_reduction_significant\": kl_reduction_significant,\n",
    "                \"sink_kl_correlation\": float(sink_kl_correlation) if not self.optimize_calcs else 0.0,\n",
    "                \"correlation_p_value\": float(correlation_p_value) if not self.optimize_calcs else 1.0,\n",
    "                \"correlation_significant\": correlation_significant\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"avg_kl_divergence\": float(np.mean(kl_to_avg)),\n",
    "            \"max_kl_divergence\": float(np.max(kl_to_avg)) if not self.optimize_calcs else 0.0,\n",
    "            \"min_kl_divergence\": float(np.min(kl_to_avg)) if not self.optimize_calcs else 0.0,\n",
    "            **sink_metrics\n",
    "        }\n",
    "\n",
    "    def analyze_text_and_aggregate(self, text, sink_thresholds=[0.8, 0.9, 0.95], sample_id=None):\n",
    "        \"\"\"\n",
    "        Analyze a text sample through the model and collect metrics directly without storing full results.\n",
    "        Optimized for speed by skipping layers and reducing computation.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=self.max_seq_length).to(self.model.device)\n",
    "        token_count = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                **inputs,\n",
    "                output_attentions=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "        attentions = outputs.attentions  # tuple of (layer, batch, head, seq_len, seq_len)\n",
    "\n",
    "        sample_metrics = {\n",
    "            \"sample_id\": sample_id,\n",
    "            \"token_count\": token_count,\n",
    "            \"layer_metrics\": {}\n",
    "        }\n",
    "\n",
    "        # Pick layers to analyze\n",
    "        if self.skip_layers:\n",
    "            # Analyze only a subset of layers for speed\n",
    "            if len(attentions) <= 6:\n",
    "                layer_indices = list(range(len(attentions)))\n",
    "            else:\n",
    "                # For larger models, analyze every other layer plus first and last\n",
    "                layer_indices = [0] + list(range(1, len(attentions)-1, 2)) + [len(attentions)-1]\n",
    "        else:\n",
    "            layer_indices = list(range(len(attentions)))\n",
    "\n",
    "        for layer_idx in layer_indices:\n",
    "            # Each layer has shape (batch, head, seq_len, seq_len)\n",
    "            layer_attention = attentions[layer_idx][0].cpu().numpy()  # shape: (head, seq_len, seq_len)\n",
    "\n",
    "            layer_data = {\n",
    "                \"sink_analysis\": {},\n",
    "                \"kl_metrics\": {}\n",
    "            }\n",
    "\n",
    "            # Calculate KL divergence without consideration of sinks (for baseline)\n",
    "            baseline_kl = self.fast_kl_divergence(layer_attention)\n",
    "            layer_data[\"kl_metrics\"][\"baseline\"] = baseline_kl\n",
    "\n",
    "            # Identify attention sinks at different thresholds\n",
    "            for threshold in sink_thresholds:\n",
    "                sink_info = self.identify_attention_sinks(\n",
    "                    layer_attention,\n",
    "                    threshold=threshold\n",
    "                )\n",
    "\n",
    "                # Calculate KL divergence with identified sinks\n",
    "                kl_metrics = self.fast_kl_divergence(\n",
    "                    layer_attention,\n",
    "                    sink_indices=sink_info[\"sink_indices\"]\n",
    "                )\n",
    "\n",
    "                layer_data[\"sink_analysis\"][str(threshold)] = sink_info\n",
    "                layer_data[\"kl_metrics\"][str(threshold)] = kl_metrics\n",
    "\n",
    "            sample_metrics[\"layer_metrics\"][str(layer_idx)] = layer_data\n",
    "\n",
    "        del outputs, attentions, layer_attention\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return sample_metrics\n",
    "\n",
    "    def run_aggregated_analysis(self, texts, sink_thresholds=[0.8, 0.9, 0.95], batch_size=8):\n",
    "        \"\"\"\n",
    "        Run the complete analysis on all samples and generate aggregated results.\n",
    "        Optimized for speed by processing in slightly larger batches and reducing computation.\n",
    "        \"\"\"\n",
    "        self.log(f\"Running analysis on {len(texts)} text samples...\", force=True)\n",
    "\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "\n",
    "        self.sample_metrics = []\n",
    "\n",
    "        # Process texts in batches to manage memory\n",
    "        total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "\n",
    "        progress_bar = tqdm(total=len(texts), desc=\"Processing samples\", disable=not self.verbose)\n",
    "        samples_processed = 0\n",
    "\n",
    "        for batch_idx in range(0, len(texts), batch_size):\n",
    "            batch_end = min(batch_idx + batch_size, len(texts))\n",
    "            batch_num = batch_idx//batch_size + 1\n",
    "\n",
    "            self.log(f\"\\nProcessing batch {batch_num}/{total_batches}: samples {batch_idx+1} to {batch_end}\", force=True)\n",
    "\n",
    "            # Process each text in the batch\n",
    "            batch_results = []\n",
    "            for i in range(batch_idx, batch_end):\n",
    "                try:\n",
    "                    # Analyze text and collect metrics\n",
    "                    metrics = self.analyze_text_and_aggregate(\n",
    "                        texts[i],\n",
    "                        sink_thresholds=sink_thresholds,\n",
    "                        sample_id=i+1\n",
    "                    )\n",
    "\n",
    "                    batch_results.append(metrics)\n",
    "\n",
    "                    samples_processed += 1\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.log(f\"Error analyzing text {i+1}: {str(e)}\", force=True)\n",
    "                    if self.verbose:\n",
    "                        traceback.print_exc()\n",
    "\n",
    "                    samples_processed += 1\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "            self.sample_metrics.extend(batch_results)\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            self._track_memory()\n",
    "\n",
    "            progress = samples_processed / len(texts)\n",
    "            if progress > 0 and progress < 1:\n",
    "                elapsed = progress_bar.format_dict[\"elapsed\"]\n",
    "                remaining = elapsed / progress - elapsed\n",
    "                self.log(f\"Progress: {progress:.1%} - Est. time remaining: {remaining/60:.1f} minutes\", force=True)\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        self.aggregated_results = self.aggregate_all_metrics(sink_thresholds)\n",
    "\n",
    "        self.generate_numerical_summary()\n",
    "\n",
    "        return self.aggregated_results\n",
    "\n",
    "    def aggregate_all_metrics(self, sink_thresholds):\n",
    "        \"\"\"\n",
    "        Aggregate metrics from all processed samples.\n",
    "        Optimized to focus only on the metrics needed for the final report.\n",
    "        \"\"\"\n",
    "        self.log(\"\\nAggregating results across all samples...\", force=True)\n",
    "\n",
    "        if not self.sample_metrics:\n",
    "            self.log(\"No sample metrics to aggregate!\", force=True)\n",
    "            return {}\n",
    "\n",
    "        # Get layer indices from first sample (may be partial if skip_layers=True)\n",
    "        first_sample = self.sample_metrics[0]\n",
    "        analyzed_layers = sorted([int(idx) for idx in first_sample[\"layer_metrics\"].keys()])\n",
    "        self.log(f\"Found {len(analyzed_layers)} analyzed layers\", force=True)\n",
    "\n",
    "        aggregated = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"total_samples\": len(self.sample_metrics),\n",
    "            \"thresholds\": sink_thresholds,\n",
    "            \"analyzed_layers\": analyzed_layers,\n",
    "            \"layer_count\": self.layer_count,  # Total model layers, not just analyzed\n",
    "            \"layer_stats\": {layer_idx: {} for layer_idx in analyzed_layers},\n",
    "            \"kl_trends\": {\n",
    "                \"baseline\": [],\n",
    "                **{str(threshold): [] for threshold in sink_thresholds}\n",
    "            },\n",
    "            \"kl_reduction_trends\": {str(threshold): [] for threshold in sink_thresholds},\n",
    "            \"sink_concentration_trends\": {str(threshold): [] for threshold in sink_thresholds}\n",
    "        }\n",
    "\n",
    "        for layer_idx in analyzed_layers:\n",
    "            layer_key = str(layer_idx)\n",
    "\n",
    "            metrics = {\n",
    "                \"baseline\": {\n",
    "                    \"avg_kl_divergence\": []\n",
    "                }\n",
    "            }\n",
    "\n",
    "            for threshold in sink_thresholds:\n",
    "                threshold_key = str(threshold)\n",
    "                metrics[threshold_key] = {\n",
    "                    \"avg_kl_divergence\": [],\n",
    "                    \"avg_kl_reduction\": [],\n",
    "                    \"kl_reduction_p_values\": [],\n",
    "                    \"correlation_p_values\": [],\n",
    "                    \"sink_count\": [],\n",
    "                    \"avg_sink_concentration\": []\n",
    "                }\n",
    "\n",
    "            # Collect metrics from each sample\n",
    "            for sample in self.sample_metrics:\n",
    "                if layer_key not in sample[\"layer_metrics\"]:\n",
    "                    continue\n",
    "\n",
    "                layer_data = sample[\"layer_metrics\"][layer_key]\n",
    "\n",
    "                # Baseline KL metrics \n",
    "                if \"baseline\" in layer_data[\"kl_metrics\"]:\n",
    "                    baseline_metrics = layer_data[\"kl_metrics\"][\"baseline\"]\n",
    "                    if \"avg_kl_divergence\" in baseline_metrics:\n",
    "                        metrics[\"baseline\"][\"avg_kl_divergence\"].append(baseline_metrics[\"avg_kl_divergence\"])\n",
    "\n",
    "                for threshold in sink_thresholds:\n",
    "                    threshold_key = str(threshold)\n",
    "\n",
    "                    if threshold_key not in layer_data[\"sink_analysis\"] or threshold_key not in layer_data[\"kl_metrics\"]:\n",
    "                        continue\n",
    "\n",
    "                    sink_info = layer_data[\"sink_analysis\"][threshold_key]\n",
    "                    kl_info = layer_data[\"kl_metrics\"][threshold_key]\n",
    "\n",
    "                    if \"avg_kl_divergence\" in kl_info:\n",
    "                        metrics[threshold_key][\"avg_kl_divergence\"].append(kl_info[\"avg_kl_divergence\"])\n",
    "\n",
    "                    if \"avg_kl_reduction\" in kl_info:\n",
    "                        metrics[threshold_key][\"avg_kl_reduction\"].append(kl_info[\"avg_kl_reduction\"])\n",
    "\n",
    "                    if \"sink_count\" in sink_info:\n",
    "                        metrics[threshold_key][\"sink_count\"].append(sink_info[\"sink_count\"])\n",
    "\n",
    "                    if \"avg_sink_concentration\" in sink_info:\n",
    "                        metrics[threshold_key][\"avg_sink_concentration\"].append(sink_info[\"avg_sink_concentration\"])\n",
    "\n",
    "                    if \"kl_reduction_p_value\" in kl_info:\n",
    "                        metrics[threshold_key][\"kl_reduction_p_values\"].append(kl_info[\"kl_reduction_p_value\"])\n",
    "\n",
    "                    if \"correlation_p_value\" in kl_info:\n",
    "                        metrics[threshold_key][\"correlation_p_values\"].append(kl_info[\"correlation_p_value\"])\n",
    "\n",
    "            layer_stats = {}\n",
    "\n",
    "            baseline_stats = {}\n",
    "            for key, values in metrics[\"baseline\"].items():\n",
    "                if values:\n",
    "                    baseline_stats[key] = {\n",
    "                        \"mean\": float(np.nanmean(values)),\n",
    "                        \"std\": float(np.nanstd(values)) if len(values) > 1 else 0.0,\n",
    "                        \"count\": len(values)\n",
    "                    }\n",
    "            layer_stats[\"baseline\"] = baseline_stats\n",
    "\n",
    "            for threshold in sink_thresholds:\n",
    "                threshold_key = str(threshold)\n",
    "                threshold_stats = {}\n",
    "                for key, values in metrics[threshold_key].items():\n",
    "                    if values:\n",
    "                        threshold_stats[key] = {\n",
    "                            \"mean\": float(np.nanmean(values)),\n",
    "                            \"std\": float(np.nanstd(values)) if len(values) > 1 else 0.0,\n",
    "                            \"count\": len(values)\n",
    "                        }\n",
    "                layer_stats[threshold_key] = threshold_stats\n",
    "\n",
    "            # Store layer statistics\n",
    "            aggregated[\"layer_stats\"][layer_idx] = layer_stats\n",
    "\n",
    "            # 1. Baseline KL trend\n",
    "            if \"avg_kl_divergence\" in baseline_stats:\n",
    "                aggregated[\"kl_trends\"][\"baseline\"].append({\n",
    "                    \"layer\": layer_idx,\n",
    "                    \"value\": baseline_stats[\"avg_kl_divergence\"][\"mean\"],\n",
    "                    \"std\": baseline_stats[\"avg_kl_divergence\"][\"std\"]\n",
    "                })\n",
    "\n",
    "            # 2. Threshold-specific trends \n",
    "            for threshold in sink_thresholds:\n",
    "                threshold_key = str(threshold)\n",
    "                threshold_stats = layer_stats.get(threshold_key, {})\n",
    "\n",
    "                # KL divergence trend\n",
    "                if \"avg_kl_divergence\" in threshold_stats:\n",
    "                    aggregated[\"kl_trends\"][threshold_key].append({\n",
    "                        \"layer\": layer_idx,\n",
    "                        \"value\": threshold_stats[\"avg_kl_divergence\"][\"mean\"],\n",
    "                        \"std\": threshold_stats[\"avg_kl_divergence\"][\"std\"]\n",
    "                    })\n",
    "\n",
    "                # KL reduction trend\n",
    "                if \"avg_kl_reduction\" in threshold_stats:\n",
    "                    aggregated[\"kl_reduction_trends\"][threshold_key].append({\n",
    "                        \"layer\": layer_idx,\n",
    "                        \"value\": threshold_stats[\"avg_kl_reduction\"][\"mean\"],\n",
    "                        \"std\": threshold_stats[\"avg_kl_reduction\"][\"std\"]\n",
    "                    })\n",
    "\n",
    "                # Sink concentration trend\n",
    "                if \"avg_sink_concentration\" in threshold_stats:\n",
    "                    aggregated[\"sink_concentration_trends\"][threshold_key].append({\n",
    "                        \"layer\": layer_idx,\n",
    "                        \"value\": threshold_stats[\"avg_sink_concentration\"][\"mean\"],\n",
    "                        \"std\": threshold_stats[\"avg_sink_concentration\"][\"std\"]\n",
    "                    })\n",
    "\n",
    "                # Compute combined p-values using Fisher's method\n",
    "                if \"kl_reduction_p_values\" in metrics[threshold_key] and metrics[threshold_key][\"kl_reduction_p_values\"]:\n",
    "                    from scipy import stats\n",
    "                    p_values = np.array(metrics[threshold_key][\"kl_reduction_p_values\"])\n",
    "                    valid_p = p_values[~np.isnan(p_values)]\n",
    "                    if len(valid_p) > 0:\n",
    "                        # Fisher's method: -2 * sum(ln(p))\n",
    "                        fisher_statistic = -2 * np.sum(np.log(valid_p + 1e-10))\n",
    "                        combined_p = 1 - stats.chi2.cdf(fisher_statistic, 2 * len(valid_p))\n",
    "                        threshold_stats[\"kl_reduction_significance\"] = {\n",
    "                            \"combined_p_value\": float(combined_p),\n",
    "                            \"significant\": bool(combined_p < 0.05),  \n",
    "                            \"sample_count\": len(valid_p)\n",
    "                        }\n",
    "\n",
    "                # Do the same for correlation p-values\n",
    "                if \"correlation_p_values\" in metrics[threshold_key] and metrics[threshold_key][\"correlation_p_values\"]:\n",
    "                    from scipy import stats\n",
    "                    p_values = np.array(metrics[threshold_key][\"correlation_p_values\"])\n",
    "                    valid_p = p_values[~np.isnan(p_values)]\n",
    "                    if len(valid_p) > 0:\n",
    "                        fisher_statistic = -2 * np.sum(np.log(valid_p + 1e-10))\n",
    "                        combined_p = 1 - stats.chi2.cdf(fisher_statistic, 2 * len(valid_p))\n",
    "                        threshold_stats[\"correlation_significance\"] = {\n",
    "                            \"combined_p_value\": float(combined_p),\n",
    "                            \"significant\": bool(combined_p < 0.05), \n",
    "                            \"sample_count\": len(valid_p)\n",
    "                        }\n",
    "\n",
    "        aggregated[\"cross_layer_analysis\"] = self.compute_cross_layer_correlations(aggregated, sink_thresholds)\n",
    "\n",
    "        raw_data_file = os.path.join(self.output_dir, \"aggregated_raw_data.json\")\n",
    "        with open(raw_data_file, 'w') as f:\n",
    "            json.dump(aggregated, f, indent=2)\n",
    "        self.log(f\"Saved detailed raw data to {raw_data_file}\", force=True)\n",
    "\n",
    "        return aggregated\n",
    "\n",
    "    def compute_cross_layer_correlations(self, aggregated_results, sink_thresholds):\n",
    "        \"\"\"\n",
    "        Compute correlations between different layers and metrics.\n",
    "        Add p-values for statistical significance assessment.\n",
    "\n",
    "        Args:\n",
    "            aggregated_results: The aggregated analysis results\n",
    "            sink_thresholds: List of thresholds used for sink analysis\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with correlation analyses\n",
    "        \"\"\"\n",
    "        cross_layer_analysis = {\n",
    "            \"layer_position_correlations\": {},\n",
    "            \"inter_metric_correlations\": {}\n",
    "        }\n",
    "\n",
    "        analyzed_layers = aggregated_results[\"analyzed_layers\"]\n",
    "\n",
    "        # 1. Correlation between layer position and metrics\n",
    "        metrics_to_correlate = [\n",
    "            (\"kl_reduction\", \"KL Reduction\"),\n",
    "            (\"sink_concentration\", \"Sink Concentration\")\n",
    "        ]\n",
    "\n",
    "        for metric_key, metric_name in metrics_to_correlate:\n",
    "            for threshold in sink_thresholds:\n",
    "                threshold_key = str(threshold)\n",
    "\n",
    "                if f\"{metric_key}_trends\" in aggregated_results and threshold_key in aggregated_results[f\"{metric_key}_trends\"]:\n",
    "                    trend_data = aggregated_results[f\"{metric_key}_trends\"][threshold_key]\n",
    "\n",
    "                    layers = [item[\"layer\"] for item in trend_data]\n",
    "                    values = [item[\"value\"] for item in trend_data]\n",
    "\n",
    "                    if len(layers) >= 2:  \n",
    "                        try:\n",
    "                            # Pearson correlation (linear relationship)\n",
    "                            pearson_r, pearson_p = pearsonr(layers, values)\n",
    "\n",
    "                            # Spearman correlation (monotonic but not necessarily linear)\n",
    "                            spearman_r, spearman_p = spearmanr(layers, values)\n",
    "\n",
    "                            cross_layer_analysis[\"layer_position_correlations\"][f\"{metric_key}_{threshold_key}\"] = {\n",
    "                                \"metric\": metric_name,\n",
    "                                \"threshold\": threshold,\n",
    "                                \"pearson_correlation\": float(pearson_r),\n",
    "                                \"pearson_p_value\": float(pearson_p),\n",
    "                                \"pearson_significant\": bool(pearson_p < 0.05), \n",
    "                                \"spearman_correlation\": float(spearman_r),\n",
    "                                \"spearman_p_value\": float(spearman_p),\n",
    "                                \"spearman_significant\": bool(spearman_p < 0.05), \n",
    "                                \"sample_size\": len(layers)\n",
    "                            }\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "        # 2. Correlations between different metrics\n",
    "        metrics_map = {\n",
    "            \"kl_reduction\": \"KL Reduction\",\n",
    "            \"sink_concentration\": \"Sink Concentration\"\n",
    "        }\n",
    "\n",
    "        for threshold in sink_thresholds:\n",
    "            threshold_key = str(threshold)\n",
    "\n",
    "            # Compare each pair of metrics\n",
    "            for metric1 in metrics_map:\n",
    "                for metric2 in metrics_map:\n",
    "                    if metric1 >= metric2:  # Avoid duplicate correlations and self-correlations\n",
    "                        continue\n",
    "\n",
    "                    # Extract data for both metrics\n",
    "                    trend1_key = f\"{metric1}_trends\"\n",
    "                    trend2_key = f\"{metric2}_trends\"\n",
    "\n",
    "                    if (trend1_key in aggregated_results and trend2_key in aggregated_results and\n",
    "                        threshold_key in aggregated_results[trend1_key] and\n",
    "                        threshold_key in aggregated_results[trend2_key]):\n",
    "\n",
    "                        trend1_data = aggregated_results[trend1_key][threshold_key]\n",
    "                        trend2_data = aggregated_results[trend2_key][threshold_key]\n",
    "\n",
    "                        # Match up the same layers\n",
    "                        values1 = []\n",
    "                        values2 = []\n",
    "                        layers = []\n",
    "\n",
    "                        for layer in analyzed_layers:\n",
    "                            val1 = next((item[\"value\"] for item in trend1_data if item[\"layer\"] == layer), None)\n",
    "                            val2 = next((item[\"value\"] for item in trend2_data if item[\"layer\"] == layer), None)\n",
    "\n",
    "                            if val1 is not None and val2 is not None:\n",
    "                                values1.append(val1)\n",
    "                                values2.append(val2)\n",
    "                                layers.append(layer)\n",
    "\n",
    "                        if len(values1) >= 2:  # Need at least 2 points for correlation\n",
    "                            try:\n",
    "                                pearson_r, pearson_p = pearsonr(values1, values2)\n",
    "\n",
    "                                spearman_r, spearman_p = spearmanr(values1, values2)\n",
    "\n",
    "                                corr_key = f\"{metric1}_{metric2}_{threshold_key}\"\n",
    "                                cross_layer_analysis[\"inter_metric_correlations\"][corr_key] = {\n",
    "                                    \"metric1\": metrics_map[metric1],\n",
    "                                    \"metric2\": metrics_map[metric2],\n",
    "                                    \"threshold\": threshold,\n",
    "                                    \"pearson_correlation\": float(pearson_r),\n",
    "                                    \"pearson_p_value\": float(pearson_p),\n",
    "                                    \"pearson_significant\": bool(pearson_p < 0.05),\n",
    "                                    \"spearman_correlation\": float(spearman_r),\n",
    "                                    \"spearman_p_value\": float(spearman_p),\n",
    "                                    \"spearman_significant\": bool(spearman_p < 0.05),\n",
    "                                    \"sample_size\": len(values1)\n",
    "                                }\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "        return cross_layer_analysis\n",
    "\n",
    "    def generate_numerical_summary(self):\n",
    "        \"\"\"Generate a concise numerical summary with just the numbers.\"\"\"\n",
    "        if not self.aggregated_results:\n",
    "            self.log(\"No aggregated results to report.\", force=True)\n",
    "            return None\n",
    "\n",
    "        self.log(\"\\nGenerating numerical summary...\", force=True)\n",
    "\n",
    "        sink_thresholds = self.aggregated_results[\"thresholds\"]\n",
    "        analyzed_layers = self.aggregated_results[\"analyzed_layers\"]\n",
    "        summary_path = os.path.join(self.output_dir, \"attention_sink_kl_summary.txt\")\n",
    "\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(\"ATTENTION SINK KL DIVERGENCE ANALYSIS - NUMERICAL SUMMARY\\n\")\n",
    "            f.write(\"======================================================\\n\\n\")\n",
    "\n",
    "            f.write(f\"Model: {self.model_name}\\n\")\n",
    "            f.write(f\"Samples: {self.aggregated_results['total_samples']}\\n\")\n",
    "            f.write(f\"Layers: {self.aggregated_results['layer_count']} (analyzed {len(analyzed_layers)} layers)\\n\")\n",
    "            f.write(f\"Max sequence length: {self.max_seq_length}\\n\")\n",
    "            f.write(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "            f.write(\"KL REDUCTION SUMMARY\\n\")\n",
    "            f.write(\"-------------------\\n\\n\")\n",
    "\n",
    "            headers = [\"Layer\"]\n",
    "            for threshold in sink_thresholds:\n",
    "                headers.append(f\"KL Red. (t={threshold})\")\n",
    "\n",
    "            table_data = []\n",
    "\n",
    "            for layer in analyzed_layers:\n",
    "                row = [layer]\n",
    "\n",
    "                for threshold in sink_thresholds:\n",
    "                    threshold_key = str(threshold)\n",
    "                    if threshold_key in self.aggregated_results[\"kl_reduction_trends\"]:\n",
    "                        layer_data = [item for item in self.aggregated_results[\"kl_reduction_trends\"][threshold_key]\n",
    "                                    if item[\"layer\"] == layer]\n",
    "\n",
    "                        if layer_data:\n",
    "                            row.append(f\"{layer_data[0]['value']:.4f}\")\n",
    "                        else:\n",
    "                            row.append(\"N/A\")\n",
    "                    else:\n",
    "                        row.append(\"N/A\")\n",
    "\n",
    "                table_data.append(row)\n",
    "\n",
    "            f.write(tabulate.tabulate(table_data, headers=headers, tablefmt=\"plain\"))\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "            f.write(\"SINK CONCENTRATION SUMMARY\\n\")\n",
    "            f.write(\"-------------------------\\n\\n\")\n",
    "\n",
    "            headers = [\"Layer\"]\n",
    "            for threshold in sink_thresholds:\n",
    "                headers.append(f\"Sink Conc. (t={threshold})\")\n",
    "\n",
    "            table_data = []\n",
    "\n",
    "            for layer in analyzed_layers:\n",
    "                row = [layer]\n",
    "\n",
    "                for threshold in sink_thresholds:\n",
    "                    threshold_key = str(threshold)\n",
    "                    if threshold_key in self.aggregated_results[\"sink_concentration_trends\"]:\n",
    "                        layer_data = [item for item in self.aggregated_results[\"sink_concentration_trends\"][threshold_key]\n",
    "                                    if item[\"layer\"] == layer]\n",
    "\n",
    "                        if layer_data:\n",
    "                            row.append(f\"{layer_data[0]['value']*100:.2f}%\")\n",
    "                        else:\n",
    "                            row.append(\"N/A\")\n",
    "                    else:\n",
    "                        row.append(\"N/A\")\n",
    "\n",
    "                table_data.append(row)\n",
    "\n",
    "            f.write(tabulate.tabulate(table_data, headers=headers, tablefmt=\"plain\"))\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "            f.write(\"OPTIMAL VALUES BY LAYER\\n\")\n",
    "            f.write(\"---------------------\\n\\n\")\n",
    "\n",
    "            headers = [\"Layer\", \"Max KL Red.\", \"Best Thresh.\", \"Max Conc.\", \"Best Thresh.\"]\n",
    "            table_data = []\n",
    "\n",
    "            for layer in analyzed_layers:\n",
    "                row = [layer]\n",
    "\n",
    "                # Best KL reduction\n",
    "                max_reduction = -float('inf')\n",
    "                best_reduction_threshold = None\n",
    "\n",
    "                for threshold in sink_thresholds:\n",
    "                    threshold_key = str(threshold)\n",
    "                    if threshold_key in self.aggregated_results[\"kl_reduction_trends\"]:\n",
    "                        layer_data = [item for item in self.aggregated_results[\"kl_reduction_trends\"][threshold_key]\n",
    "                                    if item[\"layer\"] == layer]\n",
    "\n",
    "                        if layer_data and layer_data[0][\"value\"] > max_reduction:\n",
    "                            max_reduction = layer_data[0][\"value\"]\n",
    "                            best_reduction_threshold = threshold\n",
    "\n",
    "                if max_reduction > -float('inf'):\n",
    "                    row.append(f\"{max_reduction:.4f}\")\n",
    "                    row.append(f\"{best_reduction_threshold}\")\n",
    "                else:\n",
    "                    row.append(\"N/A\")\n",
    "                    row.append(\"N/A\")\n",
    "\n",
    "                # Best sink concentration\n",
    "                max_concentration = -float('inf')\n",
    "                best_concentration_threshold = None\n",
    "\n",
    "                for threshold in sink_thresholds:\n",
    "                    threshold_key = str(threshold)\n",
    "                    if threshold_key in self.aggregated_results[\"sink_concentration_trends\"]:\n",
    "                        layer_data = [item for item in self.aggregated_results[\"sink_concentration_trends\"][threshold_key]\n",
    "                                    if item[\"layer\"] == layer]\n",
    "\n",
    "                        if layer_data and layer_data[0][\"value\"] > max_concentration:\n",
    "                            max_concentration = layer_data[0][\"value\"]\n",
    "                            best_concentration_threshold = threshold\n",
    "\n",
    "                if max_concentration > -float('inf'):\n",
    "                    row.append(f\"{max_concentration*100:.2f}%\")\n",
    "                    row.append(f\"{best_concentration_threshold}\")\n",
    "                else:\n",
    "                    row.append(\"N/A\")\n",
    "                    row.append(\"N/A\")\n",
    "\n",
    "                table_data.append(row)\n",
    "\n",
    "            f.write(tabulate.tabulate(table_data, headers=headers, tablefmt=\"plain\"))\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "            f.write(\"PERFORMANCE METRICS\\n\")\n",
    "            f.write(\"------------------\\n\")\n",
    "            f.write(f\"Peak memory usage: {self.peak_memory:.2f} GB\\n\")\n",
    "            f.write(f\"Total samples processed: {self.aggregated_results['total_samples']}\\n\")\n",
    "            f.write(f\"Optimization settings: max_seq_length={self.max_seq_length}, skip_layers={self.skip_layers}, optimize_calcs={self.optimize_calcs}\\n\")\n",
    "\n",
    "            f.write(\"\\nKL REDUCTION SUMMARY (with statistical significance)\\n\")\n",
    "            f.write(\"-----------------------------------------------\\n\\n\")\n",
    "\n",
    "            headers = [\"Layer\"]\n",
    "            for threshold in sink_thresholds:\n",
    "                headers.append(f\"KL Red. (t={threshold})\")\n",
    "                headers.append(\"p-value\")  \n",
    "                headers.append(\"Significant\")  \n",
    "\n",
    "            table_data = []\n",
    "\n",
    "            for layer in analyzed_layers:\n",
    "                row = [layer]\n",
    "\n",
    "                for threshold in sink_thresholds:\n",
    "                    threshold_key = str(threshold)\n",
    "                    layer_stats = self.aggregated_results[\"layer_stats\"].get(str(layer), {})\n",
    "                    threshold_stats = layer_stats.get(threshold_key, {})\n",
    "\n",
    "                    # KL reduction value\n",
    "                    if \"avg_kl_reduction\" in threshold_stats:\n",
    "                        row.append(f\"{threshold_stats['avg_kl_reduction']['mean']:.4f}\")\n",
    "                    else:\n",
    "                        row.append(\"N/A\")\n",
    "\n",
    "                    # P-value\n",
    "                    if \"kl_reduction_significance\" in threshold_stats:\n",
    "                        p_value = threshold_stats[\"kl_reduction_significance\"][\"combined_p_value\"]\n",
    "                        row.append(f\"{p_value:.4f}\")\n",
    "\n",
    "                        # Significance\n",
    "                        row.append(\"Yes\" if p_value < 0.05 else \"No\")\n",
    "                    else:\n",
    "                        row.append(\"N/A\")\n",
    "                        row.append(\"N/A\")\n",
    "\n",
    "                table_data.append(row)\n",
    "\n",
    "            f.write(tabulate.tabulate(table_data, headers=headers, tablefmt=\"plain\"))\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "            f.write(\"STATISTICAL SIGNIFICANCE SUMMARY\\n\")\n",
    "            f.write(\"------------------------------\\n\\n\")\n",
    "            f.write(\"This section shows which findings are statistically significant (p < 0.05).\\n\\n\")\n",
    "\n",
    "            headers = [\"Threshold\", \"KL Reduction Significant\", \"Correlation Significant\"]\n",
    "            table_data = []\n",
    "\n",
    "            for threshold in sink_thresholds:\n",
    "                threshold_key = str(threshold)\n",
    "                row = [threshold]\n",
    "\n",
    "                kl_significant = False\n",
    "                for layer in analyzed_layers:\n",
    "                    layer_stats = self.aggregated_results[\"layer_stats\"].get(str(layer), {})\n",
    "                    threshold_stats = layer_stats.get(threshold_key, {})\n",
    "                    if \"kl_reduction_significance\" in threshold_stats and threshold_stats[\"kl_reduction_significance\"][\"significant\"]:\n",
    "                        kl_significant = True\n",
    "                        break\n",
    "\n",
    "                row.append(\"Yes\" if kl_significant else \"No\")\n",
    "\n",
    "                corr_significant = False\n",
    "                for layer in analyzed_layers:\n",
    "                    layer_stats = self.aggregated_results[\"layer_stats\"].get(str(layer), {})\n",
    "                    threshold_stats = layer_stats.get(threshold_key, {})\n",
    "                    if \"correlation_significance\" in threshold_stats and threshold_stats[\"correlation_significance\"][\"significant\"]:\n",
    "                        corr_significant = True\n",
    "                        break\n",
    "\n",
    "                row.append(\"Yes\" if corr_significant else \"No\")\n",
    "\n",
    "                table_data.append(row)\n",
    "\n",
    "            f.write(tabulate.tabulate(table_data, headers=headers, tablefmt=\"plain\"))\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "            if \"cross_layer_analysis\" in self.aggregated_results:\n",
    "                cross_layer = self.aggregated_results[\"cross_layer_analysis\"]\n",
    "\n",
    "                f.write(\"CROSS-LAYER CORRELATION ANALYSIS\\n\")\n",
    "                f.write(\"-------------------------------\\n\\n\")\n",
    "\n",
    "                if \"layer_position_correlations\" in cross_layer:\n",
    "                    f.write(\"Correlations with Layer Position:\\n\")\n",
    "                    f.write(\"---------------------------------\\n\")\n",
    "\n",
    "                    headers = [\"Metric\", \"Threshold\", \"Pearson r\", \"p-value\", \"Significant\", \"Spearman r\", \"p-value\", \"Significant\"]\n",
    "                    table_data = []\n",
    "\n",
    "                    for key, data in cross_layer[\"layer_position_correlations\"].items():\n",
    "                        row = [\n",
    "                            data.get(\"metric\", \"\"),\n",
    "                            data.get(\"threshold\", \"\"),\n",
    "                            f\"{data.get('pearson_correlation', 0):.4f}\",\n",
    "                            f\"{data.get('pearson_p_value', 0):.4f}\",\n",
    "                            \"Yes\" if data.get(\"pearson_significant\", False) else \"No\",\n",
    "                            f\"{data.get('spearman_correlation', 0):.4f}\",\n",
    "                            f\"{data.get('spearman_p_value', 0):.4f}\",\n",
    "                            \"Yes\" if data.get(\"spearman_significant\", False) else \"No\"\n",
    "                        ]\n",
    "                        table_data.append(row)\n",
    "\n",
    "                    f.write(tabulate.tabulate(table_data, headers=headers, tablefmt=\"plain\"))\n",
    "                    f.write(\"\\n\\n\")\n",
    "\n",
    "                if \"inter_metric_correlations\" in cross_layer:\n",
    "                    f.write(\"Correlations Between Metrics:\\n\")\n",
    "                    f.write(\"----------------------------\\n\")\n",
    "\n",
    "                    headers = [\"Metric 1\", \"Metric 2\", \"Threshold\", \"Pearson r\", \"p-value\", \"Significant\"]\n",
    "                    table_data = []\n",
    "\n",
    "                    for key, data in cross_layer[\"inter_metric_correlations\"].items():\n",
    "                        row = [\n",
    "                            data.get(\"metric1\", \"\"),\n",
    "                            data.get(\"metric2\", \"\"),\n",
    "                            data.get(\"threshold\", \"\"),\n",
    "                            f\"{data.get('pearson_correlation', 0):.4f}\",\n",
    "                            f\"{data.get('pearson_p_value', 0):.4f}\",\n",
    "                            \"Yes\" if data.get(\"pearson_significant\", False) else \"No\"\n",
    "                        ]\n",
    "                        table_data.append(row)\n",
    "\n",
    "                    f.write(tabulate.tabulate(table_data, headers=headers, tablefmt=\"plain\"))\n",
    "                    f.write(\"\\n\\n\")\n",
    "\n",
    "        self.log(f\"Numerical summary saved to {summary_path}\", force=True)\n",
    "        return summary_path\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.model is not None:\n",
    "            self.model = self.model.to(\"cpu\")\n",
    "            del self.model\n",
    "            self.model = None\n",
    "\n",
    "        if self.tokenizer is not None:\n",
    "            del self.tokenizer\n",
    "            self.tokenizer = None\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.log(\"Resources cleaned up\", force=True)\n",
    "\n",
    "\n",
    "def get_sample_texts_from_dataset(dataset_path, n_samples=50):\n",
    "    \"\"\"\n",
    "    Extract sample texts from a dataset for analysis.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the dataset CSV\n",
    "        n_samples: Number of samples to extract\n",
    "\n",
    "    Returns:\n",
    "        List of text samples\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(dataset_path):\n",
    "            print(f\"ERROR: Dataset file not found at path: {dataset_path}\")\n",
    "            return []\n",
    "\n",
    "        # Load dataset\n",
    "        print(f\"Attempting to load dataset from: {dataset_path}\")\n",
    "        data = pd.read_csv(dataset_path)\n",
    "        print(f\"Successfully loaded dataset with {len(data)} rows\")\n",
    "\n",
    "        # Print column names to help debugging\n",
    "        print(f\"Dataset columns: {', '.join(data.columns)}\")\n",
    "\n",
    "        # Check if text column exists\n",
    "        if 'text' not in data.columns:\n",
    "            print(\"ERROR: No 'text' column found in dataset. Available columns:\")\n",
    "            for col in data.columns:\n",
    "                print(f\" - {col}\")\n",
    "            return []\n",
    "\n",
    "        # Sample texts\n",
    "        if len(data) > n_samples:\n",
    "            print(f\"Sampling {n_samples} texts from dataset\")\n",
    "            samples = data.sample(n_samples)\n",
    "        else:\n",
    "            print(f\"Using all {len(data)} texts from dataset\")\n",
    "            samples = data\n",
    "\n",
    "        # Extract texts\n",
    "        texts = samples['text'].tolist()\n",
    "\n",
    "        non_empty_texts = [t for t in texts if isinstance(t, str) and len(t) > 0]\n",
    "        if len(non_empty_texts) < len(texts):\n",
    "            print(f\"WARNING: {len(texts) - len(non_empty_texts)} empty texts found and will be excluded\")\n",
    "            texts = non_empty_texts\n",
    "\n",
    "        if texts:\n",
    "            print(f\"Sample text (first 100 chars): {texts[0][:100]}...\")\n",
    "\n",
    "        return texts\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"ERROR: Dataset file is empty: {dataset_path}\")\n",
    "        return []\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"ERROR: Failed to parse dataset file. Make sure it's a valid CSV: {dataset_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading dataset: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb50c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run ultra-fast attention sink KL divergence analysis.\"\"\"\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        # Mount Google Drive if in Colab\n",
    "        drive.mount('/content/drive')\n",
    "        is_colab = True\n",
    "    except ImportError:\n",
    "        is_colab = False\n",
    "        print(\"Not running in Google Colab, skipping drive mount\")\n",
    "\n",
    "\n",
    "    model_name = \"EleutherAI/pythia-12b\"\n",
    "\n",
    "    if is_colab:\n",
    "        dataset_path = \"/content/drive/MyDrive/wiki_dataset_position.csv\"\n",
    "        output_dir = f\"/content/drive/MyDrive/Sink/pvalue/kl_div/{model_name.split('/')[-1]}\"\n",
    "    else:\n",
    "        dataset_path = \"./wiki_dataset_position.csv\"\n",
    "        output_dir = f\"./attention_sink_kl_{model_name.split('/')[-1]}_fast\"\n",
    "\n",
    "    # Analysis parameters\n",
    "    num_samples = 50           \n",
    "    batch_size = 8            \n",
    "    sink_thresholds = [0.8, 0.9, 0.95]  # Thresholds for attention sink identification\n",
    "\n",
    "    # Speed optimization parameters\n",
    "    max_seq_length = 128        # Truncate sequences to this length for speed\n",
    "    skip_layers = True          # Analyze every other layer for speed\n",
    "    optimize_calcs = True       # Use optimized calculations for massive speedup\n",
    "    verbose = True              # Set to True for detailed progress logs\n",
    "\n",
    "    print(f\"Starting analysis with model: {model_name}\")\n",
    "    print(f\"Dataset path: {dataset_path}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"Analyzing {num_samples} samples with optimization settings:\")\n",
    "    print(f\" - Max sequence length: {max_seq_length}\")\n",
    "    print(f\" - Skip layers: {skip_layers}\")\n",
    "    print(f\" - Optimize calculations: {optimize_calcs}\")\n",
    "\n",
    "    # Check if dataset file exists\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"ERROR: Dataset file not found at path: {dataset_path}\")\n",
    "        print(\"Please check the path and try again.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    analyzer = UltraFastAttentionSinkKLAnalysis(\n",
    "        model_name=model_name,\n",
    "        output_dir=output_dir,\n",
    "        max_seq_length=max_seq_length,\n",
    "        skip_layers=skip_layers,\n",
    "        optimize_calcs=optimize_calcs,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    texts = get_sample_texts_from_dataset(dataset_path, n_samples=num_samples)\n",
    "\n",
    "    if not texts:\n",
    "        print(\"No texts loaded from dataset. Using default examples.\")\n",
    "        texts = [\n",
    "            \"The concept of attention mechanisms allows transformers to focus on relevant parts of the input.\",\n",
    "            \"Attention sinks are positions that accumulate high amounts of attention from many tokens.\",\n",
    "            \"Information theory measures like KL divergence help us understand how models process data.\",\n",
    "            \"Transformer models coordinate information flow between attention heads across different layers.\"\n",
    "        ]\n",
    "        print(f\"Using {len(texts)} default example texts\")\n",
    "    else:\n",
    "        print(f\"Successfully loaded {len(texts)} texts from dataset\")\n",
    "\n",
    "    try:\n",
    "        print(\"Loading model...\")\n",
    "        analyzer.load_model(use_4bit=True) \n",
    "\n",
    "        start_time = time.time()\n",
    "        print(\"Beginning analysis...\")\n",
    "        results = analyzer.run_aggregated_analysis(\n",
    "            texts=texts,\n",
    "            sink_thresholds=sink_thresholds,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Analysis completed successfully in {total_time/60:.2f} minutes!\")\n",
    "\n",
    "        print(f\"Results saved to: {output_dir}\")\n",
    "        print(\"Summary file should be at: \" + os.path.join(output_dir, \"attention_sink_kl_summary.txt\"))\n",
    "        print(\"JSON data file should be at: \" + os.path.join(output_dir, \"aggregated_raw_data.json\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Analysis error: {str(e)}\")\n",
    "        print(\"Full traceback:\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        print(\"Cleaning up resources...\")\n",
    "        analyzer.cleanup()\n",
    "        print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
