{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84155f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from ripser import ripser\n",
    "import persim\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import gc\n",
    "from google.colab import drive\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import h5py\n",
    "import psutil\n",
    "from typing import Dict, List, Tuple, Any, Optional, Union\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"transformer_topology\")\n",
    "\n",
    "# Filter common warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "class AggregateTopologyAnalyzer:\n",
    "    \"\"\"\n",
    "    Class to analyze transformer attention topology and collect aggregate statistics.\n",
    "    Instead of generating per-sample analysis files, this aggregates metrics across samples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, max_tokens=48, device=None, cache_dir=None):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with a model.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name or path of the transformers model to analyze\n",
    "            max_tokens: Maximum number of tokens to process\n",
    "            device: Device to run model on ('cuda' or 'cpu')\n",
    "            cache_dir: Directory to cache model files\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.max_tokens = max_tokens\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.cache_dir = cache_dir\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "        # Force CPU if CUDA disabled\n",
    "        if os.environ.get(\"CUDA_VISIBLE_DEVICES\") == \"\":\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "        # Store memory tracking info\n",
    "        self.memory_tracker = {\n",
    "            'peak': 0,\n",
    "            'current': 0\n",
    "        }\n",
    "\n",
    "        self.sample_count = 0\n",
    "\n",
    "        self.betti_numbers = defaultdict(lambda: {'dim0': [], 'dim1': []})\n",
    "        self.attention_stats = defaultdict(lambda: {'mean': [], 'max': [], 'std': []})\n",
    "        self.persistence_stats = defaultdict(lambda: {'dim0_avg': [], 'dim0_max': [], 'dim1_avg': [], 'dim1_max': []})\n",
    "        self.head_specialization = defaultdict(lambda: defaultdict(list))\n",
    "        self.token_focus = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "        logger.info(f\"Initialized AggregateTopologyAnalyzer for model: {model_name} on {self.device}\")\n",
    "\n",
    "    def _ensure_float32(self, array):\n",
    "        \"\"\"\n",
    "        Ensure array is in float32 format for consistent processing.\n",
    "        \n",
    "        Args:\n",
    "            array: Numpy array that might need conversion\n",
    "        \n",
    "        Returns:\n",
    "            array in float32 format\n",
    "        \"\"\"\n",
    "        if array.dtype != np.float32:\n",
    "            return array.astype(np.float32)\n",
    "        return array\n",
    "\n",
    "    def _update_memory_stats(self):\n",
    "        \"\"\"Update memory usage statistics.\"\"\"\n",
    "        if torch.cuda.is_available() and self.device != \"cpu\":\n",
    "            current = torch.cuda.memory_allocated() / (1024 ** 3)  # GB\n",
    "            peak = torch.cuda.max_memory_allocated() / (1024 ** 3)  # GB\n",
    "            self.memory_tracker['current'] = current\n",
    "            self.memory_tracker['peak'] = max(self.memory_tracker['peak'], peak)\n",
    "        else:\n",
    "            process = psutil.Process(os.getpid())\n",
    "            current = process.memory_info().rss / (1024 ** 3)  # GB\n",
    "            self.memory_tracker['current'] = current\n",
    "            self.memory_tracker['peak'] = max(self.memory_tracker['peak'], current)\n",
    "\n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load the model and tokenizer.\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            logger.info(\"Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                cache_dir=self.cache_dir\n",
    "            )\n",
    "\n",
    "        if self.model is None:\n",
    "            logger.info(f\"Loading model in {self.device} mode...\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                device_map=self.device,\n",
    "                output_attentions=True,\n",
    "                cache_dir=self.cache_dir,\n",
    "                torch_dtype=torch.float32\n",
    "            )\n",
    "        return self.tokenizer, self.model\n",
    "\n",
    "    def process_text(self, text):\n",
    "        \"\"\"\n",
    "        Process a text sample and extract attention maps.\n",
    "\n",
    "        Args:\n",
    "            text: Text to analyze\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing tokens and attention maps\n",
    "        \"\"\"\n",
    "        if self.tokenizer is None or self.model is None:\n",
    "            self.load_model_and_tokenizer()\n",
    "\n",
    "        # Tokenize with limited sequence length\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_tokens,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Move inputs to the correct device\n",
    "        if self.device != \"cpu\":\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        logger.info(f\"Processing {len(tokens)} tokens\")\n",
    "\n",
    "        # Run model inference\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_attentions=True)\n",
    "\n",
    "        # Get attention maps and move to CPU immediately to save GPU memory\n",
    "        attention_maps = [attn.cpu().numpy() for attn in outputs.attentions]\n",
    "\n",
    "        if self.device != \"cpu\":\n",
    "            del inputs, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        self._update_memory_stats()\n",
    "        return {\n",
    "            \"tokens\": tokens,\n",
    "            \"attention_maps\": attention_maps\n",
    "        }\n",
    "\n",
    "    def analyze_sample_and_aggregate(self, text, sample_id=None, layer_subset=None):\n",
    "        \"\"\"\n",
    "        Analyze a sample and add its metrics to aggregated statistics.\n",
    "        Instead of saving individual files, this just collects data for the aggregate report.\n",
    "\n",
    "        Args:\n",
    "            text: Text to analyze\n",
    "            sample_id: Identifier for the sample\n",
    "            layer_subset: Subset of layers to analyze (None = analyze specific layers)\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary with sample metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Process the text and get attention maps\n",
    "            results = self.process_text(text)\n",
    "            tokens = results[\"tokens\"]\n",
    "            attention_maps = results[\"attention_maps\"]\n",
    "\n",
    "            # If layer_subset is None, specify key layers\n",
    "            num_layers = len(attention_maps)\n",
    "            if layer_subset is None:\n",
    "                layer_step = max(1, num_layers // 4)\n",
    "                layer_subset = [0, layer_step, 2*layer_step, num_layers-1]\n",
    "                layer_subset = sorted(list(set([min(l, num_layers-1) for l in layer_subset])))\n",
    "\n",
    "            # Extract metrics for each selected layer in parallel\n",
    "            with ThreadPoolExecutor(max_workers=min(len(layer_subset), os.cpu_count() or 4)) as executor:\n",
    "                futures = []\n",
    "                for layer_idx in layer_subset:\n",
    "                    futures.append(\n",
    "                        executor.submit(\n",
    "                            self._extract_layer_metrics,\n",
    "                            layer_idx=layer_idx,\n",
    "                            tokens=tokens,\n",
    "                            attention_map=attention_maps[layer_idx]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # Collect results\n",
    "                layer_metrics = {}\n",
    "                for future in futures:\n",
    "                    layer_idx, metrics = future.result()\n",
    "                    layer_metrics[layer_idx] = metrics\n",
    "\n",
    "            self._update_aggregate_stats(layer_metrics, tokens, sample_id)\n",
    "\n",
    "            self.sample_count += 1\n",
    "\n",
    "            # Calculate approximate sample metrics\n",
    "            sample_metrics = {\n",
    "                'sample_id': sample_id,\n",
    "                'num_tokens': len(tokens),\n",
    "                'layers_analyzed': list(layer_metrics.keys()),\n",
    "                'attention_means': {\n",
    "                    layer_idx: metrics['attention_mean']\n",
    "                    for layer_idx, metrics in layer_metrics.items()\n",
    "                },\n",
    "                'betti_numbers': {\n",
    "                    layer_idx: metrics['betti_numbers']\n",
    "                    for layer_idx, metrics in layer_metrics.items()\n",
    "                    if 'betti_numbers' in metrics\n",
    "                }\n",
    "            }\n",
    "\n",
    "            del results, tokens, attention_maps, layer_metrics\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            return sample_metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing sample {sample_id}: {str(e)}\", exc_info=True)\n",
    "            return {'error': str(e), 'sample_id': sample_id}\n",
    "\n",
    "    def _extract_layer_metrics(self, layer_idx, tokens, attention_map):\n",
    "        \"\"\"\n",
    "        Extract metrics from a layer's attention map.\n",
    "\n",
    "        Args:\n",
    "            layer_idx: Index of the layer\n",
    "            tokens: List of tokens\n",
    "            attention_map: Attention map for this layer\n",
    "\n",
    "        Returns:\n",
    "            tuple: (layer_idx, metrics_dict)\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        try:\n",
    "            # Get average attention across heads\n",
    "            avg_attention = np.mean(attention_map, axis=1)[0]\n",
    "\n",
    "            # Ensure float32 precision\n",
    "            avg_attention = self._ensure_float32(avg_attention)\n",
    "\n",
    "            attn_mean = np.mean(avg_attention)\n",
    "            attn_max = np.max(avg_attention)\n",
    "            attn_min = np.min(avg_attention)\n",
    "            attn_std = np.std(avg_attention)\n",
    "\n",
    "            metrics.update({\n",
    "                'attention_mean': attn_mean,\n",
    "                'attention_max': attn_max,\n",
    "                'attention_min': attn_min,\n",
    "                'attention_std': attn_std\n",
    "            })\n",
    "\n",
    "            # Most attended tokens (top 5)\n",
    "            token_attention = np.mean(avg_attention, axis=0)\n",
    "            top_indices = np.argsort(-token_attention)[:5]\n",
    "            top_tokens = {}\n",
    "            for idx in top_indices:\n",
    "                if idx < len(tokens):\n",
    "                    top_tokens[tokens[idx]] = float(token_attention[idx])\n",
    "            metrics['top_tokens'] = top_tokens\n",
    "\n",
    "            # Compute distance matrix for persistence homology\n",
    "            distance_matrix = 1 - avg_attention\n",
    "\n",
    "            # Downsample for persistence computation\n",
    "            max_size = min(30, distance_matrix.shape[0])\n",
    "            if distance_matrix.shape[0] > max_size:\n",
    "                indices = np.linspace(0, distance_matrix.shape[0]-1, max_size, dtype=int)\n",
    "                distance_matrix = distance_matrix[indices][:, indices]\n",
    "\n",
    "            # Compute persistence diagram\n",
    "            diagram = ripser(distance_matrix, maxdim=1, distance_matrix=True)\n",
    "\n",
    "            # Extract Betti numbers\n",
    "            betti_numbers = [len(dgm) for dgm in diagram['dgms']]\n",
    "            metrics['betti_numbers'] = betti_numbers\n",
    "\n",
    "            # Extract persistence statistics\n",
    "            persistence_stats = self._extract_persistence_stats(diagram)\n",
    "            metrics.update(persistence_stats)\n",
    "\n",
    "            # Analyze individual attention heads\n",
    "            head_metrics = self._analyze_heads(attention_map, tokens)\n",
    "            metrics['head_metrics'] = head_metrics\n",
    "\n",
    "            # Topology at multiple thresholds\n",
    "            topology_metrics = self._compute_topology_metrics(avg_attention, thresholds=[0.2, 0.4])\n",
    "            metrics['topology'] = topology_metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting metrics for layer {layer_idx}: {str(e)}\")\n",
    "            metrics['error'] = str(e)\n",
    "\n",
    "        return layer_idx, metrics\n",
    "\n",
    "    def _extract_persistence_stats(self, diagram):\n",
    "        \"\"\"Extract statistics from a persistence diagram.\"\"\"\n",
    "        stats = {}\n",
    "\n",
    "        if len(diagram['dgms']) > 0 and len(diagram['dgms'][0]) > 0:\n",
    "            dim0_features = [(birth, death) for birth, death in diagram['dgms'][0] if death != float('inf')]\n",
    "            if dim0_features:\n",
    "                stats['dim0_avg_persistence'] = float(np.mean([death - birth for birth, death in dim0_features]))\n",
    "                stats['dim0_max_persistence'] = float(np.max([death - birth for birth, death in dim0_features]))\n",
    "\n",
    "        if len(diagram['dgms']) > 1 and len(diagram['dgms'][1]) > 0:\n",
    "            dim1_features = [(birth, death) for birth, death in diagram['dgms'][1] if death != float('inf')]\n",
    "            if dim1_features:\n",
    "                stats['dim1_avg_persistence'] = float(np.mean([death - birth for birth, death in dim1_features]))\n",
    "                stats['dim1_max_persistence'] = float(np.max([death - birth for birth, death in dim1_features]))\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def _analyze_heads(self, attention_map, tokens):\n",
    "        \"\"\"Analyze individual attention heads.\"\"\"\n",
    "        num_heads = attention_map.shape[1]\n",
    "        head_metrics = []\n",
    "\n",
    "        for head_idx in range(num_heads):\n",
    "            head_attention = attention_map[0, head_idx]\n",
    "\n",
    "            head_attention = self._ensure_float32(head_attention)\n",
    "\n",
    "            avg_attn = float(np.mean(head_attention))\n",
    "            max_attn = float(np.max(head_attention))\n",
    "            std_attn = float(np.std(head_attention))\n",
    "            entropy = float(-np.sum(head_attention * np.log2(head_attention + 1e-10)))\n",
    "\n",
    "            # Find most attended token\n",
    "            token_attention = np.mean(head_attention, axis=0)\n",
    "            top_idx = np.argmax(token_attention)\n",
    "            top_token = tokens[top_idx] if top_idx < len(tokens) else \"unknown\"\n",
    "            top_strength = float(token_attention[top_idx])\n",
    "\n",
    "            # Find strongest connection\n",
    "            max_i, max_j = np.unravel_index(np.argmax(head_attention), head_attention.shape)\n",
    "            if max_i < len(tokens) and max_j < len(tokens):\n",
    "                strongest_conn = {\n",
    "                    'source': tokens[max_i],\n",
    "                    'target': tokens[max_j],\n",
    "                    'strength': float(head_attention[max_i, max_j])\n",
    "                }\n",
    "            else:\n",
    "                strongest_conn = None\n",
    "\n",
    "            head_metrics.append({\n",
    "                'head_idx': head_idx,\n",
    "                'avg_attention': avg_attn,\n",
    "                'max_attention': max_attn,\n",
    "                'std_attention': std_attn,\n",
    "                'entropy': entropy,\n",
    "                'top_token': top_token,\n",
    "                'top_strength': top_strength,\n",
    "                'strongest_connection': strongest_conn\n",
    "            })\n",
    "\n",
    "        return head_metrics\n",
    "\n",
    "    def _compute_topology_metrics(self, attention_map, thresholds=None):\n",
    "        \"\"\"\n",
    "        Compute topological metrics at different thresholds.\n",
    "        \n",
    "        Args:\n",
    "            attention_map: Attention map to analyze\n",
    "            thresholds: List of thresholds to use (default: [0.2, 0.4])\n",
    "        \"\"\"\n",
    "        if thresholds is None:\n",
    "            thresholds = [0.2, 0.4]\n",
    "            \n",
    "        metrics = {}\n",
    "\n",
    "        attention_map = self._ensure_float32(attention_map)\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            # Create binary adjacency matrix\n",
    "            binary_adj = (attention_map > threshold).astype(int)\n",
    "\n",
    "            # Count connections\n",
    "            total_connections = int(np.sum(binary_adj))\n",
    "            max_possible = attention_map.shape[0] * attention_map.shape[1]\n",
    "            connection_density = float(total_connections / max_possible)\n",
    "\n",
    "            metrics[f'threshold_{threshold}'] = {\n",
    "                'connection_density': connection_density,\n",
    "                'total_connections': total_connections\n",
    "            }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _update_aggregate_stats(self, layer_metrics, tokens, sample_id):\n",
    "        \"\"\"\n",
    "        Update aggregate statistics with metrics from a sample.\n",
    "\n",
    "        Args:\n",
    "            layer_metrics: Dictionary of metrics for each layer\n",
    "            tokens: List of tokens in the sample\n",
    "            sample_id: Identifier for the sample\n",
    "        \"\"\"\n",
    "        for layer_idx, metrics in layer_metrics.items():\n",
    "            # Update Betti numbers\n",
    "            if 'betti_numbers' in metrics:\n",
    "                if len(metrics['betti_numbers']) > 0:\n",
    "                    self.betti_numbers[layer_idx]['dim0'].append(metrics['betti_numbers'][0])\n",
    "                if len(metrics['betti_numbers']) > 1:\n",
    "                    self.betti_numbers[layer_idx]['dim1'].append(metrics['betti_numbers'][1])\n",
    "\n",
    "            # Update attention statistics\n",
    "            self.attention_stats[layer_idx]['mean'].append(metrics['attention_mean'])\n",
    "            self.attention_stats[layer_idx]['max'].append(metrics['attention_max'])\n",
    "            self.attention_stats[layer_idx]['std'].append(metrics['attention_std'])\n",
    "\n",
    "            # Update persistence statistics\n",
    "            if 'dim0_avg_persistence' in metrics:\n",
    "                self.persistence_stats[layer_idx]['dim0_avg'].append(metrics['dim0_avg_persistence'])\n",
    "            if 'dim0_max_persistence' in metrics:\n",
    "                self.persistence_stats[layer_idx]['dim0_max'].append(metrics['dim0_max_persistence'])\n",
    "            if 'dim1_avg_persistence' in metrics:\n",
    "                self.persistence_stats[layer_idx]['dim1_avg'].append(metrics['dim1_avg_persistence'])\n",
    "            if 'dim1_max_persistence' in metrics:\n",
    "                self.persistence_stats[layer_idx]['dim1_max'].append(metrics['dim1_max_persistence'])\n",
    "\n",
    "            # Update head specialization data\n",
    "            if 'head_metrics' in metrics:\n",
    "                for head_data in metrics['head_metrics']:\n",
    "                    head_idx = head_data['head_idx']\n",
    "                    top_token = head_data['top_token']\n",
    "                    self.head_specialization[layer_idx][head_idx].append(top_token)\n",
    "\n",
    "                    # Track token focus frequency\n",
    "                    if top_token not in self.token_focus[layer_idx][head_idx]:\n",
    "                        self.token_focus[layer_idx][head_idx][top_token] = 1\n",
    "                    else:\n",
    "                        self.token_focus[layer_idx][head_idx][top_token] += 1\n",
    "\n",
    "    def generate_aggregate_report(self, output_path):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive report with aggregate statistics and statistical significance tests.\n",
    "        \"\"\"\n",
    "        output_dir = Path(output_path)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        vis_dir = output_dir / \"visualizations\"\n",
    "        vis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        test_results = self.run_statistical_tests()\n",
    "\n",
    "        self._generate_visualizations(vis_dir)\n",
    "\n",
    "        report_path = output_dir / \"aggregate_topology_report.md\"\n",
    "\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"# Transformer Attention Topology Analysis\\n\\n\")\n",
    "\n",
    "            f.write(f\"## Analysis Overview\\n\\n\")\n",
    "            f.write(f\"- **Model**: {self.model_name}\\n\")\n",
    "            f.write(f\"- **Samples Analyzed**: {self.sample_count}\\n\")\n",
    "            f.write(f\"- **Max Tokens Per Sample**: {self.max_tokens}\\n\")\n",
    "            f.write(f\"- **Layers Analyzed**: {sorted(self.attention_stats.keys())}\\n\")\n",
    "            f.write(f\"- **Analysis Date**: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "            f.write(f\"## Summary of Findings\\n\\n\")\n",
    "\n",
    "            # Write key findings section with statistical significance\n",
    "            key_findings = self._identify_key_findings()\n",
    "            updated_findings = self.update_findings_with_statistics(key_findings, test_results)\n",
    "\n",
    "            f.write(\"### Key Findings\\n\\n\")\n",
    "            for i, finding in enumerate(updated_findings, 1):\n",
    "                f.write(f\"{i}. {finding}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            # Write Betti numbers section\n",
    "            f.write(\"## Topological Features (Betti Numbers)\\n\\n\")\n",
    "            f.write(\"Betti numbers measure topological features: Betti₀ counts connected components, \")\n",
    "            f.write(\"while Betti₁ counts loops/cycles in the attention structure.\\n\\n\")\n",
    "\n",
    "            f.write(\"| Layer | Betti₀ (Mean ± Std) | Betti₁ (Mean ± Std) | Sample Count |\\n\")\n",
    "            f.write(\"|-------|-------------------|-------------------|-------------|\\n\")\n",
    "\n",
    "            for layer_idx in sorted(self.betti_numbers.keys()):\n",
    "                dim0_data = self.betti_numbers[layer_idx]['dim0']\n",
    "                dim1_data = self.betti_numbers[layer_idx]['dim1']\n",
    "\n",
    "                if dim0_data:\n",
    "                    dim0_mean = np.mean(dim0_data)\n",
    "                    dim0_std = np.std(dim0_data)\n",
    "                    dim0_str = f\"{dim0_mean:.2f} ± {dim0_std:.2f}\"\n",
    "                else:\n",
    "                    dim0_str = \"N/A\"\n",
    "\n",
    "                if dim1_data:\n",
    "                    dim1_mean = np.mean(dim1_data)\n",
    "                    dim1_std = np.std(dim1_data)\n",
    "                    dim1_str = f\"{dim1_mean:.2f} ± {dim1_std:.2f}\"\n",
    "                else:\n",
    "                    dim1_str = \"N/A\"\n",
    "\n",
    "                sample_count = len(dim0_data) or len(dim1_data)\n",
    "                f.write(f\"| {layer_idx} | {dim0_str} | {dim1_str} | {sample_count} |\\n\")\n",
    "\n",
    "            f.write(\"\\n![Betti Numbers Across Layers](visualizations/betti_numbers.png)\\n\\n\")\n",
    "\n",
    "            f.write(\"## Attention Distribution Analysis\\n\\n\")\n",
    "            f.write(\"This section examines how attention is distributed across tokens in different layers.\\n\\n\")\n",
    "\n",
    "            f.write(\"| Layer | Mean Attention (± Std) | Max Attention (± Std) | Attention StdDev (± Std) |\\n\")\n",
    "            f.write(\"|-------|----------------------|---------------------|------------------------|\\n\")\n",
    "\n",
    "            for layer_idx in sorted(self.attention_stats.keys()):\n",
    "                mean_data = self.attention_stats[layer_idx]['mean']\n",
    "                max_data = self.attention_stats[layer_idx]['max']\n",
    "                std_data = self.attention_stats[layer_idx]['std']\n",
    "\n",
    "                if mean_data:\n",
    "                    mean_mean = np.mean(mean_data)\n",
    "                    mean_std = np.std(mean_data)\n",
    "                    mean_str = f\"{mean_mean:.4f} ± {mean_std:.4f}\"\n",
    "                else:\n",
    "                    mean_str = \"N/A\"\n",
    "\n",
    "                if max_data:\n",
    "                    max_mean = np.mean(max_data)\n",
    "                    max_std = np.std(max_data)\n",
    "                    max_str = f\"{max_mean:.4f} ± {max_std:.4f}\"\n",
    "                else:\n",
    "                    max_str = \"N/A\"\n",
    "\n",
    "                if std_data:\n",
    "                    std_mean = np.mean(std_data)\n",
    "                    std_std = np.std(std_data)\n",
    "                    std_str = f\"{std_mean:.4f} ± {std_std:.4f}\"\n",
    "                else:\n",
    "                    std_str = \"N/A\"\n",
    "\n",
    "                f.write(f\"| {layer_idx} | {mean_str} | {max_str} | {std_str} |\\n\")\n",
    "\n",
    "            f.write(\"\\n![Attention Statistics Across Layers](visualizations/attention_stats.png)\\n\\n\")\n",
    "\n",
    "            f.write(\"## Topological Persistence Analysis\\n\\n\")\n",
    "            f.write(\"Persistence measures the 'significance' of topological features. \")\n",
    "            f.write(\"Higher persistence values indicate more prominent features.\\n\\n\")\n",
    "\n",
    "            f.write(\"| Layer | Dim0 Avg Persistence | Dim1 Avg Persistence |\\n\")\n",
    "            f.write(\"|-------|---------------------|---------------------|\\n\")\n",
    "\n",
    "            for layer_idx in sorted(self.persistence_stats.keys()):\n",
    "                stats = self.persistence_stats[layer_idx]\n",
    "\n",
    "                if stats['dim0_avg']:\n",
    "                    dim0_avg = np.mean(stats['dim0_avg'])\n",
    "                    dim0_str = f\"{dim0_avg:.4f}\"\n",
    "                else:\n",
    "                    dim0_str = \"N/A\"\n",
    "\n",
    "                if stats['dim1_avg']:\n",
    "                    dim1_avg = np.mean(stats['dim1_avg'])\n",
    "                    dim1_str = f\"{dim1_avg:.4f}\"\n",
    "                else:\n",
    "                    dim1_str = \"N/A\"\n",
    "\n",
    "                f.write(f\"| {layer_idx} | {dim0_str} | {dim1_str} |\\n\")\n",
    "\n",
    "            f.write(\"\\n![Persistence Statistics Across Layers](visualizations/persistence_stats.png)\\n\\n\")\n",
    "\n",
    "            f.write(\"## Statistical Significance\\n\\n\")\n",
    "            f.write(\"A detailed analysis of statistical significance tests is available in the \")\n",
    "            f.write(\"[Statistical Significance Report](statistical_significance.md).\\n\\n\")\n",
    "            f.write(\"This report shows which differences between layers are statistically significant \")\n",
    "            f.write(\"(p < 0.05) and which might be due to random variation.\\n\")\n",
    "\n",
    "\n",
    "            f.write(\"## Attention Head Specialization\\n\\n\")\n",
    "            f.write(\"This section identifies attention heads that consistently focus on specific token types across samples.\\n\\n\")\n",
    "\n",
    "            specialized_heads = self._identify_specialized_heads()\n",
    "\n",
    "            if specialized_heads:\n",
    "                f.write(\"### Top Specialized Heads\\n\\n\")\n",
    "                f.write(\"| Layer | Head | Top Token | Consistency (%) | Samples |\\n\")\n",
    "                f.write(\"|-------|------|-----------|-----------------|--------|\\n\")\n",
    "\n",
    "                for head_data in specialized_heads[:15]:  # Show top 15\n",
    "                    layer_idx = head_data['layer']\n",
    "                    head_idx = head_data['head']\n",
    "                    token = head_data['token']\n",
    "                    consistency = head_data['consistency'] * 100\n",
    "                    sample_count = head_data['samples']\n",
    "\n",
    "                    f.write(f\"| {layer_idx} | {head_idx} | {token} | {consistency:.1f}% | {sample_count} |\\n\")\n",
    "\n",
    "                f.write(\"\\n![Head Specialization Heatmap](visualizations/head_specialization.png)\\n\\n\")\n",
    "            else:\n",
    "                f.write(\"No strongly specialized heads were identified in the analysis.\\n\\n\")\n",
    "\n",
    "            layers = sorted(self.attention_stats.keys())\n",
    "            if len(layers) >= 3:\n",
    "                f.write(\"## Early vs. Late Layer Comparison\\n\\n\")\n",
    "                f.write(\"This section compares the characteristics of early and late transformer layers.\\n\\n\")\n",
    "\n",
    "                early_layer = layers[0]\n",
    "                late_layer = layers[-1]\n",
    "\n",
    "                # Compare key metrics\n",
    "                metrics_table = [\n",
    "                    [\"Metric\", f\"Layer {early_layer}\", f\"Layer {late_layer}\", \"Change\"],\n",
    "                ]\n",
    "\n",
    "                # Betti0\n",
    "                early_betti0 = np.mean(self.betti_numbers[early_layer]['dim0']) if self.betti_numbers[early_layer]['dim0'] else 0\n",
    "                late_betti0 = np.mean(self.betti_numbers[late_layer]['dim0']) if self.betti_numbers[late_layer]['dim0'] else 0\n",
    "                betti0_change = late_betti0 - early_betti0\n",
    "                metrics_table.append([\n",
    "                    \"Betti₀ (components)\",\n",
    "                    f\"{early_betti0:.2f}\",\n",
    "                    f\"{late_betti0:.2f}\",\n",
    "                    f\"{betti0_change:+.2f}\"\n",
    "                ])\n",
    "\n",
    "                # Betti1\n",
    "                early_betti1 = np.mean(self.betti_numbers[early_layer]['dim1']) if self.betti_numbers[early_layer]['dim1'] else 0\n",
    "                late_betti1 = np.mean(self.betti_numbers[late_layer]['dim1']) if self.betti_numbers[late_layer]['dim1'] else 0\n",
    "                betti1_change = late_betti1 - early_betti1\n",
    "                metrics_table.append([\n",
    "                    \"Betti₁ (loops)\",\n",
    "                    f\"{early_betti1:.2f}\",\n",
    "                    f\"{late_betti1:.2f}\",\n",
    "                    f\"{betti1_change:+.2f}\"\n",
    "                ])\n",
    "\n",
    "                # Mean attention\n",
    "                early_attn = np.mean(self.attention_stats[early_layer]['mean']) if self.attention_stats[early_layer]['mean'] else 0\n",
    "                late_attn = np.mean(self.attention_stats[late_layer]['mean']) if self.attention_stats[late_layer]['mean'] else 0\n",
    "                attn_change = late_attn - early_attn\n",
    "                metrics_table.append([\n",
    "                    \"Mean Attention\",\n",
    "                    f\"{early_attn:.4f}\",\n",
    "                    f\"{late_attn:.4f}\",\n",
    "                    f\"{attn_change:+.4f}\"\n",
    "                ])\n",
    "\n",
    "                # Persistence\n",
    "                early_pers = np.mean(self.persistence_stats[early_layer]['dim0_avg']) if self.persistence_stats[early_layer]['dim0_avg'] else 0\n",
    "                late_pers = np.mean(self.persistence_stats[late_layer]['dim0_avg']) if self.persistence_stats[late_layer]['dim0_avg'] else 0\n",
    "                pers_change = late_pers - early_pers\n",
    "                metrics_table.append([\n",
    "                    \"Dim0 Persistence\",\n",
    "                    f\"{early_pers:.4f}\",\n",
    "                    f\"{late_pers:.4f}\",\n",
    "                    f\"{pers_change:+.4f}\"\n",
    "                ])\n",
    "\n",
    "                # Render table\n",
    "                f.write(\"### Key Metrics Comparison\\n\\n\")\n",
    "                f.write(\"| \" + \" | \".join(metrics_table[0]) + \" |\\n\")\n",
    "                f.write(\"|\" + \"|\".join([\"---\"] * len(metrics_table[0])) + \"|\\n\")\n",
    "                for row in metrics_table[1:]:\n",
    "                    f.write(\"| \" + \" | \".join(row) + \" |\\n\")\n",
    "\n",
    "                f.write(\"\\n### Interpretations\\n\\n\")\n",
    "                f.write(\"Based on these comparisons:\\n\\n\")\n",
    "\n",
    "                if betti0_change > 0:\n",
    "                    f.write(\"- **Component Structure**: Later layers have more disconnected components, suggesting \")\n",
    "                    f.write(\"more fragmentation or specialization in attention patterns.\\n\")\n",
    "                elif betti0_change < 0:\n",
    "                    f.write(\"- **Component Structure**: Later layers have fewer disconnected components, suggesting \")\n",
    "                    f.write(\"more integration or consolidation of attention patterns.\\n\")\n",
    "\n",
    "                if betti1_change > 0:\n",
    "                    f.write(\"- **Loop Structure**: Later layers have more loops, suggesting more complex \")\n",
    "                    f.write(\"cyclic relationships between tokens.\\n\")\n",
    "                elif betti1_change < 0:\n",
    "                    f.write(\"- **Loop Structure**: Later layers have fewer loops, suggesting simpler \")\n",
    "                    f.write(\"token relationships with less cyclic attention patterns.\\n\")\n",
    "\n",
    "                if attn_change > 0.01:\n",
    "                    f.write(\"- **Attention Intensity**: Later layers show higher average attention values, \")\n",
    "                    f.write(\"indicating stronger focus on specific tokens.\\n\")\n",
    "                elif attn_change < -0.01:\n",
    "                    f.write(\"- **Attention Intensity**: Later layers show lower average attention values, \")\n",
    "                    f.write(\"indicating more distributed focus across tokens.\\n\")\n",
    "\n",
    "            f.write(\"### Technical Details\\n\\n\")\n",
    "            f.write(f\"- **Peak Memory Usage**: {self.memory_tracker['peak']:.2f} GB\\n\")\n",
    "            f.write(f\"- **Device Used**: {self.device}\\n\")\n",
    "    \n",
    "        self.generate_statistical_report(test_results, output_dir)\n",
    "\n",
    "        stats_path = output_dir / \"aggregate_statistics.json\"\n",
    "        self._save_aggregate_stats(stats_path)\n",
    "\n",
    "        logger.info(f\"Aggregate report saved to {report_path}\")\n",
    "        return str(report_path)\n",
    "\n",
    "\n",
    "    def _generate_visualizations(self, vis_dir):\n",
    "        \"\"\"Generate visualizations of aggregate statistics.\"\"\"\n",
    "\n",
    "        self._create_betti_number_visualization(vis_dir / \"betti_numbers.png\")\n",
    "\n",
    "        self._create_attention_stats_visualization(vis_dir / \"attention_stats.png\")\n",
    "\n",
    "        self._create_persistence_stats_visualization(vis_dir / \"persistence_stats.png\")\n",
    "\n",
    "        self._create_head_specialization_visualization(vis_dir / \"head_specialization.png\")\n",
    "\n",
    "        layers = sorted(self.attention_stats.keys())\n",
    "        if len(layers) >= 3:\n",
    "            self._create_layer_comparison_visualization(vis_dir / \"layer_comparison.png\",\n",
    "                                                       early_layer=layers[0],\n",
    "                                                       late_layer=layers[-1])\n",
    "\n",
    "    def _create_betti_number_visualization(self, output_path):\n",
    "        \"\"\"Create a visualization of Betti numbers across layers.\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        layers = sorted(self.betti_numbers.keys())\n",
    "        dim0_means = []\n",
    "        dim0_stds = []\n",
    "        dim1_means = []\n",
    "        dim1_stds = []\n",
    "\n",
    "        for layer in layers:\n",
    "            dim0_data = self.betti_numbers[layer]['dim0']\n",
    "            dim1_data = self.betti_numbers[layer]['dim1']\n",
    "\n",
    "            dim0_means.append(np.mean(dim0_data) if dim0_data else 0)\n",
    "            dim0_stds.append(np.std(dim0_data) if len(dim0_data) > 1 else 0)\n",
    "            dim1_means.append(np.mean(dim1_data) if dim1_data else 0)\n",
    "            dim1_stds.append(np.std(dim1_data) if len(dim1_data) > 1 else 0)\n",
    "\n",
    "        plt.errorbar(layers, dim0_means, yerr=dim0_stds, fmt='o-', label='Betti₀ (Components)', capsize=4)\n",
    "        plt.errorbar(layers, dim1_means, yerr=dim1_stds, fmt='s-', label='Betti₁ (Loops)', capsize=4)\n",
    "\n",
    "        plt.title('Betti Numbers Across Layers (All Samples)')\n",
    "        plt.xlabel('Layer')\n",
    "        plt.ylabel('Betti Number (Mean ± Std)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.xticks(layers)\n",
    "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=120)\n",
    "        plt.close()\n",
    "\n",
    "    def _create_attention_stats_visualization(self, output_path):\n",
    "        \"\"\"Create a visualization of attention statistics across layers.\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        layers = sorted(self.attention_stats.keys())\n",
    "        means = []\n",
    "        maxes = []\n",
    "        stds = []\n",
    "\n",
    "        for layer in layers:\n",
    "            mean_data = self.attention_stats[layer]['mean']\n",
    "            max_data = self.attention_stats[layer]['max']\n",
    "            std_data = self.attention_stats[layer]['std']\n",
    "\n",
    "            means.append(np.mean(mean_data) if mean_data else 0)\n",
    "            maxes.append(np.mean(max_data) if max_data else 0)\n",
    "            stds.append(np.mean(std_data) if std_data else 0)\n",
    "\n",
    "        plt.plot(layers, means, 'o-', label='Mean Attention')\n",
    "        plt.plot(layers, maxes, 's-', label='Max Attention')\n",
    "        plt.plot(layers, stds, '^-', label='Std Deviation')\n",
    "\n",
    "        plt.title('Attention Statistics Across Layers (All Samples)')\n",
    "        plt.xlabel('Layer')\n",
    "        plt.ylabel('Attention Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.xticks(layers)\n",
    "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=120)\n",
    "        plt.close()\n",
    "\n",
    "    def _create_persistence_stats_visualization(self, output_path):\n",
    "        \"\"\"Create a visualization of persistence statistics across layers.\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        layers = sorted(self.persistence_stats.keys())\n",
    "        dim0_avgs = []\n",
    "        dim1_avgs = []\n",
    "\n",
    "        for layer in layers:\n",
    "            stats = self.persistence_stats[layer]\n",
    "\n",
    "            dim0_avgs.append(np.mean(stats['dim0_avg']) if stats['dim0_avg'] else 0)\n",
    "            dim1_avgs.append(np.mean(stats['dim1_avg']) if stats['dim1_avg'] else 0)\n",
    "\n",
    "        plt.plot(layers, dim0_avgs, 'o-', label='Dim 0 Avg Persistence')\n",
    "        plt.plot(layers, dim1_avgs, 's-', label='Dim 1 Avg Persistence')\n",
    "\n",
    "        plt.title('Average Persistence Across Layers (All Samples)')\n",
    "        plt.xlabel('Layer')\n",
    "        plt.ylabel('Average Persistence')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.xticks(layers)\n",
    "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=120)\n",
    "        plt.close()\n",
    "\n",
    "    def _create_head_specialization_visualization(self, output_path):\n",
    "        \"\"\"Create a heatmap of head specialization consistency.\"\"\"\n",
    "        # Get all layers and maximum number of heads\n",
    "        layers = sorted(self.token_focus.keys())\n",
    "        max_heads = 0\n",
    "\n",
    "        for layer in layers:\n",
    "            if self.token_focus[layer]:\n",
    "                max_heads = max(max_heads, max(self.token_focus[layer].keys()) + 1)\n",
    "\n",
    "        if max_heads == 0:\n",
    "            logger.warning(\"No head specialization data available for visualization\")\n",
    "            return\n",
    "\n",
    "        # Create consistency matrix\n",
    "        consistency_matrix = np.zeros((len(layers), max_heads))\n",
    "        consistency_matrix.fill(np.nan)  # Fill with NaN for missing data\n",
    "\n",
    "        # Fill matrix with consistency values\n",
    "        for i, layer_idx in enumerate(layers):\n",
    "            for head_idx in range(max_heads):\n",
    "                if head_idx in self.token_focus[layer_idx]:\n",
    "                    token_counts = self.token_focus[layer_idx][head_idx]\n",
    "                    if token_counts:\n",
    "                        max_count = max(token_counts.values())\n",
    "                        total = sum(token_counts.values())\n",
    "                        consistency = max_count / total if total > 0 else 0\n",
    "                        consistency_matrix[i, head_idx] = consistency\n",
    "\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        ax = sns.heatmap(consistency_matrix, cmap='viridis', vmin=0, vmax=1,\n",
    "                        xticklabels=range(max_heads), yticklabels=layers,\n",
    "                        cbar_kws={'label': 'Token Focus Consistency'})\n",
    "\n",
    "        plt.title('Head Specialization Consistency Across Samples')\n",
    "        plt.xlabel('Head Index')\n",
    "        plt.ylabel('Layer')\n",
    "\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.yticks(rotation=0)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=120)\n",
    "        plt.close()\n",
    "\n",
    "    def _create_layer_comparison_visualization(self, output_path, early_layer, late_layer):\n",
    "        \"\"\"Create a comparative visualization of early vs late layers.\"\"\"\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "        axes[0, 0].set_title('Betti Numbers Comparison')\n",
    "\n",
    "        early_betti0 = np.mean(self.betti_numbers[early_layer]['dim0']) if self.betti_numbers[early_layer]['dim0'] else 0\n",
    "        early_betti1 = np.mean(self.betti_numbers[early_layer]['dim1']) if self.betti_numbers[early_layer]['dim1'] else 0\n",
    "        late_betti0 = np.mean(self.betti_numbers[late_layer]['dim0']) if self.betti_numbers[late_layer]['dim0'] else 0\n",
    "        late_betti1 = np.mean(self.betti_numbers[late_layer]['dim1']) if self.betti_numbers[late_layer]['dim1'] else 0\n",
    "\n",
    "        betti_values = [early_betti0, early_betti1, late_betti0, late_betti1]\n",
    "        betti_labels = [f'Layer {early_layer}\\nBetti₀', f'Layer {early_layer}\\nBetti₁',\n",
    "                     f'Layer {late_layer}\\nBetti₀', f'Layer {late_layer}\\nBetti₁']\n",
    "\n",
    "        bars = axes[0, 0].bar(betti_labels, betti_values, color=['blue', 'green', 'blue', 'green'], alpha=0.7)\n",
    "        bars[2].set_color('orange')\n",
    "        bars[3].set_color('red')\n",
    "\n",
    "        axes[0, 0].set_ylabel('Average Value')\n",
    "        axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        axes[0, 1].set_title('Attention Statistics Comparison')\n",
    "\n",
    "        early_mean = np.mean(self.attention_stats[early_layer]['mean']) if self.attention_stats[early_layer]['mean'] else 0\n",
    "        early_max = np.mean(self.attention_stats[early_layer]['max']) if self.attention_stats[early_layer]['max'] else 0\n",
    "        early_std = np.mean(self.attention_stats[early_layer]['std']) if self.attention_stats[early_layer]['std'] else 0\n",
    "\n",
    "        late_mean = np.mean(self.attention_stats[late_layer]['mean']) if self.attention_stats[late_layer]['mean'] else 0\n",
    "        late_max = np.mean(self.attention_stats[late_layer]['max']) if self.attention_stats[late_layer]['max'] else 0\n",
    "        late_std = np.mean(self.attention_stats[late_layer]['std']) if self.attention_stats[late_layer]['std'] else 0\n",
    "\n",
    "        attn_values = [early_mean, early_max, early_std, late_mean, late_max, late_std]\n",
    "        attn_labels = [f'Layer {early_layer}\\nMean', f'Layer {early_layer}\\nMax', f'Layer {early_layer}\\nStd',\n",
    "                     f'Layer {late_layer}\\nMean', f'Layer {late_layer}\\nMax', f'Layer {late_layer}\\nStd']\n",
    "\n",
    "        bars = axes[0, 1].bar(attn_labels, attn_values, color=['blue', 'blue', 'blue', 'orange', 'orange', 'orange'], alpha=0.7)\n",
    "\n",
    "        axes[0, 1].set_ylabel('Attention Value')\n",
    "        axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        axes[1, 0].set_title('Persistence Values Comparison')\n",
    "\n",
    "        early_dim0 = np.mean(self.persistence_stats[early_layer]['dim0_avg']) if self.persistence_stats[early_layer]['dim0_avg'] else 0\n",
    "        early_dim1 = np.mean(self.persistence_stats[early_layer]['dim1_avg']) if self.persistence_stats[early_layer]['dim1_avg'] else 0\n",
    "        late_dim0 = np.mean(self.persistence_stats[late_layer]['dim0_avg']) if self.persistence_stats[late_layer]['dim0_avg'] else 0\n",
    "        late_dim1 = np.mean(self.persistence_stats[late_layer]['dim1_avg']) if self.persistence_stats[late_layer]['dim1_avg'] else 0\n",
    "\n",
    "        pers_values = [early_dim0, early_dim1, late_dim0, late_dim1]\n",
    "        pers_labels = [f'Layer {early_layer}\\nDim0', f'Layer {early_layer}\\nDim1',\n",
    "                      f'Layer {late_layer}\\nDim0', f'Layer {late_layer}\\nDim1']\n",
    "\n",
    "        bars = axes[1, 0].bar(pers_labels, pers_values, color=['blue', 'green', 'orange', 'red'], alpha=0.7)\n",
    "\n",
    "        axes[1, 0].set_ylabel('Average Persistence')\n",
    "        axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        axes[1, 1].set_title('Head Specialization Comparison')\n",
    "\n",
    "        early_heads = 0\n",
    "        late_heads = 0\n",
    "\n",
    "        for head_idx in self.token_focus.get(early_layer, {}):\n",
    "            token_counts = self.token_focus[early_layer][head_idx]\n",
    "            if token_counts:\n",
    "                max_count = max(token_counts.values())\n",
    "                total = sum(token_counts.values())\n",
    "                consistency = max_count / total if total > 0 else 0\n",
    "                if consistency >= 0.5:\n",
    "                    early_heads += 1\n",
    "\n",
    "        for head_idx in self.token_focus.get(late_layer, {}):\n",
    "            token_counts = self.token_focus[late_layer][head_idx]\n",
    "            if token_counts:\n",
    "                max_count = max(token_counts.values())\n",
    "                total = sum(token_counts.values())\n",
    "                consistency = max_count / total if total > 0 else 0\n",
    "                if consistency >= 0.5:\n",
    "                    late_heads += 1\n",
    "\n",
    "        head_values = [early_heads, late_heads]\n",
    "        head_labels = [f'Layer {early_layer}', f'Layer {late_layer}']\n",
    "\n",
    "        bars = axes[1, 1].bar(head_labels, head_values, color=['blue', 'orange'], alpha=0.7)\n",
    "\n",
    "        axes[1, 1].set_ylabel('Specialized Heads Count')\n",
    "        axes[1, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=120)\n",
    "        plt.close()\n",
    "\n",
    "    def _identify_specialized_heads(self):\n",
    "        \"\"\"Identify attention heads with consistent focus across samples.\"\"\"\n",
    "        specialized_heads = []\n",
    "\n",
    "        for layer_idx in self.token_focus:\n",
    "            for head_idx in self.token_focus[layer_idx]:\n",
    "                token_counts = self.token_focus[layer_idx][head_idx]\n",
    "                if token_counts:\n",
    "                    max_token = max(token_counts.items(), key=lambda x: x[1])\n",
    "                    token = max_token[0]\n",
    "                    count = max_token[1]\n",
    "                    total = sum(token_counts.values())\n",
    "                    consistency = count / total if total > 0 else 0\n",
    "\n",
    "                    if consistency >= 0.3 and total >= 5:  # At least 30% consistency and 5 samples\n",
    "                        specialized_heads.append({\n",
    "                            'layer': layer_idx,\n",
    "                            'head': head_idx,\n",
    "                            'token': token,\n",
    "                            'consistency': consistency,\n",
    "                            'samples': total\n",
    "                        })\n",
    "\n",
    "        # Sort by consistency\n",
    "        specialized_heads.sort(key=lambda x: x['consistency'], reverse=True)\n",
    "\n",
    "        return specialized_heads\n",
    "\n",
    "    def _identify_key_findings(self):\n",
    "        \"\"\"Identify key findings from the analysis.\"\"\"\n",
    "        findings = []\n",
    "\n",
    "        if not self.betti_numbers or not self.attention_stats:\n",
    "            findings.append(\"Insufficient data for comprehensive analysis.\")\n",
    "            return findings\n",
    "\n",
    "        # Betti number trends\n",
    "        layers = sorted(self.betti_numbers.keys())\n",
    "        if len(layers) >= 2:\n",
    "            early_layer = layers[0]\n",
    "            late_layer = layers[-1]\n",
    "\n",
    "            early_betti0 = np.mean(self.betti_numbers[early_layer]['dim0']) if self.betti_numbers[early_layer]['dim0'] else 0\n",
    "            late_betti0 = np.mean(self.betti_numbers[late_layer]['dim0']) if self.betti_numbers[late_layer]['dim0'] else 0\n",
    "\n",
    "            if abs(late_betti0 - early_betti0) > 1.0:\n",
    "                if late_betti0 > early_betti0:\n",
    "                    findings.append(f\"**Component fragmentation**: Connected components increase from {early_betti0:.1f} in layer {early_layer} to {late_betti0:.1f} in layer {late_layer}, suggesting attention becomes more fragmented in deeper layers.\")\n",
    "                else:\n",
    "                    findings.append(f\"**Component integration**: Connected components decrease from {early_betti0:.1f} in layer {early_layer} to {late_betti0:.1f} in layer {late_layer}, suggesting attention becomes more integrated in deeper layers.\")\n",
    "\n",
    "            early_betti1 = np.mean(self.betti_numbers[early_layer]['dim1']) if self.betti_numbers[early_layer]['dim1'] else 0\n",
    "            late_betti1 = np.mean(self.betti_numbers[late_layer]['dim1']) if self.betti_numbers[late_layer]['dim1'] else 0\n",
    "\n",
    "            if abs(late_betti1 - early_betti1) > 0.5:\n",
    "                if late_betti1 > early_betti1:\n",
    "                    findings.append(f\"**Cyclic complexity**: Loops increase from {early_betti1:.1f} in layer {early_layer} to {late_betti1:.1f} in layer {late_layer}, suggesting attention forms more complex cyclic patterns in deeper layers.\")\n",
    "                else:\n",
    "                    findings.append(f\"**Cyclic simplification**: Loops decrease from {early_betti1:.1f} in layer {early_layer} to {late_betti1:.1f} in layer {late_layer}, suggesting attention forms simpler patterns in deeper layers.\")\n",
    "\n",
    "        # attention patterns\n",
    "        layers = sorted(self.attention_stats.keys())\n",
    "        if len(layers) >= 2:\n",
    "            early_layer = layers[0]\n",
    "            late_layer = layers[-1]\n",
    "\n",
    "            early_mean = np.mean(self.attention_stats[early_layer]['mean']) if self.attention_stats[early_layer]['mean'] else 0\n",
    "            late_mean = np.mean(self.attention_stats[late_layer]['mean']) if self.attention_stats[late_layer]['mean'] else 0\n",
    "\n",
    "            if abs(late_mean - early_mean) > 0.01:\n",
    "                if late_mean > early_mean:\n",
    "                    findings.append(f\"**Attention intensity**: Average attention increases from {early_mean:.4f} in layer {early_layer} to {late_mean:.4f} in layer {late_layer}, suggesting stronger focus in deeper layers.\")\n",
    "                else:\n",
    "                    findings.append(f\"**Attention diffusion**: Average attention decreases from {early_mean:.4f} in layer {early_layer} to {late_mean:.4f} in layer {late_layer}, suggesting more distributed focus in deeper layers.\")\n",
    "\n",
    "        # Look for head specialization\n",
    "        specialized_heads = self._identify_specialized_heads()\n",
    "        if specialized_heads:\n",
    "            top_specialized = specialized_heads[0]\n",
    "            findings.append(f\"**Head specialization**: Found {len(specialized_heads)} heads with consistent token focus. The most specialized is layer {top_specialized['layer']}, head {top_specialized['head']} focusing on token '{top_specialized['token']}' with {top_specialized['consistency']*100:.1f}% consistency.\")\n",
    "\n",
    "            # Look for layer patterns in specialization\n",
    "            layer_counts = {}\n",
    "            for head in specialized_heads:\n",
    "                layer = head['layer']\n",
    "                if layer not in layer_counts:\n",
    "                    layer_counts[layer] = 0\n",
    "                layer_counts[layer] += 1\n",
    "\n",
    "            if len(layer_counts) >= 2:\n",
    "                most_specialized_layer = max(layer_counts.items(), key=lambda x: x[1])\n",
    "                findings.append(f\"**Layer specialization**: Layer {most_specialized_layer[0]} has the most specialized attention heads ({most_specialized_layer[1]}), suggesting specialized processing at this depth.\")\n",
    "\n",
    "        # Identify persistence patterns\n",
    "        layers = sorted(self.persistence_stats.keys())\n",
    "        if len(layers) >= 2:\n",
    "            early_layer = layers[0]\n",
    "            late_layer = layers[-1]\n",
    "\n",
    "            early_dim0 = np.mean(self.persistence_stats[early_layer]['dim0_avg']) if self.persistence_stats[early_layer]['dim0_avg'] else 0\n",
    "            late_dim0 = np.mean(self.persistence_stats[late_layer]['dim0_avg']) if self.persistence_stats[late_layer]['dim0_avg'] else 0\n",
    "\n",
    "            if abs(late_dim0 - early_dim0) > 0.05:\n",
    "                if late_dim0 > early_dim0:\n",
    "                    findings.append(f\"**Topological significance**: Dimension 0 persistence increases from {early_dim0:.4f} in layer {early_layer} to {late_dim0:.4f} in layer {late_layer}, suggesting more stable connected components in deeper layers.\")\n",
    "                else:\n",
    "                    findings.append(f\"**Topological significance**: Dimension 0 persistence decreases from {early_dim0:.4f} in layer {early_layer} to {late_dim0:.4f} in layer {late_layer}, suggesting less stable connected components in deeper layers.\")\n",
    "\n",
    "        return findings\n",
    "\n",
    "    def _save_aggregate_stats(self, output_path):\n",
    "        \"\"\"Save aggregate statistics to a JSON file.\"\"\"\n",
    "\n",
    "        serializable_stats = {\n",
    "            'sample_count': self.sample_count,\n",
    "            'model_name': self.model_name,\n",
    "            'device': self.device,\n",
    "            'memory_usage': self.memory_tracker,\n",
    "\n",
    "            'betti_numbers': {},\n",
    "            'attention_stats': {},\n",
    "            'persistence_stats': {},\n",
    "            'head_specialization': {},\n",
    "            'token_focus': {}\n",
    "        }\n",
    "\n",
    "        for layer_idx, data in self.betti_numbers.items():\n",
    "            serializable_stats['betti_numbers'][str(layer_idx)] = {\n",
    "                'dim0': [float(x) for x in data['dim0']],\n",
    "                'dim1': [float(x) for x in data['dim1']]\n",
    "            }\n",
    "\n",
    "        # Convert attention stats\n",
    "        for layer_idx, data in self.attention_stats.items():\n",
    "            serializable_stats['attention_stats'][str(layer_idx)] = {\n",
    "                'mean': [float(x) for x in data['mean']],\n",
    "                'max': [float(x) for x in data['max']],\n",
    "                'std': [float(x) for x in data['std']]\n",
    "            }\n",
    "\n",
    "        # Convert persistence stats\n",
    "        for layer_idx, data in self.persistence_stats.items():\n",
    "            serializable_stats['persistence_stats'][str(layer_idx)] = {\n",
    "                'dim0_avg': [float(x) for x in data['dim0_avg']],\n",
    "                'dim0_max': [float(x) for x in data['dim0_max']],\n",
    "                'dim1_avg': [float(x) for x in data['dim1_avg']],\n",
    "                'dim1_max': [float(x) for x in data['dim1_max']]\n",
    "            }\n",
    "\n",
    "        # Convert head specialization\n",
    "        for layer_idx, heads in self.head_specialization.items():\n",
    "            serializable_stats['head_specialization'][str(layer_idx)] = {}\n",
    "            for head_idx, tokens in heads.items():\n",
    "                serializable_stats['head_specialization'][str(layer_idx)][str(head_idx)] = tokens\n",
    "\n",
    "        # Convert token focus\n",
    "        for layer_idx, heads in self.token_focus.items():\n",
    "            serializable_stats['token_focus'][str(layer_idx)] = {}\n",
    "            for head_idx, tokens in heads.items():\n",
    "                serializable_stats['token_focus'][str(layer_idx)][str(head_idx)] = dict(tokens)\n",
    "\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(serializable_stats, f, indent=2)\n",
    "\n",
    "        logger.info(f\"Aggregate statistics saved to {output_path}\")\n",
    "\n",
    "    def run_statistical_tests(self):\n",
    "        \"\"\"\n",
    "        Run statistical significance tests on the collected data.\n",
    "        This helps determine if differences between layers are meaningful or just random variation.\n",
    "        \"\"\"\n",
    "        logger.info(\"Running statistical significance tests between layers...\")\n",
    "\n",
    "        test_results = {}\n",
    "\n",
    "        layers = sorted(self.betti_numbers.keys())\n",
    "        if len(layers) < 2:\n",
    "            logger.warning(\"Need at least 2 layers for statistical testing\")\n",
    "            return {}\n",
    "\n",
    "        for i in range(len(layers)-1):\n",
    "            for j in range(i+1, len(layers)):\n",
    "                layer_i = layers[i]\n",
    "                layer_j = layers[j]\n",
    "\n",
    "                comparison_key = f\"layer_{layer_i}_vs_{layer_j}\"\n",
    "                test_results[comparison_key] = {}\n",
    "\n",
    "                # Test Betti Numbers (Topological Features)\n",
    "\n",
    "                # Test Betti0 (connected components)\n",
    "                if (self.betti_numbers[layer_i]['dim0'] and\n",
    "                    self.betti_numbers[layer_j]['dim0']):\n",
    "                    # Run Welch's t-test (doesn't assume equal variance)\n",
    "                    stat, pval = stats.ttest_ind(\n",
    "                        self.betti_numbers[layer_i]['dim0'],\n",
    "                        self.betti_numbers[layer_j]['dim0'],\n",
    "                        equal_var=False\n",
    "                    )\n",
    "\n",
    "                    test_results[comparison_key]['betti0'] = {\n",
    "                        'pval': float(pval),\n",
    "                        'significant': pval < 0.05,\n",
    "                        'mean_i': float(np.mean(self.betti_numbers[layer_i]['dim0'])),\n",
    "                        'mean_j': float(np.mean(self.betti_numbers[layer_j]['dim0']))\n",
    "                    }\n",
    "\n",
    "                # Test Betti1 (loops/cycles)\n",
    "                if (self.betti_numbers[layer_i]['dim1'] and\n",
    "                    self.betti_numbers[layer_j]['dim1']):\n",
    "                    # Run Welch's t-test\n",
    "                    stat, pval = stats.ttest_ind(\n",
    "                        self.betti_numbers[layer_i]['dim1'],\n",
    "                        self.betti_numbers[layer_j]['dim1'],\n",
    "                        equal_var=False\n",
    "                    )\n",
    "\n",
    "                    test_results[comparison_key]['betti1'] = {\n",
    "                        'pval': float(pval),\n",
    "                        'significant': pval < 0.05,\n",
    "                        'mean_i': float(np.mean(self.betti_numbers[layer_i]['dim1'])),\n",
    "                        'mean_j': float(np.mean(self.betti_numbers[layer_j]['dim1']))\n",
    "                    }\n",
    "\n",
    "                # Test Attention Statistics \n",
    "\n",
    "                # Test mean attention\n",
    "                if (self.attention_stats[layer_i]['mean'] and\n",
    "                    self.attention_stats[layer_j]['mean']):\n",
    "                    stat, pval = stats.ttest_ind(\n",
    "                        self.attention_stats[layer_i]['mean'],\n",
    "                        self.attention_stats[layer_j]['mean'],\n",
    "                        equal_var=False\n",
    "                    )\n",
    "\n",
    "                    test_results[comparison_key]['mean_attention'] = {\n",
    "                        'pval': float(pval),\n",
    "                        'significant': pval < 0.05,\n",
    "                        'mean_i': float(np.mean(self.attention_stats[layer_i]['mean'])),\n",
    "                        'mean_j': float(np.mean(self.attention_stats[layer_j]['mean']))\n",
    "                    }\n",
    "\n",
    "                # Test max attention\n",
    "                if (self.attention_stats[layer_i]['max'] and\n",
    "                    self.attention_stats[layer_j]['max']):\n",
    "                    stat, pval = stats.ttest_ind(\n",
    "                        self.attention_stats[layer_i]['max'],\n",
    "                        self.attention_stats[layer_j]['max'],\n",
    "                        equal_var=False\n",
    "                    )\n",
    "\n",
    "                    test_results[comparison_key]['max_attention'] = {\n",
    "                        'pval': float(pval),\n",
    "                        'significant': pval < 0.05,\n",
    "                        'mean_i': float(np.mean(self.attention_stats[layer_i]['max'])),\n",
    "                        'mean_j': float(np.mean(self.attention_stats[layer_j]['max']))\n",
    "                    }\n",
    "\n",
    "                # Test Persistence Statistics \n",
    "\n",
    "                # Test dimension 0 average persistence\n",
    "                if (self.persistence_stats[layer_i]['dim0_avg'] and\n",
    "                    self.persistence_stats[layer_j]['dim0_avg']):\n",
    "                    stat, pval = stats.ttest_ind(\n",
    "                        self.persistence_stats[layer_i]['dim0_avg'],\n",
    "                        self.persistence_stats[layer_j]['dim0_avg'],\n",
    "                        equal_var=False\n",
    "                    )\n",
    "\n",
    "                    test_results[comparison_key]['persistence_dim0'] = {\n",
    "                        'pval': float(pval),\n",
    "                        'significant': pval < 0.05,\n",
    "                        'mean_i': float(np.mean(self.persistence_stats[layer_i]['dim0_avg'])),\n",
    "                        'mean_j': float(np.mean(self.persistence_stats[layer_j]['dim0_avg']))\n",
    "                    }\n",
    "\n",
    "        # Find and log significant results\n",
    "        significant_findings = []\n",
    "        for comparison, tests in test_results.items():\n",
    "            for metric, result in tests.items():\n",
    "                if result['significant']:\n",
    "                    layer_i = comparison.split('_vs_')[0].replace('layer_', '')\n",
    "                    layer_j = comparison.split('_vs_')[1]\n",
    "                    significant_findings.append(\n",
    "                        f\"{metric} differs significantly between layer {layer_i} and layer {layer_j} (p={result['pval']:.4f})\"\n",
    "                    )\n",
    "\n",
    "        if significant_findings:\n",
    "            logger.info(f\"Found {len(significant_findings)} statistically significant differences\")\n",
    "            for finding in significant_findings[:5]:  # Log first 5 findings\n",
    "                logger.info(finding)\n",
    "            if len(significant_findings) > 5:\n",
    "                logger.info(f\"...and {len(significant_findings) - 5} more\")\n",
    "        else:\n",
    "            logger.info(\"No statistically significant differences found between layers\")\n",
    "\n",
    "        return test_results\n",
    "\n",
    "    def update_findings_with_statistics(self, findings, test_results):\n",
    "        \"\"\"\n",
    "        Update key findings with statistical significance information.\n",
    "        \"\"\"\n",
    "        updated_findings = []\n",
    "\n",
    "        for finding in findings:\n",
    "\n",
    "            matched = False\n",
    "\n",
    "            if \"Connected components increase from\" in finding or \"Connected components decrease from\" in finding:\n",
    "\n",
    "                import re\n",
    "                match = re.search(r\"layer (\\d+) to .* in layer (\\d+)\", finding)\n",
    "                if match:\n",
    "                    layer_i, layer_j = match.groups()\n",
    "                    comparison_key = f\"layer_{layer_i}_vs_{layer_j}\"\n",
    "\n",
    "                    if comparison_key in test_results and 'betti0' in test_results[comparison_key]:\n",
    "                        result = test_results[comparison_key]['betti0']\n",
    "                        if result['significant']:\n",
    "                            finding += f\" This difference is statistically significant (p={result['pval']:.4f}).\"\n",
    "                        else:\n",
    "                            finding += f\" However, this difference is not statistically significant (p={result['pval']:.4f}).\"\n",
    "                        matched = True\n",
    "\n",
    "            elif \"Loops increase from\" in finding or \"Loops decrease from\" in finding:\n",
    "                import re\n",
    "                match = re.search(r\"layer (\\d+) to .* in layer (\\d+)\", finding)\n",
    "                if match:\n",
    "                    layer_i, layer_j = match.groups()\n",
    "                    comparison_key = f\"layer_{layer_i}_vs_{layer_j}\"\n",
    "\n",
    "                    if comparison_key in test_results and 'betti1' in test_results[comparison_key]:\n",
    "                        result = test_results[comparison_key]['betti1']\n",
    "                        if result['significant']:\n",
    "                            finding += f\" This difference is statistically significant (p={result['pval']:.4f}).\"\n",
    "                        else:\n",
    "                            finding += f\" However, this difference is not statistically significant (p={result['pval']:.4f}).\"\n",
    "                        matched = True\n",
    "\n",
    "            elif \"Average attention increases from\" in finding or \"Average attention decreases from\" in finding:\n",
    "                import re\n",
    "                match = re.search(r\"layer (\\d+) to .* in layer (\\d+)\", finding)\n",
    "                if match:\n",
    "                    layer_i, layer_j = match.groups()\n",
    "                    comparison_key = f\"layer_{layer_i}_vs_{layer_j}\"\n",
    "\n",
    "                    if comparison_key in test_results and 'mean_attention' in test_results[comparison_key]:\n",
    "                        result = test_results[comparison_key]['mean_attention']\n",
    "                        if result['significant']:\n",
    "                            finding += f\" This difference is statistically significant (p={result['pval']:.4f}).\"\n",
    "                        else:\n",
    "                            finding += f\" However, this difference is not statistically significant (p={result['pval']:.4f}).\"\n",
    "                        matched = True\n",
    "\n",
    "            updated_findings.append(finding)\n",
    "\n",
    "        return updated_findings\n",
    "\n",
    "    def generate_statistical_report(self, test_results, output_dir):\n",
    "        \"\"\"\n",
    "        Generate a simple report of statistical significance tests.\n",
    "        \"\"\"\n",
    "        from pathlib import Path\n",
    "\n",
    "        output_path = Path(output_dir) / \"statistical_significance.md\"\n",
    "\n",
    "        with open(output_path, \"w\") as f:\n",
    "            f.write(\"# Statistical Significance Analysis\\n\\n\")\n",
    "            f.write(\"This report shows which differences between layers are statistically significant.\\n\\n\")\n",
    "            f.write(\"## Interpretation\\n\\n\")\n",
    "            f.write(\"- p-value < 0.05: The difference is statistically significant\\n\")\n",
    "            f.write(\"- p-value ≥ 0.05: The difference could be due to random variation\\n\\n\")\n",
    "\n",
    "            f.write(\"## Significant Differences Between Layers\\n\\n\")\n",
    "\n",
    "            sig_count = 0\n",
    "            for comparison, tests in test_results.items():\n",
    "                for metric, result in tests.items():\n",
    "                    if result['significant']:\n",
    "                        sig_count += 1\n",
    "\n",
    "            if sig_count == 0:\n",
    "                f.write(\"No statistically significant differences were found between layers.\\n\\n\")\n",
    "            else:\n",
    "                f.write(f\"Found {sig_count} statistically significant differences:\\n\\n\")\n",
    "\n",
    "                f.write(\"| Comparison | Metric | Layer 1 Mean | Layer 2 Mean | p-value |\\n\")\n",
    "                f.write(\"|------------|--------|--------------|--------------|--------|\\n\")\n",
    "\n",
    "                for comparison, tests in sorted(test_results.items()):\n",
    "                    layer_i = comparison.split('_vs_')[0].replace('layer_', '')\n",
    "                    layer_j = comparison.split('_vs_')[1]\n",
    "\n",
    "                    for metric, result in sorted(tests.items()):\n",
    "                        if result['significant']:\n",
    "\n",
    "                            if metric == 'betti0':\n",
    "                                metric_name = \"Betti₀ (Components)\"\n",
    "                            elif metric == 'betti1':\n",
    "                                metric_name = \"Betti₁ (Loops)\"\n",
    "                            elif metric == 'mean_attention':\n",
    "                                metric_name = \"Mean Attention\"\n",
    "                            elif metric == 'max_attention':\n",
    "                                metric_name = \"Max Attention\"\n",
    "                            elif metric == 'persistence_dim0':\n",
    "                                metric_name = \"Dim0 Persistence\"\n",
    "                            else:\n",
    "                                metric_name = metric.replace('_', ' ').title()\n",
    "\n",
    "                            f.write(f\"| Layer {layer_i} vs {layer_j} | {metric_name} | {result['mean_i']:.4f} | {result['mean_j']:.4f} | {result['pval']:.4f} |\\n\")\n",
    "\n",
    "            f.write(\"\\n## All Test Results\\n\\n\")\n",
    "            f.write(\"For completeness, here are all statistical test results, significant or not:\\n\\n\")\n",
    "\n",
    "            f.write(\"| Comparison | Metric | Layer 1 Mean | Layer 2 Mean | p-value | Significant? |\\n\")\n",
    "            f.write(\"|------------|--------|--------------|--------------|---------|-------------|\\n\")\n",
    "\n",
    "            for comparison, tests in sorted(test_results.items()):\n",
    "                layer_i = comparison.split('_vs_')[0].replace('layer_', '')\n",
    "                layer_j = comparison.split('_vs_')[1]\n",
    "\n",
    "                for metric, result in sorted(tests.items()):\n",
    "\n",
    "                    if metric == 'betti0':\n",
    "                        metric_name = \"Betti₀ (Components)\"\n",
    "                    elif metric == 'betti1':\n",
    "                        metric_name = \"Betti₁ (Loops)\"\n",
    "                    elif metric == 'mean_attention':\n",
    "                        metric_name = \"Mean Attention\"\n",
    "                    elif metric == 'max_attention':\n",
    "                        metric_name = \"Max Attention\"\n",
    "                    elif metric == 'persistence_dim0':\n",
    "                        metric_name = \"Dim0 Persistence\"\n",
    "                    else:\n",
    "                        metric_name = metric.replace('_', ' ').title()\n",
    "\n",
    "                    f.write(f\"| Layer {layer_i} vs {layer_j} | {metric_name} | {result['mean_i']:.4f} | {result['mean_j']:.4f} | {result['pval']:.4f} | {'Yes' if result['significant'] else 'No'} |\\n\")\n",
    "\n",
    "        logger.info(f\"Statistical significance report saved to {output_path}\")\n",
    "        return str(output_path)\n",
    "\n",
    "\n",
    "class AggregateAnalysisRunner:\n",
    "    \"\"\"\n",
    "    Runs the aggregate-only analysis on a dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, output_dir, max_tokens=48, max_workers=None, cache_dir=None):\n",
    "        \"\"\"\n",
    "        Initialize the analysis runner.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model to analyze\n",
    "            output_dir: Directory to save results\n",
    "            max_tokens: Maximum number of tokens to process\n",
    "            max_workers: Maximum number of parallel workers (None = use CPU count)\n",
    "            cache_dir: Directory to cache model files\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.max_tokens = max_tokens\n",
    "        self.max_workers = max_workers or min(os.cpu_count() or 4, 8)  # Limit to 8 workers max\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Initialized AggregateAnalysisRunner for model: {model_name}\")\n",
    "\n",
    "    def run_analysis(self, dataset_path, n_samples=500, focus_word_column=None, layer_subset=None):\n",
    "        \"\"\"\n",
    "        Run the aggregate-only analysis on a dataset.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            data = pd.read_csv(dataset_path)\n",
    "            logger.info(f\"Loaded dataset with {len(data)} rows\")\n",
    "\n",
    "            analyzer = AggregateTopologyAnalyzer(\n",
    "                model_name=self.model_name,\n",
    "                max_tokens=self.max_tokens,\n",
    "                cache_dir=self.cache_dir\n",
    "            )\n",
    "\n",
    "            _, model = analyzer.load_model_and_tokenizer()\n",
    "\n",
    "            dummy_input = analyzer.tokenizer(\"Test\", return_tensors=\"pt\")\n",
    "            if analyzer.device != \"cpu\":\n",
    "                dummy_input = {k: v.to(analyzer.device) for k, v in dummy_input.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = analyzer.model(**dummy_input, output_attentions=True)\n",
    "\n",
    "            actual_num_layers = len(outputs.attentions)\n",
    "            logger.info(f\"Model has {actual_num_layers} attention layers\")\n",
    "\n",
    "            if layer_subset is None:\n",
    "                layer_step = max(1, actual_num_layers // 4)\n",
    "                layer_subset = [0, layer_step, 2*layer_step, 3*layer_step, actual_num_layers-1]\n",
    "                layer_subset = sorted(list(set([min(l, actual_num_layers-1) for l in layer_subset])))\n",
    "            else:\n",
    "                layer_subset = [min(l, actual_num_layers-1) for l in layer_subset]\n",
    "                layer_subset = sorted(list(set(layer_subset))) \n",
    "\n",
    "            logger.info(f\"Analyzing layers: {layer_subset}\")\n",
    "\n",
    "            logger.info(\"Selecting samples...\")\n",
    "            samples = self._select_samples(data, n_samples, focus_word_column)\n",
    "            logger.info(f\"Selected {len(samples)} samples for analysis\")\n",
    "\n",
    "            logger.info(\"Starting sample processing...\")\n",
    "            sample_counter = 0\n",
    "            successful_samples = 0\n",
    "\n",
    "            for i, sample in enumerate(tqdm(samples, desc=\"Processing samples\")):\n",
    "                try:\n",
    "                    sample_id = i + 1\n",
    "                    text = sample['text']\n",
    "                    focus_word = sample.get('focus_word')\n",
    "\n",
    "                    metrics = analyzer.analyze_sample_and_aggregate(\n",
    "                        text=text,\n",
    "                        sample_id=sample_id,\n",
    "                        layer_subset=layer_subset\n",
    "                    )\n",
    "\n",
    "                    if 'error' not in metrics:\n",
    "                        successful_samples += 1\n",
    "\n",
    "                    sample_counter += 1\n",
    "                    if sample_counter % 10 == 0:\n",
    "                        logger.info(f\"Processed {sample_counter}/{len(samples)} samples\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing sample {i+1}: {str(e)}\")\n",
    "\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            logger.info(\"Generating aggregate report...\")\n",
    "            report_path = analyzer.generate_aggregate_report(self.output_dir)\n",
    "\n",
    "            execution_time = time.time() - start_time\n",
    "            logger.info(f\"Analysis completed in {execution_time/60:.2f} minutes\")\n",
    "            logger.info(f\"Successfully processed {successful_samples}/{len(samples)} samples\")\n",
    "            logger.info(f\"Report available at: {report_path}\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Analysis failed: {str(e)}\", exc_info=True)\n",
    "            return False\n",
    "\n",
    "    def _select_samples(self, data, n_samples, focus_word_column=None):\n",
    "        \"\"\"\n",
    "        Select samples from the dataset.\n",
    "\n",
    "        Args:\n",
    "            data: Pandas DataFrame with the dataset\n",
    "            n_samples: Number of samples to select\n",
    "            focus_word_column: Name of column containing focus words (if any)\n",
    "\n",
    "        Returns:\n",
    "            list: List of selected samples\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "\n",
    "        if focus_word_column and focus_word_column in data.columns:\n",
    "            focus_words = data[focus_word_column].value_counts().index[:n_samples]\n",
    "            for word in focus_words:\n",
    "                word_data = data[data[focus_word_column] == word]\n",
    "                if len(word_data) > 0:\n",
    "                    if 'text' in word_data.columns:\n",
    "                        word_data = word_data.iloc[word_data['text'].str.len().argsort()]\n",
    "                    sample = word_data.iloc[0]\n",
    "                    samples.append({\n",
    "                        'text': sample['text'],\n",
    "                        'focus_word': word\n",
    "                    })\n",
    "\n",
    "        if len(samples) < n_samples:\n",
    "            if 'text' in data.columns:\n",
    "                try:\n",
    "                    sorted_data = data.iloc[data['text'].str.len().argsort()]\n",
    "                    chunk_size = len(sorted_data) // 3\n",
    "                    begin_chunk = sorted_data.iloc[:chunk_size]\n",
    "                    middle_chunk = sorted_data.iloc[chunk_size:2*chunk_size]\n",
    "\n",
    "                    # Sample from different chunks\n",
    "                    more_needed = n_samples - len(samples)\n",
    "                    from_begin = min(more_needed // 2, len(begin_chunk))\n",
    "                    from_middle = min(more_needed - from_begin, len(middle_chunk))\n",
    "\n",
    "                    more_samples_begin = begin_chunk.sample(from_begin)\n",
    "                    more_samples_middle = middle_chunk.sample(from_middle)\n",
    "                    more_samples = pd.concat([more_samples_begin, more_samples_middle])\n",
    "\n",
    "                    if len(more_samples) < more_needed:\n",
    "                        remaining = more_needed - len(more_samples)\n",
    "                        more_samples = pd.concat([more_samples, data.sample(remaining)])\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error in intelligent sampling: {e}, falling back to random\")\n",
    "                    more_samples = data.sample(min(n_samples - len(samples), len(data)))\n",
    "            else:\n",
    "                more_samples = data.sample(min(n_samples - len(samples), len(data)))\n",
    "\n",
    "            for _, row in more_samples.iterrows():\n",
    "                samples.append({\n",
    "                    'text': row['text'],\n",
    "                    'focus_word': row.get(focus_word_column) if focus_word_column else None\n",
    "                })\n",
    "\n",
    "        return samples[:n_samples]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46355676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Run the aggregate-only analysis.\n",
    "    \"\"\"\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    model_name = \"meta-llama/llama-3.2.1B\"  \n",
    "    dataset_path = \"/content/drive/MyDrive/wiki_dataset.csv\" \n",
    "    output_dir = f\"/content/drive/MyDrive/Sink/topology/{model_name.split('/')[-1]}\"\n",
    "\n",
    "    try:\n",
    "        runner = AggregateAnalysisRunner(\n",
    "            model_name=model_name,\n",
    "            output_dir=output_dir,\n",
    "            max_tokens=48,\n",
    "            max_workers=4\n",
    "        )\n",
    "\n",
    "        runner.run_analysis(\n",
    "            dataset_path=dataset_path,\n",
    "            n_samples=500,\n",
    "            focus_word_column='focus_word'\n",
    "        )\n",
    "\n",
    "        print(\"Analysis completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Analysis error: {str(e)}\", exc_info=True)\n",
    "        print(f\"Analysis error: {e}\")\n",
    "    finally:\n",
    "        print(\"Analysis finished. Check output directory for results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
